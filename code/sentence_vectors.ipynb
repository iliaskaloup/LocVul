{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1db34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "import gensim.downloader\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d358cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c28c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'ffmpeg_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'qemu_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b80a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c07d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 314\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2927887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    9136\n",
      "1    6727\n",
      "Name: target, dtype: int64\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "print(data[\"target\"].value_counts())\n",
    "max_length = data['length'].max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dce2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  project  target                                               func  length\n",
      "0    qemu       1  static void filter_mirror_setup(netfilterstate...      31\n",
      "1    qemu       1  static void test_init(testdata *d)\\r\\n{\\r\\n   ...      86\n",
      "2    qemu       1  static inline int64_t sub64(const int64_t a, c...      15\n",
      "3    qemu       1  static int xen_9pfs_connect(struct xendevice *...     260\n",
      "4    qemu       1  static void nbd_refresh_filename(blockdriverst...     149\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c27bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer models if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_list = [word_tokenize(sentence) for sentence in data[\"func\"].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7e0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(words, w2v_vectors, dim):\n",
    "    words_vecs = [w2v_vectors[word] for word in words if word in w2v_vectors]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e6fa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        p_dist = K.sum(K.square(anchor-positive), axis=-1)\n",
    "        n_dist = K.sum(K.square(anchor-negative), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249556a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TripletSemiHardLoss' from 'tensorflow.keras.losses' (D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\keras\\losses\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48584\\2843733200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTripletSemiHardLoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuildMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnIn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TripletSemiHardLoss' from 'tensorflow.keras.losses' (D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\keras\\losses\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import TripletSemiHardLoss\n",
    "\n",
    "def buildMLP(n_inputs):\n",
    "    nIn = n_inputs\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(256, input_dim=nIn, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)))#hidden\n",
    "    model.add(Dense(256, input_dim=nIn))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256))#hidden\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1)) #binary classification\n",
    "    model.add(Activation('sigmoid'))#Output # sigmoid # softmax\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001) #, epsilon=1e-8, decay=1e-6\n",
    "    model.compile(loss=TripletLoss(), optimizer=optimizer, metrics=[f1_metric]) # 'binary_crossentropy' # \"sparse_categorical_crossentropy\"\n",
    "    # triplet loss a=0.5 b=0.001 g=0.5\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4879f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fold number=  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TripletLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41672\\700316593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#         Y_train=pd.DataFrame(Y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mmyModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model summary\\m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41672\\3423137520.py\u001b[0m in \u001b[0;36mbuildMLP\u001b[1;34m(n_inputs)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Output # sigmoid # softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, epsilon=1e-8, decay=1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTripletLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1_metric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 'binary_crossentropy' # \"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m# triplet loss a=0.5 b=0.001 g=0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TripletLoss' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array(tokenized_list)\n",
    "y = data[\"target\"].values\n",
    "\n",
    "############## cross validation\n",
    "scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'f2']\n",
    "values = [np.array([]) for i in range(0, len(scores))]\n",
    "score_dict = OrderedDict(zip(scores, values))\n",
    "k=10\n",
    "f=0\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "nb_epoch = 100\n",
    "BS = 32\n",
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    f = f + 1\n",
    "    print('fold number= ',f)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.array([vectorize(sentence, w2v_vectors, dim) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence, w2v_vectors, dim) for sentence in X_test])\n",
    "\n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "\n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "\n",
    "    myModel = buildMLP(X_train.shape[1])\n",
    "\n",
    "    print(\"model summary\\m\", myModel.summary())\n",
    "    csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "    es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "    print(type(X_train))\n",
    "    history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "\n",
    "    #load best model\n",
    "    #model = load_model('best_model.h5')\n",
    "    myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "    scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "    #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "    predScores = myModel.predict(X_test)\n",
    "    predictions = (predScores > 0.5).astype(\"int32\")\n",
    "    #predictions = predScores.argmax(axis=1)\n",
    "    accuracy=accuracy_score(Y_test, predictions)\n",
    "    precision=precision_score(Y_test, predictions)\n",
    "    recall=recall_score(Y_test, predictions)\n",
    "    f1=f1_score(Y_test, predictions)\n",
    "    roc_auc=roc_auc_score(Y_test, predictions)\n",
    "    f2=5*precision*recall / (4*precision+recall)\n",
    "    print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "    acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "    print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "    print(\"Precision:%.2f%%\"%(precision*100))\n",
    "    print(\"Recall:%.2f%%\"%(recall*100))\n",
    "    print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "    print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "    print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "    print(classification_report(Y_test, predictions))\n",
    "    del myModel\n",
    "    score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "    score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "    score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "    score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "    score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "    score_dict['f2'] = np.append(score_dict['f2'], f2)\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "\n",
    "print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))\n",
    "print(\"f2: %.2f%% (%.2f%%)\" % (score_dict['f2'].mean()*100, score_dict['f2'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3806a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(tokenized_list)\n",
    "# y = data[\"target\"].values\n",
    "\n",
    "# ############## cross validation\n",
    "# scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'f2']\n",
    "# values = [np.array([]) for i in range(0, len(scores))]\n",
    "# score_dict = OrderedDict(zip(scores, values))\n",
    "# k=10\n",
    "# f=0\n",
    "# kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "# nb_epoch = 100\n",
    "# BS = 32\n",
    "# print(\"Training...\")\n",
    "# milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "# for train_index, test_index in kfold.split(X, y):\n",
    "#     f = f + 1\n",
    "#     print('fold number= ',f)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     Y_train, Y_test = y[train_index], y[test_index]\n",
    "\n",
    "#     X_train = np.array([vectorize(sentence, w2v_vectors, dim) for sentence in X_train])\n",
    "#     X_test = np.array([vectorize(sentence, w2v_vectors, dim) for sentence in X_test])\n",
    "\n",
    "# #         #sampling\n",
    "# #         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "# #         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "\n",
    "# #         #shuffle dataset\n",
    "# #         X_resampled=pd.DataFrame(X_res)\n",
    "# #         Y_resampled=pd.DataFrame(Y_res)\n",
    "# #         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "# #         newTrain = shuffle(newTrain,random_state=seed)\n",
    "# #         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "# #         X_train=pd.DataFrame(X_train)\n",
    "# #         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "# #         Y_train=pd.DataFrame(Y_train)\n",
    "\n",
    "#     myModel = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "#     myModel.fit(X_train, Y_train.ravel())\n",
    "    \n",
    "#     predictions = myModel.predict(X_test)\n",
    "#     accuracy=accuracy_score(Y_test, predictions)\n",
    "#     precision=precision_score(Y_test, predictions)\n",
    "#     recall=recall_score(Y_test, predictions)\n",
    "#     f1=f1_score(Y_test, predictions)\n",
    "#     roc_auc=roc_auc_score(Y_test, predictions)\n",
    "#     f2=5*precision*recall / (4*precision+recall)\n",
    "#     print(confusion_matrix(Y_test, predictions, labels=[0, 1]))\n",
    "#     tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\n",
    "#     acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "#     print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "#     print(\"Precision:%.2f%%\"%(precision*100))\n",
    "#     print(\"Recall:%.2f%%\"%(recall*100))\n",
    "#     print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "#     print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "#     print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "#     print(classification_report(Y_test, predictions))\n",
    "#     del myModel\n",
    "#     score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "#     score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "#     score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "#     score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "#     score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "#     score_dict['f2'] = np.append(score_dict['f2'], f2)\n",
    "\n",
    "# milli_sec2 = int(round(time.time() * 1000))\n",
    "# print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "\n",
    "# print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "# print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "# print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "# print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "# print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))\n",
    "# print(\"f2: %.2f%% (%.2f%%)\" % (score_dict['f2'].mean()*100, score_dict['f2'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89944639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
