{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among QEMU and FFMPEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'ffmpeg_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'qemu_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"target\"].value_counts())\n",
    "max_length = data['length'].max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffa700",
   "metadata": {},
   "source": [
    "PRE_PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be9a90",
   "metadata": {},
   "source": [
    "Word2Vec NL pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ac4966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "62ab5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4eaf7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_vectors.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9302ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer models if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_list = [word_tokenize(sentence) for sentence in data[\"func\"].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f15099b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each token using Word2Vec embeddings\n",
    "encoded_list = []\n",
    "for sentence_tokens in tokenized_list:\n",
    "    encoded_sentence = []\n",
    "    for token in sentence_tokens:\n",
    "        if token in w2v_vectors:\n",
    "            encoded_token = w2v_vectors[token]\n",
    "            encoded_sentence.append(encoded_token)\n",
    "    encoded_list.append(encoded_sentence)\n",
    "\n",
    "data[\"w2v_func\"] = encoded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269db8d2",
   "metadata": {},
   "source": [
    "FastText NL pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6b274972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "#fastText_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "40208525",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tokenized_list = [word_tokenize(sentence) for sentence in data[\"func\"].tolist()]\n",
    "\n",
    "# Encode each token using fastText embeddings\n",
    "encoded_list = []\n",
    "for sentence_tokens in f_tokenized_list:\n",
    "    encoded_sentence = []\n",
    "    for token in sentence_tokens:\n",
    "        if token in fastText_vectors:\n",
    "            encoded_token = fastText_vectors[token]\n",
    "            encoded_sentence.append(encoded_token)\n",
    "        else:\n",
    "            continue\n",
    "    encoded_list.append(encoded_sentence)\n",
    "\n",
    "data[\"ft_func\"] = encoded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f5db",
   "metadata": {},
   "source": [
    "BERT pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e2f89d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3ef02bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = bert.get_input_embeddings()\n",
    "embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7ab68807",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer.encode(sente, padding=True, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in data[\"func\"]] # Tokenize the complete sentences\n",
    "\n",
    "lines_pad = []\n",
    "for seq in sequences:\n",
    "    lines_pad.append(seq[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "03a02517",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"bert_func\"] = lines_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7efba",
   "metadata": {},
   "source": [
    "CodeBERT pre-trained CPP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7c8c38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"microsoft/codebert-base\" # \"neulab/codebert-cpp\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ea203cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "codebert_embeddings = codebert.get_input_embeddings()\n",
    "embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "num_words = len(embedding_matrix)\n",
    "print(num_words)\n",
    "dim = len(embedding_matrix[0])\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd3d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [tokenizer(sente, return_tensors=\"tf\", padding=True, truncation=True, add_special_tokens=False) for sente in data[\"func\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7df0640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def extractEmbList(sequences):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "#         if len(seq) < max_len:\n",
    "#             for i in range(len(seq), max_len):\n",
    "#                 seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    \n",
    "    [arr.tolist() for arr in lines_pad]\n",
    "    \n",
    "    return lines_pad\n",
    "\n",
    "lines_pad = np.array(extractEmbList(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c326e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"codebert_func\"] = lines_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d708d8b",
   "metadata": {},
   "source": [
    "CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "624d1ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "project                                                     FFmpeg\n",
       "target                                                           0\n",
       "func             static av_cold int vdadec_init(avcodeccontext ...\n",
       "length                                                         151\n",
       "w2v_func         [[0.34179688, -0.0859375, -0.1328125, 0.023559...\n",
       "ft_func          [[0.054195, 0.0069789, 0.03935, 0.026746, -7.7...\n",
       "bert_func        [10763, 20704, 1035, 3147, 20014, 1058, 14697,...\n",
       "codebert_func    [42653, 6402, 1215, 33912, 6979, 748, 417, 182...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
