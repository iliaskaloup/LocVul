{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c53715-d4f5-4387-a656-ce236b5d8971",
   "metadata": {},
   "source": [
    "<b> Line-level Vulnerability Detection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59e0888-4aad-4bec-a17a-14b25133b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForSeq2SeqLM #, BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_scheduler\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae857b-008f-4402-b325-65505a7b1284",
   "metadata": {},
   "source": [
    "Basic Configuration of logging and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52acbab5-f04e-4514-b3da-be0511543089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:13:38 - INFO - SEED: 680\n"
     ]
    }
   ],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# Define logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Specify a constant seeder for processes\n",
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 135, 680]\n",
    "seed = seeders[9]\n",
    "logger.info(f\"SEED: {seed}\")\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "checkpoint_dir = './checkpoints_seq2seq'\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdd2725-3cf1-440e-aafd-df5171b436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, epoch, model, optimizer, scheduler, train_loss_per_epoch, val_loss_per_epoch, train_rouge_per_epoch, val_rouge_per_epoch):\n",
    "    # If model is wrapped in DataParallel, save the underlying model's state_dict\n",
    "    model_state_dict = model.module.state_dict() if torch.cuda.device_count() > 1 else model.state_dict()\n",
    "    \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model_state_dict,  # Use the correct state_dict\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_per_epoch': train_loss_per_epoch,\n",
    "        'val_loss_per_epoch': val_loss_per_epoch,\n",
    "        'train_rouge_per_epoch': train_rouge_per_epoch,\n",
    "        'val_rouge_per_epoch': val_rouge_per_epoch\n",
    "    }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d1569-5c75-4ea4-836f-ee5f61fcf9eb",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0af6892-9063-42b4-ada6-32b8fc2cf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "root_path = os.getcwd()\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))\n",
    "#dataset = dataset.iloc[0:1000,: ]\n",
    "dataset = dataset.dropna(subset=[\"processed_func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2f60cf-a6f7-41b6-81c8-254f5dfac707",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_TUNE = True  # Set this to False if you don't want to fine-tune the model and load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170d2276-82bf-44c6-87da-2fded18f2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "val_ratio = 0.1\n",
    "num_of_ratio = int(val_ratio * len(dataset))\n",
    "data = dataset.iloc[0:-num_of_ratio, :]\n",
    "test_data = dataset.iloc[-num_of_ratio:, :]\n",
    "train_data = data.iloc[0:-num_of_ratio, :]\n",
    "val_data = data.iloc[-num_of_ratio:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a05857-5480-432f-a7f5-cc9464e5469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release some memory\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d2c77c-a436-4f16-8823-f409cbb61f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:14:16 - INFO - Maximum number of words: 15441\n",
      "2025-02-28 01:14:16 - INFO - Train data length: 6613\n",
      "2025-02-28 01:14:16 - INFO - Validation data length: 842\n",
      "2025-02-28 01:14:16 - INFO - Test data length: 788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Lines</th>\n",
       "      <th>Line_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void UnwrapAndVerifyMojoHandle(mojo::ScopedSha...</td>\n",
       "      <td>size_t expect...</td>\n",
       "      <td>1,2,5,6,7,8,11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ssize_t socket_bytes_available(const socket_t ...</td>\n",
       "      <td>if (ioctl(socket-&gt;fd, FIONREAD, &amp;size) == -1)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WORD32 ih264d_parse_bslice(dec_struct_t * ps_d...</td>\n",
       "      <td>ps_dec-&gt;u4_bitoffset = ih264d_read...</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make_transform_image(png_store* PNG_CONST ps, ...</td>\n",
       "      <td>png_byte PNG_CONST bit_depth, unsigned int...</td>\n",
       "      <td>1,13,25,26,66,75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>struct key *find_keyring_by_name(const char *n...</td>\n",
       "      <td>\\t\\t\\tif (!skip_perm_check &amp;&amp;/~/\\t\\t\\t    key_...</td>\n",
       "      <td>28,29,30,31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  void UnwrapAndVerifyMojoHandle(mojo::ScopedSha...   \n",
       "1  ssize_t socket_bytes_available(const socket_t ...   \n",
       "2  WORD32 ih264d_parse_bslice(dec_struct_t * ps_d...   \n",
       "3  make_transform_image(png_store* PNG_CONST ps, ...   \n",
       "4  struct key *find_keyring_by_name(const char *n...   \n",
       "\n",
       "                                               Lines        Line_Index  \n",
       "0                                   size_t expect...    1,2,5,6,7,8,11  \n",
       "1      if (ioctl(socket->fd, FIONREAD, &size) == -1)                 5  \n",
       "2              ps_dec->u4_bitoffset = ih264d_read...               237  \n",
       "3      png_byte PNG_CONST bit_depth, unsigned int...  1,13,25,26,66,75  \n",
       "4  \\t\\t\\tif (!skip_perm_check &&/~/\\t\\t\\t    key_...       28,29,30,31  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train data\n",
    "train_data = train_data.sample(frac=1, random_state=seed).reset_index(drop=True) # shuffle training data\n",
    "\n",
    "word_counts = train_data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "logger.info(f\"Maximum number of words: {max_length}\")\n",
    "\n",
    "# keep only vulnerable samples\n",
    "train_data = train_data[train_data[\"target\"] == 1]\n",
    "train_data = train_data[~train_data['flaw_line_index'].isna()] # drop nan samples\n",
    "\n",
    "# keep the useful for Seq2Seq columns\n",
    "train_data = train_data[[\"processed_func\", \"flaw_line\", \"flaw_line_index\"]]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "train_data = pd.DataFrame(({'Text': train_data['processed_func'], 'Lines':train_data['flaw_line'], 'Line_Index':train_data['flaw_line_index']}))\n",
    "\n",
    "## validation data\n",
    "# keep only vulnerable samples\n",
    "val_data = val_data[val_data[\"target\"] == 1]\n",
    "val_data = val_data[~val_data['flaw_line_index'].isna()] # drop nan samples\n",
    "\n",
    "# keep the useful for Seq2Seq columns\n",
    "val_data = val_data[[\"processed_func\", \"flaw_line\", \"flaw_line_index\"]]\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Lines':val_data['flaw_line'], 'Line_Index':val_data['flaw_line_index']}))\n",
    "\n",
    "## test data\n",
    "# keep only vulnerable samples\n",
    "test_data = test_data[test_data[\"target\"] == 1]\n",
    "test_data = test_data[~test_data['flaw_line_index'].isna()] # drop nan samples\n",
    "\n",
    "# keep the useful for Seq2Seq columns\n",
    "test_data = test_data[[\"processed_func\", \"flaw_line\", \"flaw_line_index\"]]\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Lines':test_data['flaw_line'], 'Line_Index':test_data['flaw_line_index']}))\n",
    "\n",
    "# logs\n",
    "logger.info(f\"Train data length: {len(train_data)}\")\n",
    "logger.info(f\"Validation data length: {len(val_data)}\")\n",
    "logger.info(f\"Test data length: {len(test_data)}\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e23bd4-df78-49d6-8d22-5c154f0d32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace \"/~/\" with \"\\n\" in the 'Lines' column\n",
    "def replace_delimiter_with_newline(data):\n",
    "    # Replace \"/~/\" with \"\\n\" in the 'Lines' column\n",
    "    data['Lines'] = data['Lines'].str.replace('/~/', '\\n')\n",
    "    return data\n",
    "\n",
    "train_data = replace_delimiter_with_newline(train_data)\n",
    "val_data = replace_delimiter_with_newline(val_data)\n",
    "test_data = replace_delimiter_with_newline(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f352a04-00be-4409-ac31-b26b635a3800",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b2e845-370f-4000-8e59-a4450e9ca324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliaskaloup/anaconda3/envs/torchenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"Salesforce/codet5-base\" # \"google-t5/t5-base\" # Salesforce/codet5-base\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c00178-bcfc-4a92-afff-fc62f44c0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-02-28 01:14:17 - INFO - Max length of tokenized data: 4657\n",
      "2025-02-28 01:14:17 - INFO - Row with max length:: 5294\n",
      "2025-02-28 01:14:17 - INFO - Maximum tokenized length of Lines: 512\n"
     ]
    }
   ],
   "source": [
    "# Get the actual tokenized lengths\n",
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    max_row = 0\n",
    "    \n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Convert input_ids to a PyTorch tensor\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = torch.sum(input_ids_tensor != tokenizer.pad_token_id).item()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    logger.info(f\"Max length of tokenized data: {max_length}\")\n",
    "    logger.info(f\"Row with max length:: {max_row}\")\n",
    "    \n",
    "    return max_length\n",
    "\n",
    "target_encodings = tokenizer(\n",
    "    text=train_data['Lines'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    truncation=False,  # Do not truncate\n",
    "    padding=False      # No padding to see actual lengths\n",
    ")\n",
    "\n",
    "# Compute max length for the Lines column\n",
    "max_len_lines = getMaxLen(target_encodings)\n",
    "\n",
    "if max_len_lines > 512:\n",
    "    max_len_lines = 512\n",
    "else:\n",
    "    max_len_lines = max_len_lines\n",
    "logger.info(f\"Maximum tokenized length of Lines: {max_len_lines}\")\n",
    "#max_len_lines = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fad9303-b729-4a1c-977e-176c4956da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data, max_len_lines):\n",
    "    input_encodings = tokenizer(\n",
    "        data['Text'].tolist(),\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    target_encodings = tokenizer(\n",
    "        data['Lines'].tolist(),\n",
    "        max_length=max_len_lines,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    input_encodings['labels'] = target_encodings['input_ids']\n",
    "    \n",
    "    return input_encodings\n",
    "\n",
    "# Tokenize train, validation, and test data\n",
    "train_encodings = tokenize_data(train_data, max_len_lines)\n",
    "val_encodings = tokenize_data(val_data, max_len_lines)\n",
    "test_encodings = tokenize_data(test_data, max_len_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2a843-2dbb-4baa-acf0-edb2b645e3d5",
   "metadata": {},
   "source": [
    "Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1394f39a-6cfc-423a-9a8b-ee5ffc43988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bdb7d69-ae8a-4cbf-9c7f-9a189b12e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['labels'])\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['labels'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567e562-a7f2-417a-a4e3-638565256bad",
   "metadata": {},
   "source": [
    "Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d481ae-5d5b-4521-aeb0-cb9f88e7e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:14:21 - INFO - Device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
      ")\n",
      "No. of trainable parameters:  222882048\n"
     ]
    }
   ],
   "source": [
    "# Load the CodeT5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_variation)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Device {device}\")\n",
    "\n",
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cccc37-988a-424f-8da7-bf89c56dc2c7",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdfed119-190e-4ec7-8f05-a52acdffc889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliaskaloup/anaconda3/envs/torchenv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 10\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps = 1e-8)\n",
    "max_steps = len(train_loader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear', optimizer=optimizer, num_warmup_steps=max_steps // 5, num_training_steps=max_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31038648-25ee-403d-8a40-fc1937e7d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:14:22 - INFO - Starting training...\n",
      "2025-02-28 01:14:22 - INFO - Epoch: 1\n",
      "Training:   0%|                                                                                                                                                                                                      | 0/827 [00:00<?, ?it/s]/home/iliaskaloup/anaconda3/envs/torchenv/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [28:54<00:00,  2.10s/it]\n",
      "2025-02-28 01:43:17 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:30<00:00,  1.98s/it]\n",
      "2025-02-28 01:46:55 - INFO - Using default tokenizer.\n",
      "2025-02-28 01:46:57 - INFO - Epoch 1/10 - Train Loss: 0.4219 - Valid Loss: 0.0717\n",
      "2025-02-28 01:46:57 - INFO - Epoch 1/10 - Train ROUGE-L: 0.4425 - Valid ROUGE-L: 0.7164\n",
      "2025-02-28 01:46:59 - INFO - Model saved at epoch 1 with ROUGE-L: 0.7164\n",
      "2025-02-28 01:46:59 - INFO - Epoch: 2\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [31:15<00:00,  2.27s/it]\n",
      "2025-02-28 02:18:14 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:28<00:00,  1.97s/it]\n",
      "2025-02-28 02:21:52 - INFO - Using default tokenizer.\n",
      "2025-02-28 02:21:54 - INFO - Epoch 2/10 - Train Loss: 0.0817 - Valid Loss: 0.0595\n",
      "2025-02-28 02:21:54 - INFO - Epoch 2/10 - Train ROUGE-L: 0.7160 - Valid ROUGE-L: 0.7817\n",
      "2025-02-28 02:21:56 - INFO - Model saved at epoch 2 with ROUGE-L: 0.7817\n",
      "2025-02-28 02:21:56 - INFO - Epoch: 3\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [30:25<00:00,  2.21s/it]\n",
      "2025-02-28 02:52:22 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [04:06<00:00,  2.33s/it]\n",
      "2025-02-28 02:56:38 - INFO - Using default tokenizer.\n",
      "2025-02-28 02:56:40 - INFO - Epoch 3/10 - Train Loss: 0.0635 - Valid Loss: 0.0544\n",
      "2025-02-28 02:56:40 - INFO - Epoch 3/10 - Train ROUGE-L: 0.7902 - Valid ROUGE-L: 0.7982\n",
      "2025-02-28 02:56:42 - INFO - Model saved at epoch 3 with ROUGE-L: 0.7982\n",
      "2025-02-28 02:56:42 - INFO - Epoch: 4\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [30:18<00:00,  2.20s/it]\n",
      "2025-02-28 03:27:01 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:04<00:00,  1.74s/it]\n",
      "2025-02-28 03:30:15 - INFO - Using default tokenizer.\n",
      "2025-02-28 03:30:16 - INFO - Epoch 4/10 - Train Loss: 0.0514 - Valid Loss: 0.0527\n",
      "2025-02-28 03:30:16 - INFO - Epoch 4/10 - Train ROUGE-L: 0.8149 - Valid ROUGE-L: 0.8224\n",
      "2025-02-28 03:30:19 - INFO - Model saved at epoch 4 with ROUGE-L: 0.8224\n",
      "2025-02-28 03:30:19 - INFO - Epoch: 5\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [29:48<00:00,  2.16s/it]\n",
      "2025-02-28 04:00:07 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:35<00:00,  2.03s/it]\n",
      "2025-02-28 04:03:52 - INFO - Using default tokenizer.\n",
      "2025-02-28 04:03:53 - INFO - Epoch 5/10 - Train Loss: 0.0425 - Valid Loss: 0.0527\n",
      "2025-02-28 04:03:53 - INFO - Epoch 5/10 - Train ROUGE-L: 0.8235 - Valid ROUGE-L: 0.8268\n",
      "2025-02-28 04:03:55 - INFO - Model saved at epoch 5 with ROUGE-L: 0.8268\n",
      "2025-02-28 04:03:55 - INFO - Epoch: 6\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [28:58<00:00,  2.10s/it]\n",
      "2025-02-28 04:32:54 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:33<00:00,  2.01s/it]\n",
      "2025-02-28 04:36:37 - INFO - Using default tokenizer.\n",
      "2025-02-28 04:36:38 - INFO - Epoch 6/10 - Train Loss: 0.0357 - Valid Loss: 0.0527\n",
      "2025-02-28 04:36:38 - INFO - Epoch 6/10 - Train ROUGE-L: 0.8344 - Valid ROUGE-L: 0.8233\n",
      "2025-02-28 04:36:38 - INFO - Epoch: 7\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [28:51<00:00,  2.09s/it]\n",
      "2025-02-28 05:05:29 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:41<00:00,  2.09s/it]\n",
      "2025-02-28 05:09:20 - INFO - Using default tokenizer.\n",
      "2025-02-28 05:09:21 - INFO - Epoch 7/10 - Train Loss: 0.0299 - Valid Loss: 0.0522\n",
      "2025-02-28 05:09:21 - INFO - Epoch 7/10 - Train ROUGE-L: 0.8418 - Valid ROUGE-L: 0.8229\n",
      "2025-02-28 05:09:21 - INFO - Epoch: 8\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [27:55<00:00,  2.03s/it]\n",
      "2025-02-28 05:37:16 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [02:51<00:00,  1.62s/it]\n",
      "2025-02-28 05:40:17 - INFO - Using default tokenizer.\n",
      "2025-02-28 05:40:18 - INFO - Epoch 8/10 - Train Loss: 0.0255 - Valid Loss: 0.0548\n",
      "2025-02-28 05:40:18 - INFO - Epoch 8/10 - Train ROUGE-L: 0.8492 - Valid ROUGE-L: 0.8363\n",
      "2025-02-28 05:40:21 - INFO - Model saved at epoch 8 with ROUGE-L: 0.8363\n",
      "2025-02-28 05:40:21 - INFO - Epoch: 9\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [26:56<00:00,  1.95s/it]\n",
      "2025-02-28 06:07:18 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:27<00:00,  1.96s/it]\n",
      "2025-02-28 06:10:54 - INFO - Using default tokenizer.\n",
      "2025-02-28 06:10:56 - INFO - Epoch 9/10 - Train Loss: 0.0224 - Valid Loss: 0.0556\n",
      "2025-02-28 06:10:56 - INFO - Epoch 9/10 - Train ROUGE-L: 0.8553 - Valid ROUGE-L: 0.8304\n",
      "2025-02-28 06:10:56 - INFO - Epoch: 10\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 827/827 [25:45<00:00,  1.87s/it]\n",
      "2025-02-28 06:36:41 - INFO - Using default tokenizer.\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [03:22<00:00,  1.91s/it]\n",
      "2025-02-28 06:40:12 - INFO - Using default tokenizer.\n",
      "2025-02-28 06:40:14 - INFO - Epoch 10/10 - Train Loss: 0.0205 - Valid Loss: 0.0554\n",
      "2025-02-28 06:40:14 - INFO - Epoch 10/10 - Train ROUGE-L: 0.8609 - Valid ROUGE-L: 0.8324\n",
      "2025-02-28 06:40:14 - INFO - Training completed in 19551 seconds.\n"
     ]
    }
   ],
   "source": [
    "if FINE_TUNE:\n",
    "    ## Training Loop\n",
    "    # Early Stopping and Checkpointing Setup\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    best_val_rouge = -1\n",
    "    best_epoch = -1\n",
    "    no_improvement_counter = 0\n",
    "    \n",
    "    # Initialize lists for tracking loss and ROUGE scores\n",
    "    train_loss_per_epoch = []\n",
    "    val_loss_per_epoch = []\n",
    "    train_rouge_per_epoch = []\n",
    "    val_rouge_per_epoch = []\n",
    "    \n",
    "    # Start Training\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    logger.info(\"Starting training...\")\n",
    "    \n",
    "    for epoch_num in range(num_epochs):\n",
    "        logger.info(f'Epoch: {epoch_num + 1}')\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_preds = []\n",
    "        total_labels = []\n",
    "    \n",
    "        for step_num, batch_data in enumerate(tqdm(train_loader, desc='Training')):\n",
    "            input_ids, attention_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss.mean()\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "    \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions and actual labels for ROUGE\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                preds = model.module.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "            else:\n",
    "                preds = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "            total_preds.extend(decoded_preds)\n",
    "            total_labels.extend(decoded_labels)\n",
    "    \n",
    "        # Compute average training loss\n",
    "        train_loss_per_epoch.append(train_loss / len(train_loader))\n",
    "    \n",
    "        # Compute ROUGE for training set\n",
    "        train_rouge_scores = rouge_metric.compute(predictions=total_preds, references=total_labels, use_stemmer=True)\n",
    "        # Check if ROUGE score is a scalar (float) or a detailed dictionary\n",
    "        if isinstance(train_rouge_scores[\"rougeL\"], dict):\n",
    "            avg_train_rouge = train_rouge_scores[\"rougeL\"].mid.fmeasure #* 100\n",
    "        else:\n",
    "            avg_train_rouge = train_rouge_scores[\"rougeL\"] #* 100  # For scalar case\n",
    "        train_rouge_per_epoch.append(avg_train_rouge)\n",
    "    \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for step_num_e, batch_data in enumerate(tqdm(val_loader, desc='Validation')):\n",
    "                input_ids, attention_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.mean().item()\n",
    "                \n",
    "                # Collect predictions and actual labels for ROUGE\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    preds = model.module.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "                else:\n",
    "                    preds = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "                val_preds.extend(decoded_preds)\n",
    "                val_labels.extend(decoded_labels)\n",
    "    \n",
    "        # Compute average validation loss\n",
    "        val_loss_per_epoch.append(val_loss / len(val_loader))\n",
    "    \n",
    "        # Compute ROUGE for validation set\n",
    "        val_rouge_scores = rouge_metric.compute(predictions=val_preds, references=val_labels, use_stemmer=True)\n",
    "        # Check if ROUGE score is a scalar (float) or a detailed dictionary\n",
    "        if isinstance(val_rouge_scores[\"rougeL\"], dict):\n",
    "            avg_val_rouge = val_rouge_scores[\"rougeL\"].mid.fmeasure #* 100\n",
    "        else:\n",
    "            avg_val_rouge = val_rouge_scores[\"rougeL\"] #* 100  # For scalar case\n",
    "        val_rouge_per_epoch.append(avg_val_rouge)\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch_num + 1}/{num_epochs} - Train Loss: {train_loss_per_epoch[-1]:.4f} - Valid Loss: {val_loss_per_epoch[-1]:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch_num + 1}/{num_epochs} - Train ROUGE-L: {avg_train_rouge:.4f} - Valid ROUGE-L: {avg_val_rouge:.4f}\")\n",
    "    \n",
    "        # Implement Early Stopping and Save Best Model\n",
    "        if avg_val_rouge > best_val_rouge:\n",
    "            best_val_rouge = avg_val_rouge\n",
    "            best_epoch = epoch_num + 1\n",
    "            no_improvement_counter = 0\n",
    "    \n",
    "            # Save the best model\n",
    "            #torch.save(model.state_dict(), save_path)\n",
    "            save_checkpoint(save_path, epoch_num+1, model, optimizer.state_dict(), lr_scheduler.state_dict(), train_loss_per_epoch, val_loss_per_epoch, train_rouge_per_epoch, val_rouge_per_epoch)\n",
    "            logger.info(f\"Model saved at epoch {epoch_num + 1} with ROUGE-L: {best_val_rouge:.4f}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "    \n",
    "            if no_improvement_counter >= patience:\n",
    "                logger.info(f\"Early stopping after {epoch_num + 1} epochs. Best epoch: {best_epoch} with ROUGE-L: {best_val_rouge:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # Training Complete\n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    logger.info(f\"Training completed in {(milli_sec2 - milli_sec1) // 1000} seconds.\")\n",
    "    \n",
    "    # Plotting Loss and ROUGE Scores\n",
    "    epochs = range(1, len(train_loss_per_epoch) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss_per_epoch, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_per_epoch, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('losses.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # ROUGE-L plot\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_rouge_per_epoch, label='Training ROUGE-L')\n",
    "    plt.plot(epochs, val_rouge_per_epoch, label='Validation ROUGE-L')\n",
    "    plt.title('Training and Validation ROUGE-L Scores')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('ROUGE-L')\n",
    "    plt.legend()\n",
    "    plt.savefig('rouge_scores.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c22ea-cd9b-4f51-8087-87b5584929ab",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99cba82a-f4d4-47d4-a173-45a30fd2011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1597708/3599033038.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32100, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32100, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32100, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model from checkpoint during training with early stopping\n",
    "\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "# If model is wrapped in DataParallel, load state_dict directly into the underlying model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b622f8dd-3427-4803-a9ca-a4d46df01516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 06:40:15 - INFO - Starting testing...\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [02:24<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed after 144.56322193145752\n",
      "Perception time per sample: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing set\n",
    "logger.info(\"Starting testing...\")\n",
    "test_start_time = time.time()\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_loader, desc='Testing')):\n",
    "        input_ids, attention_mask, labels = [data.to(device) for data in batch_data]\n",
    "\n",
    "        # Generate predictions\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            outputs = model.module.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "\n",
    "        # Decode predicted sequences and actual labels\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        test_preds.extend(decoded_preds)\n",
    "        actual_labels.extend(decoded_labels)\n",
    "\n",
    "test_end_time = time.time()\n",
    "testing_time = test_end_time - test_start_time\n",
    "\n",
    "# Display the total testing time and average time per sample\n",
    "print(\"Testing completed after\", testing_time)\n",
    "print(\"Perception time per sample:\", int(testing_time / len(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0805934f-2a71-4298-9979-6415d9dc3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 06:42:40 - INFO - Using default tokenizer.\n",
      "2025-02-28 06:42:41 - INFO - ROUGE scores on test data: {'rouge1': 0.8354423545109672, 'rouge2': 0.8098408913911646, 'rougeL': 0.8335366037504727, 'rougeLsum': 0.8345724472632983}\n",
      "2025-02-28 06:42:41 - INFO - ROUGE-1: 0.8354\n",
      "2025-02-28 06:42:41 - INFO - ROUGE-2: 0.8098\n",
      "2025-02-28 06:42:41 - INFO - ROUGE-L: 0.8335\n",
      "2025-02-28 06:42:41 - INFO - ROUGE-Lsum: 0.8346\n"
     ]
    }
   ],
   "source": [
    "def extract_rouge_value(rouge_scores, rouge_key):\n",
    "    score = rouge_scores[rouge_key]\n",
    "    if isinstance(score, dict):  # If it's a dictionary, extract the 'mid' fmeasure\n",
    "        return score['mid'].fmeasure #* 100\n",
    "    else:  # If it's a float, return the value directly\n",
    "        return score #* 100\n",
    "        \n",
    "# Compute evaluation metrics using ROUGE\n",
    "rouge_metric = load(\"rouge\")\n",
    "# Calculate ROUGE scores for the predictions and actual sequences\n",
    "rouge_scores = rouge_metric.compute(predictions=test_preds, references=actual_labels, use_stemmer=True)\n",
    "logger.info(f\"ROUGE scores on test data: {rouge_scores}\")\n",
    "\n",
    "# Display detailed scores for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "rouge1_score = extract_rouge_value(rouge_scores, 'rouge1')\n",
    "rouge2_score = extract_rouge_value(rouge_scores, 'rouge2')\n",
    "rougeL_score = extract_rouge_value(rouge_scores, 'rougeL')\n",
    "rougeLsum_score = extract_rouge_value(rouge_scores, 'rougeLsum')\n",
    "# Log the extracted ROUGE scores\n",
    "logger.info(f\"ROUGE-1: {rouge1_score:.4f}\")\n",
    "logger.info(f\"ROUGE-2: {rouge2_score:.4f}\")\n",
    "logger.info(f\"ROUGE-L: {rougeL_score:.4f}\")\n",
    "logger.info(f\"ROUGE-Lsum: {rougeLsum_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d874643-4288-4814-91c4-37545f844d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the source code, predictions, and true labels into a single file for further analysis\n",
    "with open('test_results.txt', 'w', encoding='utf-8') as f:\n",
    "    for code, pred, label in zip(test_data['Text'], test_preds, actual_labels):\n",
    "        f.write(f\"Source Code:\\n{code}\\n{'-'*50}\\n\")\n",
    "        f.write(f\"Actual Vulnerable Lines:\\n{label}\\n{'='*50}\\n\\n\")\n",
    "        f.write(f\"Predicted Vulnerable Lines:\\n{pred}\\n{'-'*50}\\n\")\n",
    "\n",
    "# Save the source code, predictions, and actual labels into an Excel file\n",
    "results_df = pd.DataFrame({\n",
    "    'Source Code': test_data['Text'],\n",
    "    'Actual Vulnerable Lines': actual_labels,\n",
    "    'Predicted Vulnerable Lines': test_preds\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel('test_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58ec01-e44c-4c4c-876a-16cb9dca06d2",
   "metadata": {},
   "source": [
    "Generating Vulnerable Lines (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f92069a5-b922-4141-b927-326ff9b475a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Vulnerable Lines:\n",
      "    int x = 5;\n",
      "    int y = x / 0; // Division by zero\n",
      "    char data[10];\n",
      "    strcpy(data, input); // Buffer overflow\n",
      "    delete[] data; // Potential double free\n"
     ]
    }
   ],
   "source": [
    "# Function to generate vulnerable lines for a code snippet\n",
    "def generate_vulnerable_lines(model, tokenizer, code_snippet, max_length):\n",
    "    # Tokenize the input code snippet\n",
    "    inputs = tokenizer(\n",
    "        code_snippet,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',  # You can adjust padding as needed\n",
    "        max_length=512  # The max length for the input code snippet\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate predicted vulnerable lines using the model\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        outputs = model.module.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,  # Maximum length for the generated sequence (vulnerable lines)\n",
    "            num_beams=4,  # Beam search for better results (you can adjust or remove for greedy search)\n",
    "            early_stopping=True  # Stop generating once the model reaches an end token\n",
    "        )\n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,  # Maximum length for the generated sequence (vulnerable lines)\n",
    "            num_beams=4,  # Beam search for better results (you can adjust or remove for greedy search)\n",
    "            early_stopping=True  # Stop generating once the model reaches an end token\n",
    "        )\n",
    "\n",
    "    vulnerable_lines = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return vulnerable_lines\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Example usage with a code snippet\n",
    "code_snippet = \"\"\"void testFunction() {\n",
    "    int x = 5;\n",
    "    int y = x / 0; // Division by zero\n",
    "    char data[10];\n",
    "    strcpy(data, input); // Buffer overflow\n",
    "    delete[] data; // Potential double free\n",
    "}\"\"\"\n",
    "\n",
    "# Generate vulnerable lines for the example code snippet\n",
    "predicted_vulnerable_lines = generate_vulnerable_lines(model, tokenizer, code_snippet, max_len_lines)\n",
    "\n",
    "# Output the result\n",
    "print(\"Predicted Vulnerable Lines:\")\n",
    "print(predicted_vulnerable_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94e8ff6d-1036-4cc1-a741-161174bd1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Test Sample Code Snippet:\n",
      "static void ext2_put_super (struct super_block * sb)\n",
      "{\n",
      "int db_count;\n",
      "int i;\n",
      "struct ext2_sb_info *sbi = EXT2_SB(sb);\n",
      "\n",
      "dquot_disable(sb, -1, DQUOT_USAGE_ENABLED | DQUOT_LIMITS_ENABLED);\n",
      "\n",
      "\text2_xattr_put_super(sb);\n",
      "if (!(sb->s_flags & MS_RDONLY)) {\n",
      "struct ext2_super_block *es = sbi->s_es;\n",
      "\n",
      "spin_lock(&sbi->s_lock);\n",
      "es->s_state = cpu_to_le16(sbi->s_mount_state);\n",
      "spin_unlock(&sbi->s_lock);\n",
      "ext2_sync_super(sb, es, 1);\n",
      "}\n",
      "db_count = sbi->s_gdb_count;\n",
      "for (i = 0; i < db_count; i++)\n",
      "if (sbi->s_group_desc[i])\n",
      "brelse (sbi->s_group_desc[i]);\n",
      "kfree(sbi->s_group_desc);\n",
      "kfree(sbi->s_debts);\n",
      "percpu_counter_destroy(&sbi->s_freeblocks_counter);\n",
      "percpu_counter_destroy(&sbi->s_freeinodes_counter);\n",
      "percpu_counter_destroy(&sbi->s_dirs_counter);\n",
      "brelse (sbi->s_sbh);\n",
      "sb->s_fs_info = NULL;\n",
      "kfree(sbi->s_blockgroup_lock);\n",
      "kfree(sbi);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming your test dataset is loaded into a DataFrame called 'test_data'\n",
    "# Use the first sample from the test set (column 'processed_func' contains the source code)\n",
    "no_sample = 20 # 100\n",
    "first_code_snippet = test_data['Text'].iloc[no_sample]\n",
    "\n",
    "# Generate vulnerable lines for the first sample\n",
    "predicted_vulnerable_lines = generate_vulnerable_lines(model, tokenizer, first_code_snippet, max_len_lines)\n",
    "\n",
    "# Output the result\n",
    "print(\"First Test Sample Code Snippet:\")\n",
    "print(first_code_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a66649fa-9db6-43f7-82a8-445cc6098769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Actual Vulnerable Lines:\n",
      "\text2_xattr_put_super(sb);\n"
     ]
    }
   ],
   "source": [
    "print(\"\\Actual Vulnerable Lines:\")\n",
    "print(test_data['Lines'].iloc[no_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3da845-8418-48e9-881f-abe638a70116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Vulnerable Lines:\n",
      "\text2_xattr_put_super(sb);\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredicted Vulnerable Lines:\")\n",
    "print(predicted_vulnerable_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad2329-8bf7-490b-a4b2-b86062aa4cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
