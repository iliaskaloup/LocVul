{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f242e380-4f83-4b67-a372-3e996b0a26b9",
   "metadata": {},
   "source": [
    "<b> Function-level Vulnerability Prediction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790d4a5e-333d-45c1-b705-4125b7dd1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForSequenceClassification #, BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff757a7e-4217-4ec4-b392-4141732c7bf2",
   "metadata": {},
   "source": [
    "Basic Configuration of logging and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4247d39b-4c56-45f8-8461-af7681a30cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 16:25:14 - INFO - SEED: 680\n"
     ]
    }
   ],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# Define logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Specify a constant seeder for processes\n",
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 135, 680]\n",
    "seed = seeders[9]\n",
    "logger.info(f\"SEED: {seed}\")\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77d19a-acdc-4057-9c67-da10dfd0f569",
   "metadata": {},
   "source": [
    "Read data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9c5978-fa5d-4d51-a3dd-16ca629df015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "root_path = os.getcwd()\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd7fbd3-5361-406c-9b2d-8751a777792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint and fine-tuning logic\n",
    "FINE_TUNE = True  # Set this to False if you don't want to fine-tune the model and load from checkpoint\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58a2399-7db2-45c0-8d48-4498dfad9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def save_checkpoint(filename, epoch, model, optimizer, scheduler, train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch):\n",
    "    # If model is wrapped in DataParallel, save the underlying model's state_dict\n",
    "    model_state_dict = model.module.state_dict() if torch.cuda.device_count() > 1 else model.state_dict()\n",
    "    \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model_state_dict,  # Use the correct state_dict\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_per_epoch': train_loss_per_epoch,\n",
    "        'val_loss_per_epoch': val_loss_per_epoch,\n",
    "        'train_f1_per_epoch': train_f1_per_epoch,\n",
    "        'val_f1_per_epoch': val_f1_per_epoch\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    max_row = 0\n",
    "    \n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Convert input_ids to a PyTorch tensor\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = torch.sum(input_ids_tensor != tokenizer.pad_token_id).item()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    logger.info(f\"Max length of tokenized data: {max_length}\")\n",
    "    logger.info(f\"Row with max length:: {max_row}\")\n",
    "    \n",
    "    return max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb7fc4-384f-465e-b358-6b23bcc3d0cd",
   "metadata": {},
   "source": [
    "Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3654fc9-1342-4ef1-9837-c65dcbe12789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliaskaloup/anaconda3/envs/torchenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained tokenizer\n",
    "model_variation = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#bert-base-uncased #bert-base # roberta-base # distilbert-base-uncased #distilbert-base # microsoft/codebert-base-mlm\n",
    "# 'albert-base-v2'\n",
    "\n",
    "# tokenizer = RobertaTokenizer(vocab_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-vocab.json\",\n",
    "#                              merges_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-merges.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa460112-47f0-4e19-8c52-e81b43c06bfe",
   "metadata": {},
   "source": [
    "Split data sets and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93626f40-516b-4531-8310-9b387ebd02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 16:25:53 - INFO - List of projects in BigVul: ['openssl' 'linux' 'Chrome' 'poppler' 'libpcap' 'gpac' 'libarchive'\n",
      " 'suricata' 'libexpat' 'mbedtls' 'unixODBC' 'libreport' 'keepalived'\n",
      " 'Android' 'radare2' 'savannah' 'lynx-snapshots' 'libming' 'php' 'qemu'\n",
      " 'tor' 'uncurl' 'ghostscript' 'ImageMagick' 'memcached' 'samba' 'miniupnp'\n",
      " 'FreeRDP' 'OpenSC' 'wireshark' 'ImageMagick6' 'FFmpeg' 'minisphere'\n",
      " 'jasper' 'OpenJK' 'Onigmo' 'imageworsener' 'gst-plugins-ugly'\n",
      " 'php-radius' 'polarssl' 'libssh' 'spice' 'libgd' 'mruby' 'neomutt' 'dbus'\n",
      " 'ovs' 'libimobiledevice' 'gnupg' 'oniguruma' 'mod_auth_openidc'\n",
      " 'nautilus' 'ppp' 'tcpdump' 'wolfssl' 'raptor' 'VeraCrypt' 'udisks' 'exim'\n",
      " 'kde' 'harfbuzz' 'curl' 'redis' 'h2o' 'lxc' 'libtiff' 'wayland'\n",
      " 'htcondor' 'mindrot' 'irssi' 'abrt' 'php-src' 'ntp' 'rdesktop' 'uzbl'\n",
      " 'mujs' 'profanity' 'file' 'jansson' 'cyrus-imapd' 'Espruino' 'postgres'\n",
      " 'heimdal' 'w3m' 'u-boot' 'netfilter' 'zstd' 'gstreamer' 'axtls-8266'\n",
      " 'launchpad' 'leptonica' 'postgresql' 'shibboleth' 'httpd' 'openvpn'\n",
      " 'xserver' 'krb5' 'libXtst' 'mapserver' 'busybox' 'gnulib' 'ippusbxd'\n",
      " 'nspluginwrapper' 'Little-CMS' 'systemd' 'exempi' 'tartarus'\n",
      " 'tcmu-runner' 'yara' 'kvm-guest-drivers-windows' 'accountsservice'\n",
      " 'mod_auth_mellon' 'weechat' 'openmpt' 'beanstalkd' 'haproxy' 'libgcrypt'\n",
      " 'libmspack' 'git' 'libgit2' 'slurm' 'squashfs-tools' 'src' 'jq'\n",
      " 'pengutronix' 'lxde' 'libx11' 'iperf' 'ext-http' 'zlib' 'libndp'\n",
      " 'libvirt' 'libcomps' 'texlive-source' 'virglrenderer' 'libxml2'\n",
      " 'pacemaker' 'json-c' 'cgminer' 'collectd' 'libav' 'NetworkManager'\n",
      " 'libXrandr' 'libdwarf' 'libXrender' 'rufus' 'sgminer' 'yubico-pam'\n",
      " 'cJSON' 'proftpd' 'fontconfig' 'libevent' 'didiwiki' 'lxcfs' 'openafs'\n",
      " 'ngiflib' 'libxsmm' 'gnome-session' 'libsndfile' 'evince' 'libxfont'\n",
      " 'bubblewrap' 'wildmidi' 'domoticz' 'optee_os' 'capstone' 'gpmf-parser'\n",
      " 'openbsd' 'WavPack' 'ssdp-responder' 'libxkbcommon' 'libreswan' 'stb'\n",
      " 'cups' 'openjpeg' 'opa-ff' 'viabtc_exchange_server' 'dosfstools'\n",
      " 'civetweb' 'libpng' 'util-linux' 'libetpan' 'faad2'\n",
      " 'LibRaw-demosaic-pack-GPL2' 'MAC-Telnet' 'illumos-gate' 'mongo-c-driver'\n",
      " 'acpica' 'Varnish-Cache' 'atheme' 'firejail' 'libplist' 'libXv' 'flatpak'\n",
      " 'hexchat' 'nmap' 'liblouis' 'jabberd2' 'uriparser' 'frr' 'lighttpd1.4'\n",
      " 'irssi-proxy' 'pdfresurrect' 'nbd' 'altlinux' 'netdata' 'strongswan'\n",
      " 'media-tree' 'libu2f-host' 'pgbouncer' 'matio' 'sthttpd' 'rawstudio'\n",
      " 'pigz' 'libvips' 'pam_p11' 'zfs' 'openssh-portable' 'bdwgc' 'libinfinity'\n",
      " 'enlightment' 'libass' 'libXfixes' 'mosquitto' 'libuv' 'libgsf'\n",
      " 'mongoose-os' 'feh' 'pango' 'pixman' 'libzip' 'xrdb' 'libmodbus'\n",
      " 'corosync' 'tnef' 'sleuthkit' 'monkey' 'mongoose' 'quagga' 'polkit'\n",
      " 'lhasa' 'ioq3' 'pngquant' 'mstdlib' 'libjpeg-turbo' 'nfdump' 'libfep'\n",
      " 'aircrack-ng' 'suhosin' 'PDFGen' 'pam-u2f' 't1utils' 'knc' 'libXpm'\n",
      " 'infradead' 'webserver' 'pupnp-code' 'rpm' 'libXvMC' 'das_watchdog'\n",
      " 'lysator' 'linux-nfs' 'ettercap' 'moodle' 'nefarious2' 'varnish-cache'\n",
      " 'tinc' 'xcursor' 'boa' 'musl' 'bzrtp' 'libevt' 'unrealircd' 'nagioscore'\n",
      " 'libtomcrypt' 'bitlbee' 'quassel' 'picocom' 'fontforge' 'gifsicle'\n",
      " 'jerryscript' 'radvd' 'tpm2.0-tools' 'libXi' 'rsyslog' 'thor' 'librsvg'\n",
      " 'kamailio' 'libXdmcp' 'shadowsocks-libev' 'tcpreplay' 'wpitchoune'\n",
      " '3proxy' 'hylafax' 'libbsd' 'pure-ftpd' 'exfat' 'gimp' 'libmysofa'\n",
      " 'Openswan' 'proxychains-ng' 'charybdis' 'libICE' 'pyfribidi' 'drm'\n",
      " 'uwsgi' 'nedmalloc' 'torque' 'yodl' 'libx12']\n",
      "2025-02-27 16:25:53 - INFO - Number of different projects in BigVul: 310\n",
      "2025-02-27 16:25:53 - INFO - Top-10 largest projects in BigVul and their size: project\n",
      "Chrome         77173\n",
      "linux          46855\n",
      "Android         8691\n",
      "qemu            3096\n",
      "php             2709\n",
      "ImageMagick     2520\n",
      "savannah        2176\n",
      "FFmpeg          1932\n",
      "ghostscript     1867\n",
      "openssl         1860\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View the largest projects\n",
    "\n",
    "logger.info(f\"List of projects in BigVul: {dataset['project'].unique()}\")\n",
    "logger.info(f\"Number of different projects in BigVul: {len(dataset['project'].unique())}\")\n",
    "\n",
    "project_counts = dataset['project'].value_counts().nlargest(10)\n",
    "logger.info(f\"Top-10 largest projects in BigVul and their size: {project_counts}\")\n",
    "\n",
    "# Choose the selected project to include in the test set.\n",
    "# default = \"all\"\n",
    "selected_project = \"all\" # all # Chrome # linux # Android # qemu # php # ImageMagick # savannah # FFmpeg # ghostscript # openssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a443b3f-5090-4e92-a717-77dfeb71db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 16:25:53 - INFO - Train data head:     index Access Gained Attack Origin Authentication Required Availability  \\\n",
      "0   48018           NaN         Local            Not required      Partial   \n",
      "1  177287           NaN        Remote            Not required     Complete   \n",
      "2  174089           NaN        Remote            Not required          NaN   \n",
      "3   31021           NaN         Local            Not required          NaN   \n",
      "4  120877           NaN        Remote            Not required          NaN   \n",
      "\n",
      "          CVE ID                                       CVE Page   CWE ID  \\\n",
      "0  CVE-2016-9588  https://www.cvedetails.com/cve/CVE-2016-9588/  CWE-388   \n",
      "1  CVE-2016-2476  https://www.cvedetails.com/cve/CVE-2016-2476/  CWE-119   \n",
      "2  CVE-2016-2460  https://www.cvedetails.com/cve/CVE-2016-2460/  CWE-200   \n",
      "3  CVE-2013-2635  https://www.cvedetails.com/cve/CVE-2013-2635/  CWE-399   \n",
      "4  CVE-2013-2879  https://www.cvedetails.com/cve/CVE-2013-2879/  CWE-200   \n",
      "\n",
      "  Complexity Confidentiality  ... parentID  \\\n",
      "0        Low             NaN  ...      NaN   \n",
      "1     Medium        Complete  ...      NaN   \n",
      "2     Medium         Partial  ...      NaN   \n",
      "3     Medium         Partial  ...      NaN   \n",
      "4     Medium         Partial  ...      NaN   \n",
      "\n",
      "                                               patch  project  \\\n",
      "0  @@ -1389,10 +1389,10 @@ static inline bool nes...    linux   \n",
      "1  @@ -2418,6 +2418,7 @@\\n\\n             : OMX_AU...  Android   \n",
      "2  @@ -349,7 +349,7 @@\\n\\n         }\\n         ca...  Android   \n",
      "3  @@ -979,6 +979,7 @@ static int rtnl_fill_ifinf...    linux   \n",
      "4  @@ -48,10 +48,12 @@ OneClickSigninSyncStarter:...   Chrome   \n",
      "\n",
      "                                       project_after  \\\n",
      "0           ef85b67385436ddc1998f45f1d6a210f935b3388   \n",
      "1  https://android.googlesource.com/platform/fram...   \n",
      "2  https://android.googlesource.com/platform/fram...   \n",
      "3           84d73cd3fb142bf1298a8c13fd4ca50fd2432372   \n",
      "4           afbc71b7a78ac99810a6b22b2b0a2e85dde18794   \n",
      "\n",
      "                                      project_before target  \\\n",
      "0           cc0d907c0907561f108b2f4d4da24e85f18d0ca5      0   \n",
      "1  https://android.googlesource.com/platform/fram...      0   \n",
      "2  https://android.googlesource.com/platform/fram...      0   \n",
      "3           c085c49920b2f900ba716b4ca1c1a55ece9872cc      0   \n",
      "4           1e46f230bd00678488d5b4fce546e965a00ba16b      0   \n",
      "\n",
      "                                   vul_func_with_fix  \\\n",
      "0  static inline bool fixed_bits_valid(u64 val, u...   \n",
      "1  void ACodec::onSignalEndOfInputStream() {\\n   ...   \n",
      "2   virtual status_t setMaxAcquiredBufferCount(in...   \n",
      "3  static void rtnetlink_rcv(struct sk_buff *skb)...   \n",
      "4  void OneClickSigninSyncStarter::SigninDialogDe...   \n",
      "\n",
      "                                      processed_func flaw_line flaw_line_index  \n",
      "0  static inline bool fixed_bits_valid(u64 val, u...       NaN             NaN  \n",
      "1  void ACodec::onSignalEndOfInputStream() {\\n   ...       NaN             NaN  \n",
      "2   virtual status_t setMaxAcquiredBufferCount(in...       NaN             NaN  \n",
      "3  static void rtnetlink_rcv(struct sk_buff *skb)...       NaN             NaN  \n",
      "4  void OneClickSigninSyncStarter::SigninDialogDe...       NaN             NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "2025-02-27 16:25:53 - INFO - Length of training data: 150910\n",
      "2025-02-27 16:25:53 - INFO - Maximum number of words: 15441\n",
      "2025-02-27 16:25:53 - INFO - Value counts of training data: target\n",
      "0    142174\n",
      "1      8736\n",
      "Name: count, dtype: int64\n",
      "2025-02-27 16:25:53 - INFO - Percentages of classes: (6.1445833978083195, '%')\n",
      "2025-02-27 16:25:53 - INFO - Number of categories: 2\n",
      "2025-02-27 16:25:53 - INFO - Train data length: 150910\n",
      "2025-02-27 16:25:53 - INFO - Validation data length: 18863\n",
      "2025-02-27 16:25:53 - INFO - Test data length: 18863\n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "val_ratio = 0.1\n",
    "num_of_ratio = int(val_ratio * len(dataset))\n",
    "data = dataset.iloc[0:-num_of_ratio, :]\n",
    "test_data = dataset.iloc[-num_of_ratio:, :]\n",
    "train_data = data.iloc[0:-num_of_ratio, :]\n",
    "val_data = data.iloc[-num_of_ratio:, :]\n",
    "\n",
    "# if selected_project==\"all\" continue with the whole test_set, else, if one specific project is selected keep only its samples\n",
    "if selected_project != \"all\":\n",
    "    test_data = test_data[test_data['project'] == selected_project]\n",
    "\n",
    "# Shuffle dataset\n",
    "\n",
    "train_data = train_data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "logger.info(f\"Train data head: {train_data.head()}\")\n",
    "logger.info(f\"Length of training data: {len(train_data)}\")\n",
    "\n",
    "\n",
    "#train_data = train_data[train_data[\"project\"] != \"Chrome\"]\n",
    "#logger.info(f\"Length of training data without Chromium: {len(train_data)}\")\n",
    "\n",
    "\n",
    "train_data = train_data[[\"processed_func\", \"target\", \"flaw_line\", \"flaw_line_index\"]]\n",
    "train_data.head()\n",
    "\n",
    "\n",
    "# Explore data\n",
    "\n",
    "train_data = train_data.dropna(subset=[\"processed_func\"])\n",
    "\n",
    "\n",
    "word_counts = train_data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "logger.info(f\"Maximum number of words: {max_length}\")\n",
    "\n",
    "\n",
    "vc = train_data[\"target\"].value_counts()\n",
    "\n",
    "logger.info(f\"Value counts of training data: {vc}\")\n",
    "\n",
    "logger.info(f\"Percentages of classes: {(vc[1] / vc[0])*100, '%'}\")\n",
    "\n",
    "n_categories = len(vc)\n",
    "logger.info(f\"Number of categories: {n_categories}\")\n",
    "\n",
    "train_data = pd.DataFrame(({'Text': train_data['processed_func'], 'Labels': train_data['target'], 'Lines':train_data['flaw_line'], 'Line_Index':train_data['flaw_line_index']}))\n",
    "#train_data = train_data[0:100]\n",
    "train_data.head()\n",
    "\n",
    "\n",
    "#val_data = val_data[val_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Labels': val_data['target'], 'Lines':val_data['flaw_line'], 'Line_Index':val_data['flaw_line_index']}))\n",
    "val_data.head()\n",
    "\n",
    "\n",
    "#test_data = test_data[test_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Labels': test_data['target'], 'Lines':test_data['flaw_line'], 'Line_Index':test_data['flaw_line_index']}))\n",
    "\n",
    "logger.info(f\"Train data length: {len(train_data)}\")\n",
    "logger.info(f\"Validation data length: {len(val_data)}\")\n",
    "logger.info(f\"Test data length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94617d59-161c-428f-85de-a42017958d06",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c4b6e6-04ff-468b-a35b-f37725007fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing step: Under-sampling\n",
    "\n",
    "sampling = False\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 4 * class_counts[class_counts.idxmin()] # int(class_counts[class_counts.idxmax()] / 2) # 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2)\n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}\n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"])\n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ac6b4-d9e5-4c48-acb9-2ad523e53611",
   "metadata": {},
   "source": [
    "Get model and apply tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9ec1f7-015f-4237-8ef3-00e5585e26f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "# Resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# # Compute maximum length\n",
    "\n",
    "# X = tokenizer(\n",
    "#         text=X_train.tolist(),\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         return_tensors='pt',\n",
    "#         return_token_type_ids=False,\n",
    "#         return_attention_mask=True,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "# max_len = getMaxLen(X)\n",
    "max_len = 512\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "X_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "X_val = tokenizer(\n",
    "    text=val_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=test_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7663d1-65f3-4df5-90c3-a092baee09ce",
   "metadata": {},
   "source": [
    "Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1822aa0b-aa59-4f2e-af64-fce608afe257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 2e-5 #5e-05\n",
    "batch_size = 8 #16\n",
    "patience = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd97b659-9aef-4c4b-a25c-aec65e41d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 16:26:27 - INFO - Device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): RobertaForSequenceClassification(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No. of trainable parameters:  124647170\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "\n",
    "Y_train = torch.LongTensor(Y_train.tolist())\n",
    "Y_val = torch.LongTensor(val_data[\"Labels\"].tolist())\n",
    "Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "Y_train.size(), Y_val.size(), Y_test.size()\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], Y_train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val[\"input_ids\"], X_val[\"attention_mask\"], Y_val)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "max_steps = len(train_dataloader)*n_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "            num_warmup_steps=max_steps // 5,\n",
    "            num_training_steps=max_steps)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# total_steps = len(train_dataloader) * n_epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py\n",
    "#                                             num_training_steps = total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Device {device}\")\n",
    "\n",
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d129fa73-681d-4de7-a810-41d161ab157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we do not retrain our pre-trained BERT and train only the last linear dense layer\n",
    "# for param in model.roberta.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a77e5-9484-459d-bc9e-52dccb8f2d29",
   "metadata": {},
   "source": [
    "Training loop - Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a43d22-7cfa-4799-b711-ef7a939a4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 16:26:27 - INFO - Fine-tuning model: microsoft/codebert-base\n",
      "2025-02-27 16:26:27 - INFO - Starting training...\n",
      "2025-02-27 16:26:27 - INFO - Epoch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:30<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:47<00:00, 21.85it/s]\n",
      "2025-02-27 17:09:46 - INFO - Epoch 1/10 - Train Loss: 0.1140 - Valid Loss: 0.0765\n",
      "2025-02-27 17:09:46 - INFO - Epoch 1/10 - Train F1: 0.7500 - Valid F1: 0.8647\n",
      "2025-02-27 17:09:48 - INFO - Model saved at epoch 1\n",
      "2025-02-27 17:09:48 - INFO - Epoch: 2\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:28<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:47<00:00, 21.86it/s]\n",
      "2025-02-27 17:53:05 - INFO - Epoch 2/10 - Train Loss: 0.0723 - Valid Loss: 0.0715\n",
      "2025-02-27 17:53:05 - INFO - Epoch 2/10 - Train F1: 0.8691 - Valid F1: 0.8774\n",
      "2025-02-27 17:53:06 - INFO - Model saved at epoch 2\n",
      "2025-02-27 17:53:06 - INFO - Epoch: 3\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:29<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:47<00:00, 21.84it/s]\n",
      "2025-02-27 18:36:24 - INFO - Epoch 3/10 - Train Loss: 0.0630 - Valid Loss: 0.0617\n",
      "2025-02-27 18:36:24 - INFO - Epoch 3/10 - Train F1: 0.8858 - Valid F1: 0.8913\n",
      "2025-02-27 18:36:25 - INFO - Model saved at epoch 3\n",
      "2025-02-27 18:36:25 - INFO - Epoch: 4\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:29<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.83it/s]\n",
      "2025-02-27 19:19:43 - INFO - Epoch 4/10 - Train Loss: 0.0534 - Valid Loss: 0.0606\n",
      "2025-02-27 19:19:43 - INFO - Epoch 4/10 - Train F1: 0.9061 - Valid F1: 0.9042\n",
      "2025-02-27 19:19:44 - INFO - Model saved at epoch 4\n",
      "2025-02-27 19:19:44 - INFO - Epoch: 5\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:29<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:47<00:00, 21.85it/s]\n",
      "2025-02-27 20:03:02 - INFO - Epoch 5/10 - Train Loss: 0.0449 - Valid Loss: 0.0601\n",
      "2025-02-27 20:03:02 - INFO - Epoch 5/10 - Train F1: 0.9220 - Valid F1: 0.9128\n",
      "2025-02-27 20:03:04 - INFO - Model saved at epoch 5\n",
      "2025-02-27 20:03:04 - INFO - Epoch: 6\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:28<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.81it/s]\n",
      "2025-02-27 20:46:21 - INFO - Epoch 6/10 - Train Loss: 0.0379 - Valid Loss: 0.0664\n",
      "2025-02-27 20:46:21 - INFO - Epoch 6/10 - Train F1: 0.9345 - Valid F1: 0.9100\n",
      "2025-02-27 20:46:21 - INFO - Epoch: 7\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:28<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.82it/s]\n",
      "2025-02-27 21:29:39 - INFO - Epoch 7/10 - Train Loss: 0.0292 - Valid Loss: 0.0804\n",
      "2025-02-27 21:29:39 - INFO - Epoch 7/10 - Train F1: 0.9507 - Valid F1: 0.8986\n",
      "2025-02-27 21:29:39 - INFO - Epoch: 8\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:29<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:47<00:00, 21.83it/s]\n",
      "2025-02-27 22:12:57 - INFO - Epoch 8/10 - Train Loss: 0.0213 - Valid Loss: 0.0709\n",
      "2025-02-27 22:12:57 - INFO - Epoch 8/10 - Train F1: 0.9628 - Valid F1: 0.9147\n",
      "2025-02-27 22:12:58 - INFO - Model saved at epoch 8\n",
      "2025-02-27 22:12:58 - INFO - Epoch: 9\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:29<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.80it/s]\n",
      "2025-02-27 22:56:16 - INFO - Epoch 9/10 - Train Loss: 0.0165 - Valid Loss: 0.0709\n",
      "2025-02-27 22:56:16 - INFO - Epoch 9/10 - Train F1: 0.9688 - Valid F1: 0.9056\n",
      "2025-02-27 22:56:16 - INFO - Epoch: 10\n",
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18864/18864 [41:28<00:00,  7.58it/s]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.82it/s]\n",
      "2025-02-27 23:39:34 - INFO - Epoch 10/10 - Train Loss: 0.0115 - Valid Loss: 0.0814\n",
      "2025-02-27 23:39:34 - INFO - Epoch 10/10 - Train F1: 0.9791 - Valid F1: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is completed after 25986401\n"
     ]
    }
   ],
   "source": [
    "if not FINE_TUNE and os.path.exists(save_path):\n",
    "    pass\n",
    "else:\n",
    "    logger.info(f\"Fine-tuning model: {model_variation}\")\n",
    "    # Train model\n",
    "    \n",
    "    # Initialize values for implementing Callbacks\n",
    "    ## Early Stopping\n",
    "    best_val_f1 = -1\n",
    "    best_epoch = -1\n",
    "    no_improvement_counter = 0\n",
    "    ## Save best - optimal checkpointing\n",
    "    #checkpoint_dir = './checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    #save_path = os.path.join(checkpoint_dir, 'best_weights.pt')\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    train_loss_per_epoch = []\n",
    "    val_loss_per_epoch = []\n",
    "    train_f1_per_epoch = []\n",
    "    val_f1_per_epoch = []\n",
    "    \n",
    "    for epoch_num in range(n_epochs):\n",
    "        logger.info(f'Epoch: {epoch_num + 1}')\n",
    "    \n",
    "        #Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_preds = []\n",
    "        total_labels = []\n",
    "        for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "    \n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "            # clear previously calculated gradients\n",
    "            model.zero_grad() # optimizer.zero_grad()\n",
    "    \n",
    "            # get model predictions for the current batch\n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "    \n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "            # add on to the total loss\n",
    "            train_loss += loss.item()\n",
    "    \n",
    "            # backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "    \n",
    "            # clip the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "    \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "            # Print training loss after each batch\n",
    "            #print(\"Epoch {}/{} - Batch {}/{} - Training Loss: {:.4f}\".format(epoch_num+1, n_epochs, step_num+1, len(train_dataloader), loss.item()))\n",
    "    \n",
    "            # model predictions are stored on GPU. So, push it to CPU\n",
    "            preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "            # append the model predictions\n",
    "            total_preds+=list(preds)\n",
    "            total_labels+=labels.cpu().numpy().tolist()\n",
    "    \n",
    "        train_loss_per_epoch.append(train_loss / len(train_dataloader))\n",
    "        train_accuracy=accuracy_score(total_labels, total_preds)\n",
    "        if n_categories > 2:\n",
    "            train_precision=precision_score(total_labels, total_preds, average='macro')\n",
    "            train_recall=recall_score(total_labels, total_preds, average='macro')\n",
    "            train_f1=f1_score(total_labels, total_preds, average='macro')\n",
    "        else:\n",
    "            train_precision=precision_score(total_labels, total_preds)\n",
    "            train_recall=recall_score(total_labels, total_preds)\n",
    "            train_f1=f1_score(total_labels, total_preds)\n",
    "            train_roc_auc=roc_auc_score(total_labels, total_preds)\n",
    "        train_f2 = (5*train_precision*train_recall) / (4*train_precision+train_recall)\n",
    "    \n",
    "        #Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_pred = []\n",
    "        actual_labels = []\n",
    "        with torch.no_grad():\n",
    "            for step_num_e, batch_data in enumerate(tqdm(val_dataloader, desc='Validation')):\n",
    "                input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "                output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "    \n",
    "                preds = np.argmax(output.logits.cpu().detach().numpy(), axis=-1)\n",
    "                valid_pred+=list(preds)\n",
    "                actual_labels+=labels.cpu().numpy().tolist()\n",
    "    \n",
    "                loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "                valid_loss += loss.item()\n",
    "    \n",
    "        val_loss_per_epoch.append(valid_loss / len(val_dataloader))\n",
    "        val_accuracy=accuracy_score(actual_labels, valid_pred)\n",
    "        if n_categories > 2:\n",
    "            val_precision=precision_score(actual_labels, valid_pred, average='macro')\n",
    "            val_recall=recall_score(actual_labels, valid_pred, average='macro')\n",
    "            val_f1=f1_score(actual_labels, valid_pred, average='macro')\n",
    "        else:\n",
    "            val_precision=precision_score(actual_labels, valid_pred)\n",
    "            val_recall=recall_score(actual_labels, valid_pred)\n",
    "            val_f1=f1_score(actual_labels, valid_pred)\n",
    "            val_roc_auc=roc_auc_score(actual_labels, valid_pred)\n",
    "        val_f2 = (5*val_precision*val_recall) / (4*val_precision+val_recall)\n",
    "    \n",
    "        #print(\"Epoch {}/{} - Train Loss: {:.4f} - Valid Loss: {:.4f}\".format(epoch_num+1, n_epochs, train_loss_per_epoch[-1], val_loss_per_epoch[-1]))\n",
    "        #print(\"Epoch {}/{} - Train F1: {:.4f} - Valid F1: {:.4f}\".format(epoch_num+1, n_epochs, train_f1, val_f1))\n",
    "        logger.info(f\"Epoch {epoch_num + 1}/{n_epochs} - Train Loss: {train_loss_per_epoch[-1]:.4f} - Valid Loss: {val_loss_per_epoch[-1]:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch_num + 1}/{n_epochs} - Train F1: {train_f1:.4f} - Valid F1: {val_f1:.4f}\")\n",
    "\n",
    "    \n",
    "        train_f1_per_epoch.append(train_f1)\n",
    "        val_f1_per_epoch.append(val_f1)\n",
    "    \n",
    "        total_epochs = epoch_num + 1\n",
    "        # Implement Callbacks: Early Stopping and save best\n",
    "        # Check if the validation F1 score has improved\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_epoch = epoch_num + 1\n",
    "            no_improvement_counter = 0 # Reset the counter\n",
    "    \n",
    "            # Save the best model checkpoint\n",
    "            save_checkpoint(save_path, epoch_num+1, model, optimizer.state_dict(), scheduler.state_dict(), train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch)\n",
    "            logger.info(f\"Model saved at epoch {epoch_num + 1}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "    \n",
    "            if no_improvement_counter >= patience:\n",
    "                # print(\"No improvement for\", patience, \"consecutive epochs.\")\n",
    "                # print(\"Early stopping after epoch No.\", total_epochs)\n",
    "                # print(\"Best model after epoch No\", best_epoch)\n",
    "                # print(\"Best achieved val_f1 = \", best_val_f1)\n",
    "                logger.info(f\"Early stopping after epoch {total_epochs}. Best epoch: {best_epoch} with best F1 score: {best_val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Training is completed after\", milli_sec2-milli_sec1)\n",
    "\n",
    "    epochs = range(1, total_epochs + 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, train_loss_per_epoch, label ='training loss')\n",
    "    ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "    ax.set_title('Training and Validation loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig('losses.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    epochs = range(1, total_epochs + 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, train_f1_per_epoch, label = 'training F1-score')\n",
    "    ax.plot(epochs, val_f1_per_epoch, label = 'validation F1-score')\n",
    "    ax.set_title('Training and Validation F1-scores')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('F1-score')\n",
    "    ax.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig('f-scores.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9c1a1-3a26-4fc8-9fce-e16dd2ee6749",
   "metadata": {},
   "source": [
    "Execution loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b2e396-7c55-4a9f-8d18-a2769c6f2556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1021129/1291369573.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path, map_location=device)\n",
      "2025-02-27 23:39:34 - INFO - Starting testing...\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2358/2358 [01:48<00:00, 21.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load best model from checkpoint during training with early stopping\n",
    "\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "# If model is wrapped in DataParallel, load state_dict directly into the underlying model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "logger.info(\"Starting testing...\")\n",
    "test_start_time = time.time()\n",
    "model.eval()\n",
    "test_pred = []\n",
    "test_probas_pred = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "\n",
    "        loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        logits_array = output.logits.cpu().detach().numpy()\n",
    "        #probs_array = softmax(logits_array, axis=1)\n",
    "        probs_array = torch.softmax(torch.tensor(logits_array), dim=-1).numpy()\n",
    "        \n",
    "        preds = np.argmax(probs_array , axis=-1)\n",
    "        test_pred+=list(preds)\n",
    "        actual_labels+=labels.cpu().numpy().tolist()\n",
    "\n",
    "        probas = np.max(probs_array , axis=1)\n",
    "        test_probas_pred+=list(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fc3ca-f25c-4437-a8d6-a8d99d4fb066",
   "metadata": {},
   "source": [
    "Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6616de44-73b6-4299-a2fe-04e3be4a6f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:41:23 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     17808\n",
      "           1       0.96      0.87      0.91      1055\n",
      "\n",
      "    accuracy                           0.99     18863\n",
      "   macro avg       0.97      0.93      0.95     18863\n",
      "weighted avg       0.99      0.99      0.99     18863\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing is completed after 108.55353713035583\n",
      "Perception time is 0\n",
      "Accuracy:99.03%\n",
      "Precision:95.61%\n",
      "Recall:86.64%\n",
      "F1 score:90.90%\n",
      "F2 score:88.29%\n",
      "Roc_Auc score:93.20%\n",
      "TP= 914\n",
      "TN= 17766\n",
      "FP= 42\n",
      "FN= 141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute evaluation metrics\n",
    "class_report = classification_report(actual_labels, test_pred)\n",
    "logger.info(f\"Classification Report:\\n{class_report}\")\n",
    "test_end_time = time.time()\n",
    "testing_time = test_end_time - test_start_time\n",
    "print(\"Testing is completed after\", testing_time)\n",
    "print(\"Perception time is\", int(testing_time/len(test_pred)))\n",
    "\n",
    "total_test_loss = test_loss/len(test_dataloader)\n",
    "accuracy=accuracy_score(actual_labels, test_pred)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(actual_labels, test_pred, average='macro')\n",
    "    recall=recall_score(actual_labels, test_pred, average='macro')\n",
    "    f1=f1_score(actual_labels, test_pred, average='macro')\n",
    "else:\n",
    "    precision=precision_score(actual_labels, test_pred)\n",
    "    recall=recall_score(actual_labels, test_pred)\n",
    "    f1=f1_score(actual_labels, test_pred)\n",
    "    roc_auc=roc_auc_score(actual_labels, test_pred)\n",
    "f2 = (5*precision*recall) / (4*precision+recall)\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "if roc_auc:\n",
    "    print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_labels, test_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "#acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "#print(conf_matrix)\n",
    "sn.heatmap(conf_matrix, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39fc0905-ec7d-4b28-8a2a-d36429d2f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9904310024916503, 'precision': 0.9561878360411281, 'recall': 0.8688151658767772, 'f1': 0.9103592081346781, 'f2': 0.8849587891258468, 'roc_auc': 0.9332255299285054}\n"
     ]
    }
   ],
   "source": [
    "# Export classification report\n",
    "\n",
    "method = \"forSequence\"\n",
    "\n",
    "\n",
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(seed))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{seed}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n",
    "\n",
    "\n",
    "# Compute the average values of the classication metrics considering the results for all different seeders\n",
    "\n",
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method)\n",
    "\n",
    "for root, dirs, files in os.walk(results_folder):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\") and filename != \"avg.csv\":\n",
    "            csv_file_path = os.path.join(root, filename)\n",
    "\n",
    "            with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "\n",
    "                for row in reader:\n",
    "                    for metric, value in row.items():\n",
    "                        cumulative_metrics[metric] += float(value)\n",
    "            count += 1\n",
    "\n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values\n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)\n",
    "\n",
    "# # Clean up\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272508e-1b6e-4fe6-8929-75b52ea96789",
   "metadata": {},
   "source": [
    "<b> Line-level Vulnerability Detection</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada0bc33-8292-4f08-99af-b3f994990d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import pipeline\n",
    "from captum.attr import DeepLiftShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9fee64d-4b04-4195-9e86-c6ed7854906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:41:27 - INFO - Starting testing...\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2325/2325 [01:46<00:00, 21.73it/s]\n",
      "2025-02-27 23:43:14 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17808\n",
      "           1       0.95      0.93      0.94       788\n",
      "\n",
      "    accuracy                           0.99     18596\n",
      "   macro avg       0.97      0.97      0.97     18596\n",
      "weighted avg       0.99      0.99      0.99     18596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminate Test samples that are vulnerable (target=1) but they have missing line-level labels (flaw lines is nan)\n",
    "REMOVE_MISSING_LINE_LABELS = True # True # False\n",
    "\n",
    "if REMOVE_MISSING_LINE_LABELS:\n",
    "\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    test_data = test_data[~((test_data['Labels'] == 1) & (test_data['Line_Index'].isna()))]\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "    \n",
    "    \n",
    "    X_test = tokenizer(\n",
    "        text=test_data['Text'].tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # Make new predictions\n",
    "    logger.info(\"Starting testing...\")\n",
    "    test_start_time = time.time()\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_probas_pred = []\n",
    "    actual_labels = []\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "    \n",
    "            loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            logits_array = output.logits.cpu().detach().numpy()\n",
    "            #probs_array = softmax(logits_array, axis=1)\n",
    "            probs_array = torch.softmax(torch.tensor(logits_array), dim=-1).numpy()\n",
    "            \n",
    "            preds = np.argmax(probs_array , axis=-1)\n",
    "            test_pred+=list(preds)\n",
    "            actual_labels+=labels.cpu().numpy().tolist()\n",
    "    \n",
    "            probas = np.max(probs_array , axis=1)\n",
    "            test_probas_pred+=list(probas)\n",
    "    \n",
    "    # compute evaluation metrics\n",
    "    new_class_report = classification_report(actual_labels, test_pred)\n",
    "    logger.info(f\"Classification Report:\\n{new_class_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7502601c-62ad-4465-8527-2b881de27086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the function into lines and tokens\n",
    "def tokenize_function_to_lines_and_tokens(function_code, split_char):\n",
    "    # Split function into lines based on newline characters\n",
    "    lines = function_code.split(split_char)\n",
    "    \n",
    "    # Tokenize each line\n",
    "    tokenized_lines = []\n",
    "    for line in lines:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        tokenized_lines.append(tokens)\n",
    "    \n",
    "    return lines, tokenized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93162d34-623f-45c9-8a4c-c76862053486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify negative predictions ie TN and FN\n",
    "negative_indices = [i for i, pred in enumerate(test_pred) if pred == 0]  # Indices of Negative predictions (TNs + FNs)\n",
    "\n",
    "# Collect lines of negative predictions\n",
    "negative_samples = [test_data['Text'].tolist()[i] for i in negative_indices]  # Extract Negative samples from test data\n",
    "\n",
    "# Flatten\n",
    "all_neg_lines = []\n",
    "for neg_func in negative_samples:\n",
    "    neg_lines, _ = tokenize_function_to_lines_and_tokens(neg_func, '\\n')\n",
    "    for neg_line in neg_lines:\n",
    "        all_neg_lines.append(neg_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a868ee-6013-466c-a4ac-e381ea540b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:43:29 - INFO - Initializing ATTENTION explainer for Positive predictions...\n"
     ]
    }
   ],
   "source": [
    "EXPLAINER = \"ATTENTION\"  # or \"LIME\" or \"DEEPLIFTSHAP\" or \"ATTENTION\" based on user choice\n",
    "logger.info(f\"Initializing {EXPLAINER} explainer for Positive predictions...\")\n",
    "\n",
    "EXPLAIN_ONLY_TP_Accuracy = True\n",
    "EXPLAIN_ONLY_TP_CostEffect = False\n",
    "\n",
    "EXPLAIN_ONLY_TP = EXPLAIN_ONLY_TP_CostEffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "676c8eb0-2c52-4721-8453-cc6c5364ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:43:29 - INFO - Generating explanations for 778 Positive predictions (TPs and FPs)...\n"
     ]
    }
   ],
   "source": [
    "# Identify True Positives (where the predicted label and actual label are both 1)\n",
    "true_positive_indices = [i for i, (pred, label) in enumerate(zip(test_pred, Y_test.tolist())) if pred == 1 and label == 1]\n",
    "if EXPLAIN_ONLY_TP:\n",
    "    positive_indices = true_positive_indices\n",
    "    logger.info(f\"Selected {len(true_positive_indices)} True Positives for explanations.\")\n",
    "else:\n",
    "    # Identify True Positives and False Positives\n",
    "    trueNfalse_positive_indices = [i for i, pred in enumerate(test_pred) if pred == 1]  # Indices of Positive predictions (TPs + FPs)\n",
    "    logger.info(f\"Generating explanations for {len(trueNfalse_positive_indices)} Positive predictions (TPs and FPs)...\")\n",
    "    positive_indices = trueNfalse_positive_indices\n",
    "\n",
    "actual_positive_indices = [i for i, label in enumerate(Y_test.tolist()) if label == 1]  # Indices of Actual Positive predictions (TPs + FNs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22906bcc-1ca9-439f-805b-5be9c014ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [test_data['Text'].tolist()[i] for i in positive_indices]  # Extract Positive samples from test data\n",
    "\n",
    "positive_probas = [test_probas_pred[i] for i in positive_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "476cf530-64b0-4c85-9522-4e48af05e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict probabilities for LIME\n",
    "def predict_proba_func_lime(texts):\n",
    "    model.eval()\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "        \n",
    "    probabilities = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0e700df-c4bb-4af1-9cd1-e2650e4b0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the explainer (LIME or SHAP)\n",
    "def initialize_explainer():\n",
    "    if EXPLAINER == \"LIME\":\n",
    "        model.to(device)\n",
    "        return LimeTextExplainer(class_names=['Non-Vulnerable', 'Vulnerable'], random_state=seed)\n",
    "    elif EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "        model.to(device)\n",
    "        return DeepLiftShap(model)  # Initialize DeepLiftShap with the model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown explainer: {EXPLAINER}\")\n",
    "\n",
    "# Initialize the explainer\n",
    "if EXPLAINER == \"LIME\" or EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "    explainer = initialize_explainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54c4a0d2-74fb-4d53-8ae7-7aa57c775ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute LIME values for each line by summing the token-level values\n",
    "def compute_lime_values_per_line(tokenized_lines, token_scores_dict):\n",
    "    line_lime_scores = []\n",
    "\n",
    "    # Iterate over tokenized lines\n",
    "    for tokens in tokenized_lines:\n",
    "        line_score = 0  # Initialize line score\n",
    "        for token in tokens:\n",
    "            # Retrieve the LIME score for the token if it exists\n",
    "            if token in token_scores_dict:\n",
    "                line_score += token_scores_dict[token]\n",
    "        \n",
    "        # Store the summed LIME score for the line\n",
    "        line_lime_scores.append(line_score)\n",
    "    \n",
    "    return line_lime_scores\n",
    "\n",
    "# Function to compute DeepLiftSHAP values for each line by summing the token-level values\n",
    "def compute_deepliftshap_values_per_line(tokenized_lines, token_attributions):\n",
    "    line_deepliftshap_scores = []\n",
    "\n",
    "    token_idx = 0\n",
    "    for tokens in tokenized_lines:\n",
    "        line_score = sum(token_attributions[token_idx:token_idx+len(tokens)])\n",
    "        line_deepliftshap_scores.append(line_score)\n",
    "        token_idx += len(tokens)\n",
    "    \n",
    "    return line_deepliftshap_scores\n",
    "\n",
    "# Function to compute attention values for each line\n",
    "def compute_attention_values_per_line(tokenized_lines, attention_scores):\n",
    "    line_scores = []\n",
    "    token_idx = 0  # Keeps track of the token index\n",
    "    \n",
    "    for tokens in tokenized_lines:\n",
    "        line_score = sum(attention_scores[token_idx:token_idx+len(tokens)])\n",
    "        line_scores.append(line_score)\n",
    "        token_idx += len(tokens)\n",
    "    \n",
    "    return line_scores\n",
    "\n",
    "# Function to clean special token values (<s>, </s>, padding)\n",
    "def clean_special_token_values(all_values, padding=True):\n",
    "    # Special token in the beginning of the sequence\n",
    "    all_values[0] = 0\n",
    "    if padding:\n",
    "        # Set the last non-zero value (representing the </s> token) to zero\n",
    "        idx = [index for index, item in enumerate(all_values) if item != 0][-1]\n",
    "        all_values[idx] = 0\n",
    "    else:\n",
    "        # Special token at the end of the sequence\n",
    "        all_values[-1] = 0\n",
    "    return all_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0aba1-ca4b-4215-baef-ca45cf3f89b6",
   "metadata": {},
   "source": [
    "XAI-based localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13817086-cb38-4957-aa73-a3c46dbd65a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 10/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 20/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 30/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 40/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 50/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 60/778\n",
      "2025-02-27 23:43:30 - INFO - Generating ATTENTION-based explanation for sample 70/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 80/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 90/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 100/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 110/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 120/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 130/778\n",
      "2025-02-27 23:43:31 - INFO - Generating ATTENTION-based explanation for sample 140/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 150/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 160/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 170/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 180/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 190/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 200/778\n",
      "2025-02-27 23:43:32 - INFO - Generating ATTENTION-based explanation for sample 210/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 220/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 230/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 240/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 250/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 260/778\n",
      "2025-02-27 23:43:33 - INFO - Generating ATTENTION-based explanation for sample 270/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 280/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 290/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 300/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 310/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 320/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 330/778\n",
      "2025-02-27 23:43:34 - INFO - Generating ATTENTION-based explanation for sample 340/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 350/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 360/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 370/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 380/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 390/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 400/778\n",
      "2025-02-27 23:43:35 - INFO - Generating ATTENTION-based explanation for sample 410/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 420/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 430/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 440/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 450/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 460/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 470/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 480/778\n",
      "2025-02-27 23:43:36 - INFO - Generating ATTENTION-based explanation for sample 490/778\n",
      "2025-02-27 23:43:37 - INFO - Generating ATTENTION-based explanation for sample 500/778\n",
      "2025-02-27 23:43:37 - INFO - Generating ATTENTION-based explanation for sample 510/778\n",
      "2025-02-27 23:43:37 - INFO - Generating ATTENTION-based explanation for sample 520/778\n",
      "2025-02-27 23:43:37 - INFO - Generating ATTENTION-based explanation for sample 530/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 540/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 550/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 560/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 570/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 580/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 590/778\n",
      "2025-02-27 23:43:38 - INFO - Generating ATTENTION-based explanation for sample 600/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 610/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 620/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 630/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 640/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 650/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 660/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 670/778\n",
      "2025-02-27 23:43:39 - INFO - Generating ATTENTION-based explanation for sample 680/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 690/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 700/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 710/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 720/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 730/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 740/778\n",
      "2025-02-27 23:43:40 - INFO - Generating ATTENTION-based explanation for sample 750/778\n",
      "2025-02-27 23:43:41 - INFO - Generating ATTENTION-based explanation for sample 760/778\n",
      "2025-02-27 23:43:41 - INFO - Generating ATTENTION-based explanation for sample 770/778\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the LIME explanations\n",
    "explanation_results = []\n",
    "# Loop through all positive samples (True Positives and False Positives)\n",
    "for i, sample in enumerate(positive_samples):\n",
    "\n",
    "    if EXPLAINER == \"LIME\":\n",
    "        # Print logs every 10 samples\n",
    "        if (i + 1) % 10 == 0:\n",
    "            logger.info(f\"Generating LIME explanation for sample {i + 1}/{len(positive_samples)}\")\n",
    "        \n",
    "        # Generate explanation using the LIME explainer\n",
    "        explanation = explainer.explain_instance(\n",
    "            sample,  # The text/code snippet to explain\n",
    "            predict_proba_func_lime,  # The function to predict probabilities\n",
    "            num_features=20,  # Number of features to include in the explanation\n",
    "            num_samples = 50,\n",
    "            labels=[1]  # Target class (Vulnerable)\n",
    "        )\n",
    "\n",
    "    elif EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "        # Print logs every 10 samples\n",
    "        if (i + 1) % 10 == 0:\n",
    "            logger.info(f\"Generating DEEPLIFTSHAP explanation for sample {i + 1}/{len(positive_samples)}\")\n",
    "        \n",
    "        # Generate explanation using the SHAP explainer\n",
    "        # Tokenize the function into lines and tokens\n",
    "        lines, tokenized_lines = tokenize_function_to_lines_and_tokens(sample, '\\n')\n",
    "\n",
    "        # Encode the sample (input) and get embeddings\n",
    "        encodings = tokenizer(sample, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "        # Get the embeddings for input ids using the model's embedding layer\n",
    "        input_embeddings = model.get_input_embeddings()(encodings['input_ids'])\n",
    "\n",
    "        # Compute DeepLiftSHAP values per token\n",
    "        num_baselines = 16\n",
    "        baseline_inputs = torch.zeros_like(input_embeddings).repeat((num_baselines, 1, 1)).to(device)\n",
    "\n",
    "        attributions = explainer.attribute(inputs=input_embeddings, baselines=baseline_inputs)\n",
    "\n",
    "        # Convert attributions to a list of token-level attributions\n",
    "        token_attributions = attributions.squeeze().tolist()\n",
    "\n",
    "        # Clean attributions for special tokens (e.g., <s>, </s>, padding)\n",
    "        token_attributions = clean_special_token_values(token_attributions, padding=True)\n",
    "\n",
    "        # Compute DeepLiftSHAP values per line\n",
    "        line_deepliftshap_scores = compute_deepliftshap_values_per_line(tokenized_lines, token_attributions)\n",
    "\n",
    "        # Create a list of tuples containing (line_index, line_text, deepliftshap_score)\n",
    "        explanation = [(line_idx, line, line_deepliftshap_scores[line_idx]) for line_idx, line in enumerate(lines)]\n",
    "\n",
    "    \n",
    "    elif EXPLAINER == \"ATTENTION\":\n",
    "        # Print logs every 10 samples\n",
    "        if (i + 1) % 10 == 0:\n",
    "            logger.info(f\"Generating ATTENTION-based explanation for sample {i + 1}/{len(positive_samples)}\")\n",
    "        \n",
    "        # Tokenize the function into lines and tokens\n",
    "        lines, tokenized_lines = tokenize_function_to_lines_and_tokens(sample, '\\n')\n",
    "        \n",
    "        # Get model predictions along with attention weights\n",
    "        with torch.no_grad():\n",
    "            encodings = tokenizer(sample, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(device)\n",
    "            outputs = model(**encodings, output_attentions=True)\n",
    "            logits = outputs.logits.cpu().detach().numpy()\n",
    "            attentions = outputs.attentions  # Attention weights from each layer \n",
    "\n",
    "        batch_attention = attentions[0][0]\n",
    "\n",
    "        # Summarize across heads by averaging the attention scores\n",
    "        attention_summary = torch.mean(batch_attention, dim=0).cpu().numpy()  # Average across heads\n",
    "\n",
    "        # Sum the attention each token receives from others (this gives a score for each token)\n",
    "        token_attention_scores = np.sum(attention_summary, axis=0)  # Shape: (sequence_length,)\n",
    "\n",
    "         # Clean attention scores for special tokens (e.g., <s>, </s>, padding)\n",
    "        token_attention_scores = clean_special_token_values(token_attention_scores, padding=True)\n",
    "\n",
    "        # Compute attention values per line\n",
    "        line_attention_scores = compute_attention_values_per_line(tokenized_lines, token_attention_scores)\n",
    "\n",
    "        # Create a list of tuples containing (line_index, line_text, attention_score)\n",
    "        explanation = [(line_idx, line, line_attention_scores[line_idx]) for line_idx, line in enumerate(lines)]\n",
    "    \n",
    "    explanation_results.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e169a80-b8f1-4547-93cd-c597c199d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 1:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 1:\n",
      "2025-02-27 23:43:41 - INFO - Line 40: buflen < (strlen(data->basedir) + 2 * data->dirdepth + key_len + 5 + sizeof(FILE_PREFIX))) { (Score: 37.42132389545441)\n",
      "2025-02-27 23:43:41 - INFO - Line 31: static char *ps_files_path_create(char *buf, size_t buflen, ps_files *data, const char *key) (Score: 32.90829384326935)\n",
      "2025-02-27 23:43:41 - INFO - Line 59:                if (!ps_files_valid_key(key)) { (Score: 29.730502367019653)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 11:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 11:\n",
      "2025-02-27 23:43:41 - INFO - Line 64: if ((x + i) < (ssize_t) dds_info->width && (y + j) < (ssize_t) dds_info->height) (Score: 41.40173959732056)\n",
      "2025-02-27 23:43:41 - INFO - Line 40:                          Min(4, dds_info->height - y),exception); (Score: 37.64850977063179)\n",
      "2025-02-27 23:43:41 - INFO - Line 39:       q = QueueAuthenticPixels(image, x, y, Min(4, dds_info->width - x), (Score: 32.05909126996994)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 21:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 21:\n",
      "2025-02-27 23:43:41 - INFO - Line 6: dquot_disable(sb, -1, DQUOT_USAGE_ENABLED | DQUOT_LIMITS_ENABLED); (Score: 31.94140338897705)\n",
      "2025-02-27 23:43:41 - INFO - Line 13: es->s_state = cpu_to_le16(sbi->s_mount_state); (Score: 21.658260762691498)\n",
      "2025-02-27 23:43:41 - INFO - Line 24: percpu_counter_destroy(&sbi->s_freeinodes_counter); (Score: 18.773584485054016)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 31:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 31:\n",
      "2025-02-27 23:43:41 - INFO - Line 12: upayload = kmalloc(sizeof(*upayload) + datalen, GFP_KERNEL); (Score: 26.97036451101303)\n",
      "2025-02-27 23:43:41 - INFO - Line 0: int user_update(struct key *key, struct key_preparsed_payload *prep) (Score: 19.728571474552155)\n",
      "2025-02-27 23:43:41 - INFO - Line 7: if (datalen <= 0 || datalen > 32767 || !prep->data) (Score: 18.61008983850479)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 41:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 41:\n",
      "2025-02-27 23:43:41 - INFO - Line 61: length=fwrite(\"\\002\\001\\003\\000\\001\\000\\000\\000\\001\\000\\000\\000\",1,12,file); (Score: 38.761734426021576)\n",
      "2025-02-27 23:43:41 - INFO - Line 56: length=fwrite(\"\\376\\000\\003\\000\\001\\000\\000\\000\\000\\000\\000\\000\",1,12,file); (Score: 37.753382325172424)\n",
      "2025-02-27 23:43:41 - INFO - Line 55: length=fwrite(\"\\111\\111\\052\\000\\010\\000\\000\\000\\016\\000\",1,10,file); (Score: 30.730446755886078)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 51:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 51:\n",
      "2025-02-27 23:43:41 - INFO - Line 20: if (!_XReply (dpy, (xReply *) &rep, 0, xFalse)) (Score: 21.2868190407753)\n",
      "2025-02-27 23:43:41 - INFO - Line 37: _XEatData (dpy, (unsigned long) (nbytes - nread)); (Score: 20.736103177070618)\n",
      "2025-02-27 23:43:41 - INFO - Line 33:     rects = Xmalloc (nrects * sizeof (XRectangle)); (Score: 19.969127237796783)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 61:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 61:\n",
      "2025-02-27 23:43:41 - INFO - Line 8: data[i+2] == 'J' && data[i+3] == 'F' && data[i+4] == 'I' && (Score: 31.487532019615173)\n",
      "2025-02-27 23:43:41 - INFO - Line 23:                 block_length = data[i] * 256 + data[i+1]; (Score: 30.697131007909775)\n",
      "2025-02-27 23:43:41 - INFO - Line 4: if (i + 3 < data_size && data[i] == 0xFF && data[i+1] == 0xD8 && (Score: 30.094503462314606)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 71:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 71:\n",
      "2025-02-27 23:43:41 - INFO - Line 22: const long long segment_stop = (m_size < 0) ? -1 : m_start + m_size; (Score: 25.984311282634735)\n",
      "2025-02-27 23:43:41 - INFO - Line 0: long Segment::ParseCues(long long off, long long& pos, long& len) { (Score: 21.494454503059387)\n",
      "2025-02-27 23:43:41 - INFO - Line 75: if ((segment_stop >= 0) && ((pos + len) > segment_stop)) (Score: 20.90166199207306)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 81:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 81:\n",
      "2025-02-27 23:43:41 - INFO - Line 21: if (setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void*)&yes, sizeof(int)) == -1) { (Score: 37.49340605735779)\n",
      "2025-02-27 23:43:41 - INFO - Line 16: if (0 > (sfd = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP))) { (Score: 28.330691933631897)\n",
      "2025-02-27 23:43:41 - INFO - Line 7: if (WSAStartup(MAKEWORD(2,2), &wsa_data) != ERROR_SUCCESS) { (Score: 27.723312735557556)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 91:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 91:\n",
      "2025-02-27 23:43:41 - INFO - Line 10:     p1 = BuildTestPacket(id, 0, 1, 'A', 8); (Score: 20.330718964338303)\n",
      "2025-02-27 23:43:41 - INFO - Line 16:     p3 = BuildTestPacket(id, 2, 0, 'C', 3); (Score: 19.5220964550972)\n",
      "2025-02-27 23:43:41 - INFO - Line 48: if (GET_PKT_DATA(reassembled)[i] != 'C') (Score: 19.077108144760132)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 101:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 101:\n",
      "2025-02-27 23:43:41 - INFO - Line 16:     if (fscanf(fp, \" Number of pts = %d; format = %s\\n\", &n, typestr) != 2) (Score: 33.809151738882065)\n",
      "2025-02-27 23:43:41 - INFO - Line 33: if (fscanf(fp, \"   (%d, %d)\\n\", &ix, &iy) != 2) { (Score: 29.878001242876053)\n",
      "2025-02-27 23:43:41 - INFO - Line 27: if (fscanf(fp, \"   (%f, %f)\\n\", &x, &y) != 2) { (Score: 28.156652361154556)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 111:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 111:\n",
      "2025-02-27 23:43:41 - INFO - Line 9: #if (defined(OS_CHROMEOS) && defined(ARCH_CPU_ARMEL)) || defined(OS_WIN) (Score: 26.156891584396362)\n",
      "2025-02-27 23:43:41 - INFO - Line 33: #endif  // defined(OS_CHROMEOS) && defined(ARCH_CPU_ARMEL) (Score: 23.148268222808838)\n",
      "2025-02-27 23:43:41 - INFO - Line 19:       new DXVAVideoDecodeAccelerator(this, renderer_process); (Score: 21.694617092609406)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 121:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 121:\n",
      "2025-02-27 23:43:41 - INFO - Line 16: else if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops) (Score: 32.65823882818222)\n",
      "2025-02-27 23:43:41 - INFO - Line 14: if ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops) (Score: 30.535962760448456)\n",
      "2025-02-27 23:43:41 - INFO - Line 31: err = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER); (Score: 24.918177843093872)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 131:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 131:\n",
      "2025-02-27 23:43:41 - INFO - Line 26: /* Decode Trailing Ones                                              */ (Score: 49.4986292719841)\n",
      "2025-02-27 23:43:41 - INFO - Line 17: ps_tu_4x4 = (tu_sblk4x4_coeff_data_t *)ps_dec->pv_parse_tu_coeff_data; (Score: 42.436198353767395)\n",
      "2025-02-27 23:43:41 - INFO - Line 0: WORD32 ih264d_cavlc_4x4res_block_totalcoeff_2to10(UWORD32 u4_isdc, (Score: 36.13160842657089)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 141:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 141:\n",
      "2025-02-27 23:43:41 - INFO - Line 46: if( vips_foreign_load_gif_code_next( gif, &extension ) ) (Score: 22.51840990781784)\n",
      "2025-02-27 23:43:41 - INFO - Line 25: vips_error( class->nickname, \"%s\", _( \"bad frame size\" ) ); (Score: 21.871735632419586)\n",
      "2025-02-27 23:43:41 - INFO - Line 36: if( map->Colors[i].Red != map->Colors[i].Green || (Score: 20.584651291370392)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 151:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 151:\n",
      "2025-02-27 23:43:41 - INFO - Line 9: if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, \"ss|l\", (char **)&haystack, &haystack_len, (char **)&needle, &needle_len, &loffset) == FAILURE) { (Score: 67.52369731664658)\n",
      "2025-02-27 23:43:41 - INFO - Line 30: intl_error_set( NULL, U_ILLEGAL_ARGUMENT_ERROR, \"grapheme_strpos: Empty delimiter\", 1 TSRMLS_CC ); (Score: 46.40722531080246)\n",
      "2025-02-27 23:43:41 - INFO - Line 19: intl_error_set( NULL, U_ILLEGAL_ARGUMENT_ERROR, \"grapheme_strpos: Offset not contained in string\", 1 TSRMLS_CC ); (Score: 44.819960594177246)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 161:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 161:\n",
      "2025-02-27 23:43:41 - INFO - Line 4: \tcleanup_srcu_struct(&user->release_barrier); (Score: 17.211524069309235)\n",
      "2025-02-27 23:43:41 - INFO - Line 0: int ipmi_destroy_user(struct ipmi_user *user) (Score: 13.755705058574677)\n",
      "2025-02-27 23:43:41 - INFO - Line 5: kref_put(&user->refcount, free_user); (Score: 13.612696766853333)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 171:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 171:\n",
      "2025-02-27 23:43:41 - INFO - Line 2:   if (client && client->GetTextInputType() != TEXT_INPUT_TYPE_NONE) (Score: 21.621240496635437)\n",
      "2025-02-27 23:43:41 - INFO - Line 0: void InputMethodBase::OnInputMethodChanged() const { (Score: 10.77948921918869)\n",
      "2025-02-27 23:43:41 - INFO - Line 1: TextInputClient* client = GetTextInputClient(); (Score: 10.575849652290344)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 181:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 181:\n",
      "2025-02-27 23:43:41 - INFO - Line 0: void comps_objmrtree_unite(COMPS_ObjMRTree *rt1, COMPS_ObjMRTree *rt2) { (Score: 30.94678694009781)\n",
      "2025-02-27 23:43:41 - INFO - Line 39: sizeof(char)*(strlen(((COMPS_ObjMRTreeData*)it->data)->key)+1)); (Score: 27.906545042991638)\n",
      "2025-02-27 23:43:41 - INFO - Line 28: pair->subnodes = ((COMPS_ObjMRTreeData*)it->data)->subnodes; (Score: 25.492578864097595)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 191:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 191:\n",
      "2025-02-27 23:43:41 - INFO - Line 21:                                                        context_handle, (Score: 50.61738193035126)\n",
      "2025-02-27 23:43:41 - INFO - Line 22:                                                        output_token); (Score: 39.40596666932106)\n",
      "2025-02-27 23:43:41 - INFO - Line 14:         iakerb_ctx_id_t iakerb_ctx = (iakerb_ctx_id_t)*context_handle; (Score: 39.11523884534836)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 201:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 201:\n",
      "2025-02-27 23:43:41 - INFO - Line 0: InputImeEventRouter* GetInputImeEventRouter(Profile* profile) { (Score: 18.997711718082428)\n",
      "2025-02-27 23:43:41 - INFO - Line 3: return extensions::InputImeEventRouterFactory::GetInstance()->GetRouter( (Score: 18.976873874664307)\n",
      "2025-02-27 23:43:41 - INFO - Line 4:       profile->GetOriginalProfile()); (Score: 8.09262329339981)\n",
      "2025-02-27 23:43:41 - INFO - Explanation for Positive Sample 211:\n",
      "2025-02-27 23:43:41 - INFO - Ranked lines for Positive Sample 211:\n",
      "2025-02-27 23:43:41 - INFO - Line 34: \"CV:H:h:F:i:np:qRr:s:T:+t:+SA:+O:*ox:*fB\" (Score: 37.898401737213135)\n",
      "2025-02-27 23:43:41 - INFO - Line 39: \"\\0\" IF_UDHCP_VERBOSE(\"vv\") /* -v is a counter */ (Score: 24.924565076828003)\n",
      "2025-02-27 23:43:41 - INFO - Line 42: , &client_config.interface, &client_config.pidfile /* i,p */ (Score: 21.76199471950531)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 221:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 221:\n",
      "2025-02-27 23:43:42 - INFO - Line 17: if (entry.Get(syncable::IS_DEL) && entry.Get(syncable::SERVER_IS_DEL)) { (Score: 29.104976296424866)\n",
      "2025-02-27 23:43:42 - INFO - Line 28: // By the time we get to this point, we rely on the following to be true: (Score: 19.68824702501297)\n",
      "2025-02-27 23:43:42 - INFO - Line 31: // b) All unsynced changes have been re-encrypted with the default key ( (Score: 19.30074393749237)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 231:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 231:\n",
      "2025-02-27 23:43:42 - INFO - Line 42: s->msg_callback(0, 0, SSL3_RT_HEADER, p, DTLS1_RT_HEADER_LENGTH, s, s->msg_callback_arg); (Score: 42.53815454244614)\n",
      "2025-02-27 23:43:42 - INFO - Line 26: n=ssl3_read_n(s, DTLS1_RT_HEADER_LENGTH, s->s3->rbuf.len, 0); (Score: 33.659605741500854)\n",
      "2025-02-27 23:43:42 - INFO - Line 31: if (s->packet_length != DTLS1_RT_HEADER_LENGTH) (Score: 22.79609441757202)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 241:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 241:\n",
      "2025-02-27 23:43:42 - INFO - Line 39: \t  \"depth %d; enctype %d; siz %d; hres %d; vres %d; numcolors %d; \" (Score: 34.79152771830559)\n",
      "2025-02-27 23:43:42 - INFO - Line 28: \"BMP header: magic 0x%x; siz %d; res1 %d; res2 %d; off %d\\n\", (Score: 32.00920921564102)\n",
      "2025-02-27 23:43:42 - INFO - Line 40: \t  \"mincolors %d\\n\", info->len, info->width, info->height, info->numplanes, (Score: 27.77525159716606)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 251:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 251:\n",
      "2025-02-27 23:43:42 - INFO - Line 1: int n_ks_tuple, krb5_key_salt_tuple *ks_tuple, char **passptr) (Score: 27.94337296485901)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: check_1_6_dummy(kadm5_principal_ent_t entry, long mask, (Score: 23.333136022090912)\n",
      "2025-02-27 23:43:42 - INFO - Line 8: !(entry->attributes & KRB5_KDB_DISALLOW_ALL_TIX)) (Score: 22.091297686100006)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 261:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 261:\n",
      "2025-02-27 23:43:42 - INFO - Line 22: if (! TIFFGetFieldDefaulted(in, TIFFTAG_PHOTOMETRIC, &input_photometric)) (Score: 29.923957645893097)\n",
      "2025-02-27 23:43:42 - INFO - Line 19: TIFFGetFieldDefaulted(in, TIFFTAG_SAMPLESPERPIXEL, &spp); (Score: 27.598115980625153)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: loadImage(TIFF* in, struct image_data *image, struct dump_opts *dump, unsigned char **read_ptr) (Score: 26.953230381011963)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 271:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 271:\n",
      "2025-02-27 23:43:42 - INFO - Line 0: void FramerVisitorCapturingAcks::OnAckFrame(const QuicAckFrame& frame) { (Score: 24.129883408546448)\n",
      "2025-02-27 23:43:42 - INFO - Line 1:    frame_ = frame; (Score: 5.288467466831207)\n",
      "2025-02-27 23:43:42 - INFO - Line 2:  } (Score: 0.8922314643859863)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 281:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 281:\n",
      "2025-02-27 23:43:42 - INFO - Line 10: sql::Statement statement(db_->GetCachedStatement(SQL_FROM_HERE, kSql)); (Score: 24.106009751558304)\n",
      "2025-02-27 23:43:42 - INFO - Line 7:       \"SELECT cache_id, url, flags, response_id, response_size FROM Entries\" (Score: 22.95255681872368)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: bool AppCacheDatabase::FindEntriesForCache(int64_t cache_id, (Score: 17.75708419084549)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 291:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 291:\n",
      "2025-02-27 23:43:42 - INFO - Line 0: static inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas) { (Score: 31.32772672176361)\n",
      "2025-02-27 23:43:42 - INFO - Line 47: *   \" \" + flags + \" \" + data length + \"\\r\\n\" + data (with \\r\\n) (Score: 31.08774423599243)\n",
      "2025-02-27 23:43:42 - INFO - Line 23:             it = item_get(key, nkey, c, DO_UPDATE); (Score: 28.390540331602097)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 301:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 301:\n",
      "2025-02-27 23:43:42 - INFO - Line 1:                                  const char* event_name) { (Score: 25.665545761585236)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: bool IsTraceEventArgsWhitelisted(const char* category_group_name, (Score: 21.968969762325287)\n",
      "2025-02-27 23:43:42 - INFO - Line 2: if (base::MatchPattern(category_group_name, \"toplevel\") && (Score: 21.342640042304993)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 311:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 311:\n",
      "2025-02-27 23:43:42 - INFO - Line 13: UWORD8 u1_field_pic_flag, u1_redundant_pic_cnt = 0, u1_slice_type; (Score: 32.335245192050934)\n",
      "2025-02-27 23:43:42 - INFO - Line 33: u2_first_mb_in_slice = ih264d_uev(pu4_bitstrm_ofst, (Score: 30.422212839126587)\n",
      "2025-02-27 23:43:42 - INFO - Line 19: UWORD32 *pu4_bitstrm_ofst = &ps_bitstrm->u4_ofst; (Score: 28.143784523010254)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 321:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 321:\n",
      "2025-02-27 23:43:42 - INFO - Line 6:   // should never call back into the host anyway, so it is safe to tell them (Score: 18.67454496026039)\n",
      "2025-02-27 23:43:42 - INFO - Line 5:   // |instance_map_|. If the instance was deleted, observers for those instances (Score: 18.55368009209633)\n",
      "2025-02-27 23:43:42 - INFO - Line 4:   // because we won't have the opportunity to once we remove them from the (Score: 17.962503165006638)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 331:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 331:\n",
      "2025-02-27 23:43:42 - INFO - Line 19: \"NO-PROPOSAL-CHOSEN\",\t\t\"BAD-PROPOSAL-SYNTAX\", (Score: 29.925703644752502)\n",
      "2025-02-27 23:43:42 - INFO - Line 26: \"CERTIFICATE-UNAVAILABLE\",\t\"UNSUPPORTED-EXCHANGE-TYPE\", (Score: 27.99992197751999)\n",
      "2025-02-27 23:43:42 - INFO - Line 23: \"INVALID-CERT-AUTHORITY\",\t\"INVALID-HASH-INFORMATION\", (Score: 25.90362560749054)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 341:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 341:\n",
      "2025-02-27 23:43:42 - INFO - Line 0: cJSON *cJSON_DetachItemFromArray( cJSON *array, int which ) (Score: 18.612677931785583)\n",
      "2025-02-27 23:43:42 - INFO - Line 11: \tif ( c->next ) c->next->prev = c->prev; (Score: 16.756567358970642)\n",
      "2025-02-27 23:43:42 - INFO - Line 10: \t\tc->prev->next = c->next; (Score: 11.893192887306213)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 351:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 351:\n",
      "2025-02-27 23:43:42 - INFO - Line 2:                              base::Unretained(this))) {} (Score: 27.802168518304825)\n",
      "2025-02-27 23:43:42 - INFO - Line 1:       : callback_(base::Bind(&TestResultCallback::SetResult, (Score: 21.360523343086243)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: TestResultCallback() (Score: 4.176393747329712)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 361:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 361:\n",
      "2025-02-27 23:43:42 - INFO - Line 4: if (t1->tv_usec > t2->tv_usec) return +1; (Score: 20.863970935344696)\n",
      "2025-02-27 23:43:42 - INFO - Line 2: if (t1->tv_sec > t2->tv_sec) return +1; (Score: 20.71005469560623)\n",
      "2025-02-27 23:43:42 - INFO - Line 5: if (t1->tv_usec < t2->tv_usec) return -1; (Score: 20.35426127910614)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 371:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 371:\n",
      "2025-02-27 23:43:42 - INFO - Line 2:   // TODO(akalin): CheckServerReachable() can block, which may cause (Score: 18.95509222149849)\n",
      "2025-02-27 23:43:42 - INFO - Line 3:   // jank if we try to shut down sync.  Fix this. (Score: 15.921827912330627)\n",
      "2025-02-27 23:43:42 - INFO - Line 1: DCHECK(thread_checker_.CalledOnValidThread()); (Score: 13.048609673976898)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 381:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 381:\n",
      "2025-02-27 23:43:42 - INFO - Line 6: EXCEPTION_BLOCK(int, nonOpt, toInt32(MAYBE_MISSING_PARAMETER(args, 0, DefaultIsUndefined))); (Score: 38.12981712818146)\n",
      "2025-02-27 23:43:42 - INFO - Line 11: EXCEPTION_BLOCK(int, opt, toInt32(MAYBE_MISSING_PARAMETER(args, 1, DefaultIsUndefined))); (Score: 36.655037224292755)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: static v8::Handle<v8::Value> methodWithNonOptionalArgAndOptionalArgCallback(const v8::Arguments& args) (Score: 29.093632221221924)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 391:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 391:\n",
      "2025-02-27 23:43:42 - INFO - Line 17:                                            pos, (Score: 32.8073233962059)\n",
      "2025-02-27 23:43:42 - INFO - Line 43:       const int read_status = pReader->Read(pos, buflen, buf); (Score: 29.30308473110199)\n",
      "2025-02-27 23:43:42 - INFO - Line 18:                                            stop, (Score: 26.761664927005768)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 401:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 401:\n",
      "2025-02-27 23:43:42 - INFO - Line 36: // Render side: AUDIO_PCM_LOW_LATENCY is based on the Core Audio (WASAPI) (Score: 27.34325522184372)\n",
      "2025-02-27 23:43:42 - INFO - Line 46: // at 44.0kHz to ensure that we can feed them to the webrtc::VoiceEngine. (Score: 22.671967267990112)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: bool WebRtcAudioRenderer::Initialize(WebRtcAudioRendererSource* source) { (Score: 21.40633851289749)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 411:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 411:\n",
      "2025-02-27 23:43:42 - INFO - Line 10: mm_request_receive_expect(pmonitor->m_recvfd, MONITOR_ANS_PAM_INIT_CTX, &m); (Score: 35.8133819103241)\n",
      "2025-02-27 23:43:42 - INFO - Line 8: mm_request_send(pmonitor->m_recvfd, MONITOR_REQ_PAM_INIT_CTX, &m); (Score: 32.234291672706604)\n",
      "2025-02-27 23:43:42 - INFO - Line 9: debug3(\"%s: waiting for MONITOR_ANS_PAM_INIT_CTX\", __func__); (Score: 26.328236639499664)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 421:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 421:\n",
      "2025-02-27 23:43:42 - INFO - Line 4: if (!arg_shell_none && (strcmp(cfg.shell,\"/usr/bin/zsh\") == 0 || strcmp(cfg.shell,\"/bin/zsh\") == 0)) { (Score: 42.396284997463226)\n",
      "2025-02-27 23:43:42 - INFO - Line 32: \t\t\tcopy_file(\"/etc/skel/.cshrc\", fname, u, g, 0644); (Score: 28.606344521045685)\n",
      "2025-02-27 23:43:42 - INFO - Line 23: else if (!arg_shell_none && strcmp(cfg.shell,\"/bin/csh\") == 0) { (Score: 26.34350675344467)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 431:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 431:\n",
      "2025-02-27 23:43:42 - INFO - Line 38: if ((entry = phar_get_entry_info_dir(phar, path, path_len, allow_dir, for_create && !PHAR_G(readonly) && !phar->is_data ? NULL : error, security TSRMLS_CC)) == NULL) { (Score: 67.14664161205292)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: int phar_get_entry_data(phar_entry_data **ret, char *fname, int fname_len, char *path, int path_len, char *mode, char allow_dir, char **error, int security TSRMLS_DC) /* {{{ */ (Score: 58.55433893203735)\n",
      "2025-02-27 23:43:42 - INFO - Line 45: if ((entry = phar_get_entry_info(phar, path, path_len, for_create && !PHAR_G(readonly) && !phar->is_data ? NULL : error, security TSRMLS_CC)) == NULL) { (Score: 48.79018718004227)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 441:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 441:\n",
      "2025-02-27 23:43:42 - INFO - Line 40:     if (base::checked_cast<off_t>(size) > bytes_left) { (Score: 22.303591668605804)\n",
      "2025-02-27 23:43:42 - INFO - Line 27:   const uint8_t* index_ptr = stream + bytes_left - index_size; (Score: 21.726410388946533)\n",
      "2025-02-27 23:43:42 - INFO - Line 34:   for (size_t i = 0; i < num_frames; ++i) { (Score: 20.986106485128403)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 451:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 451:\n",
      "2025-02-27 23:43:42 - INFO - Line 0: void PartialMagnificationController::OnWindowDestroying(aura::Window* window) { (Score: 16.96966302394867)\n",
      "2025-02-27 23:43:42 - INFO - Line 3: aura::Window* new_root_window = GetCurrentRootWindow(); (Score: 14.778476297855377)\n",
      "2025-02-27 23:43:42 - INFO - Line 5:     SwitchTargetRootWindow(new_root_window); (Score: 12.043767303228378)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 461:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 461:\n",
      "2025-02-27 23:43:42 - INFO - Line 17:                      weak_ptr_factory_.GetWeakPtr())); (Score: 28.267034024000168)\n",
      "2025-02-27 23:43:42 - INFO - Line 11:   VLOG(1) << object_path_.value() << \": Unregistering pairing agent\"; (Score: 24.559937745332718)\n",
      "2025-02-27 23:43:42 - INFO - Line 16:           base::Bind(&BluetoothDeviceChromeOS::OnUnregisterAgentError, (Score: 20.338259905576706)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 471:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 471:\n",
      "2025-02-27 23:43:42 - INFO - Line 3: /* This just returns the (file*).  The chunk and idat control structures (Score: 16.98016020655632)\n",
      "2025-02-27 23:43:42 - INFO - Line 6:    struct control *control = png_voidcast(struct control*, (Score: 15.356341361999512)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: get_control(png_const_structrp png_ptr) (Score: 13.935535192489624)\n",
      "2025-02-27 23:43:42 - INFO - Explanation for Positive Sample 481:\n",
      "2025-02-27 23:43:42 - INFO - Ranked lines for Positive Sample 481:\n",
      "2025-02-27 23:43:42 - INFO - Line 17: DBG(READ, ul_debug(\"unbalanced quotes at: %s\", *value)); (Score: 19.412427604198456)\n",
      "2025-02-27 23:43:42 - INFO - Line 0: static int parse_token(char **name, char **value, char **cp) (Score: 16.275359094142914)\n",
      "2025-02-27 23:43:42 - INFO - Line 15: \t\tend = strchr(*value + 1, '\"'); (Score: 15.015612959861755)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 491:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 491:\n",
      "2025-02-27 23:43:43 - INFO - Line 0: void FrameSelection::MoveRangeSelection(const VisiblePosition& base_position, (Score: 17.618977308273315)\n",
      "2025-02-27 23:43:43 - INFO - Line 15: CreateVisibleSelectionWithGranularity(new_selection, granularity); (Score: 17.102863311767578)\n",
      "2025-02-27 23:43:43 - INFO - Line 5: .SetBaseAndExtentDeprecated(base_position.DeepEquivalent(), (Score: 16.189256072044373)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 501:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 501:\n",
      "2025-02-27 23:43:43 - INFO - Line 36: if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids) (Score: 32.5171594619751)\n",
      "2025-02-27 23:43:43 - INFO - Line 30: trace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush); (Score: 29.098341047763824)\n",
      "2025-02-27 23:43:43 - INFO - Line 16: if ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB)) (Score: 24.11266541481018)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 511:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 511:\n",
      "2025-02-27 23:43:43 - INFO - Line 9:   if (SearchBouncer::GetInstance()->IsNewTabPage(document_url)) { (Score: 22.8531591296196)\n",
      "2025-02-27 23:43:43 - INFO - Line 3: if (dispatcher_->IsExtensionActive(kWebStoreAppId)) { (Score: 17.890284717082977)\n",
      "2025-02-27 23:43:43 - INFO - Line 0: bool RendererPermissionsPolicyDelegate::IsRestrictedUrl( (Score: 13.452079236507416)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 521:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 521:\n",
      "2025-02-27 23:43:43 - INFO - Line 31: ret = g_verify_token_header(gss_mech_spnego, (Score: 22.75258058309555)\n",
      "2025-02-27 23:43:43 - INFO - Line 20: *responseToken = *mechListMIC = GSS_C_NO_BUFFER; (Score: 19.075139939785004)\n",
      "2025-02-27 23:43:43 - INFO - Line 48: *mechListMIC == GSS_C_NO_BUFFER) { (Score: 17.967957615852356)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 531:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 531:\n",
      "2025-02-27 23:43:43 - INFO - Line 0: int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len) (Score: 29.753901064395905)\n",
      "2025-02-27 23:43:43 - INFO - Line 4: int def_op_bytes, def_ad_bytes, goffset, simd_prefix; (Score: 20.88693755865097)\n",
      "2025-02-27 23:43:43 - INFO - Line 47: switch (ctxt->b = insn_fetch(u8, ctxt)) { (Score: 20.415579438209534)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 541:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 541:\n",
      "2025-02-27 23:43:43 - INFO - Line 2:                                         Vp9FrameHeader* fhdr) { (Score: 30.604905784130096)\n",
      "2025-02-27 23:43:43 - INFO - Line 30: fhdr->frame_type = static_cast<Vp9FrameHeader::FrameType>(reader_.ReadBool()); (Score: 28.974479854106903)\n",
      "2025-02-27 23:43:43 - INFO - Line 1:                                         off_t frame_size, (Score: 24.667484551668167)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 551:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 551:\n",
      "2025-02-27 23:43:43 - INFO - Line 4: delegate_->OnInstrumentDetailsReady(response->method_name, (Score: 15.809692919254303)\n",
      "2025-02-27 23:43:43 - INFO - Line 0: void ServiceWorkerPaymentInstrument::OnPaymentAppInvoked( (Score: 15.429288625717163)\n",
      "2025-02-27 23:43:43 - INFO - Line 1: mojom::PaymentHandlerResponsePtr response) { (Score: 11.600512623786926)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 561:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 561:\n",
      "2025-02-27 23:43:43 - INFO - Line 7: const scoped_refptr<base::SequencedTaskRunner>& file_task_runner, (Score: 21.049869120121002)\n",
      "2025-02-27 23:43:43 - INFO - Line 10: PolicyNamespaceKey(dm_protocol::kChromeUserPolicyType, std::string()), (Score: 20.71704524755478)\n",
      "2025-02-27 23:43:43 - INFO - Line 8: const scoped_refptr<base::SequencedTaskRunner>& io_task_runner) (Score: 20.65877878665924)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 571:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 571:\n",
      "2025-02-27 23:43:43 - INFO - Line 46: *buffer, portIndex, \"%u(%zu)@%p\", allottedSize, params->size(), params->pointer())); (Score: 28.9240882396698)\n",
      "2025-02-27 23:43:43 - INFO - Line 24:                 portIndex, (size_t)allottedSize, params->pointer())); (Score: 28.379926949739456)\n",
      "2025-02-27 23:43:43 - INFO - Line 20:             allottedSize, static_cast<OMX_U8 *>(params->pointer())); (Score: 25.42510935664177)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 581:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 581:\n",
      "2025-02-27 23:43:43 - INFO - Line 4:     EXPECT_CALL(mock_invalidation_client_, Acknowledge(ack_handle)); (Score: 26.41036021709442)\n",
      "2025-02-27 23:43:43 - INFO - Line 2:         ipc::invalidation::ObjectSource::CHROME_SYNC, type_name); (Score: 24.68256238102913)\n",
      "2025-02-27 23:43:43 - INFO - Line 5:     client_.InvalidateUnknownVersion(&mock_invalidation_client_, object_id, (Score: 24.06654468178749)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 591:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 591:\n",
      "2025-02-27 23:43:43 - INFO - Line 0: static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags) (Score: 24.87442833185196)\n",
      "2025-02-27 23:43:43 - INFO - Line 12: phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val); (Score: 22.010538578033447)\n",
      "2025-02-27 23:43:43 - INFO - Line 6: WARN(1, \"coherent pool not initialised!\\n\"); (Score: 15.539426624774933)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 601:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 601:\n",
      "2025-02-27 23:43:43 - INFO - Line 10:   for (BlockedRequestMap::const_iterator iter = blocked_requests_map_.begin(); (Score: 21.513762891292572)\n",
      "2025-02-27 23:43:43 - INFO - Line 12:     std::pair<std::set<ProcessRouteIDs>::iterator, bool> result = (Score: 21.311652064323425)\n",
      "2025-02-27 23:43:43 - INFO - Line 17:   for (std::set<ProcessRouteIDs>::const_iterator iter = ids.begin(); (Score: 20.71765661239624)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 611:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 611:\n",
      "2025-02-27 23:43:43 - INFO - Line 30:             downstream_equivalent, TextAffinity::kUpstream, primary_direction); (Score: 26.43522933125496)\n",
      "2025-02-27 23:43:43 - INFO - Line 35:           DownstreamIgnoringEditingBoundaries(upstream_equivalent) == position) (Score: 23.807816565036774)\n",
      "2025-02-27 23:43:43 - INFO - Line 22:       // editability. This helps in case |this| is in an editable block (Score: 22.571464002132416)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 621:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 621:\n",
      "2025-02-27 23:43:43 - INFO - Line 18: js_pushnumber(J, js_utfptrtoidx(text, m.sub[0].sp)); (Score: 23.620447397232056)\n",
      "2025-02-27 23:43:43 - INFO - Line 17: \tif (!js_regexec(re->prog, text, &m, 0)) (Score: 21.81304919719696)\n",
      "2025-02-27 23:43:43 - INFO - Line 13: js_newregexp(J, js_tostring(J, 1), 0); (Score: 20.300585627555847)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 631:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 631:\n",
      "2025-02-27 23:43:43 - INFO - Line 19:     uint64_t allocSize = mTimeToSampleCount * 2 * sizeof(uint32_t); (Score: 24.80239972472191)\n",
      "2025-02-27 23:43:43 - INFO - Line 31: for (uint32_t i = 0; i < mTimeToSampleCount * 2; ++i) { (Score: 23.258140742778778)\n",
      "2025-02-27 23:43:43 - INFO - Line 8: data_offset, header, sizeof(header)) < (ssize_t)sizeof(header)) { (Score: 22.92062336206436)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 641:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 641:\n",
      "2025-02-27 23:43:43 - INFO - Line 5:       chromeos::DBusThreadManager::Get()->GetPermissionBrokerClient(); (Score: 21.08503332734108)\n",
      "2025-02-27 23:43:43 - INFO - Line 13:       base::Bind(&UsbDeviceImpl::OpenOnBlockingThread, this, callback)); (Score: 19.94977965950966)\n",
      "2025-02-27 23:43:43 - INFO - Line 9:       base::Bind(&UsbDeviceImpl::OnOpenRequestComplete, this, callback)); (Score: 19.401760756969452)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 651:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 651:\n",
      "2025-02-27 23:43:43 - INFO - Line 33: scoped_ptr<DictionaryValue> expected_entry(node.GetEntry()->ToValue()); (Score: 21.92092090845108)\n",
      "2025-02-27 23:43:43 - INFO - Line 17: } else if (expected_model_type == syncable::TOP_LEVEL_FOLDER) { (Score: 21.79387801885605)\n",
      "2025-02-27 23:43:43 - INFO - Line 29: ExpectInt64Value(node.GetPredecessorId(), value, \"predecessorId\"); (Score: 21.77708911895752)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 661:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 661:\n",
      "2025-02-27 23:43:43 - INFO - Line 0: static inline void sem_getref_and_unlock(struct sem_array *sma) (Score: 19.310161590576172)\n",
      "2025-02-27 23:43:43 - INFO - Line 3: \tipc_unlock(&(sma)->sem_perm); (Score: 15.253226220607758)\n",
      "2025-02-27 23:43:43 - INFO - Line 2: \tipc_rcu_getref(sma); (Score: 12.070547997951508)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 671:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 671:\n",
      "2025-02-27 23:43:43 - INFO - Line 35: if (!(options->namespaces & CLONE_NEWNS) && (options->attach_flags & LXC_ATTACH_REMOUNT_PROC_SYS)) { (Score: 40.75764924287796)\n",
      "2025-02-27 23:43:43 - INFO - Line 23: ret = lxc_read_nointr_expect(ipc_socket, &status, sizeof(status), &expected); (Score: 28.917968928813934)\n",
      "2025-02-27 23:43:43 - INFO - Line 50: if (options->attach_flags & LXC_ATTACH_SET_PERSONALITY) { (Score: 21.481053173542023)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 681:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 681:\n",
      "2025-02-27 23:43:43 - INFO - Line 19: error = xfs_attr3_leaf_read(args->trans, args->dp, args->blkno, -1, &bp); (Score: 31.261258900165558)\n",
      "2025-02-27 23:43:43 - INFO - Line 37: name_rmt = xfs_attr3_leaf_name_remote(leaf, args->index); (Score: 23.941158890724182)\n",
      "2025-02-27 23:43:43 - INFO - Line 33: name_loc = xfs_attr3_leaf_name_local(leaf, args->index); (Score: 23.21965056657791)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 691:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 691:\n",
      "2025-02-27 23:43:43 - INFO - Line 30:         dlopen(I965HybridDrvVideoPath, RTLD_NOW|RTLD_GLOBAL|RTLD_NODELETE); (Score: 35.62964618206024)\n",
      "2025-02-27 23:43:43 - INFO - Line 33: dlopen(\"libva-drm.so.1\", RTLD_NOW|RTLD_GLOBAL|RTLD_NODELETE); (Score: 34.717347860336304)\n",
      "2025-02-27 23:43:43 - INFO - Line 23:         I965HybridDrvVideoPath = \"/usr/lib64/va/drivers/hybrid_drv_video.so\"; (Score: 33.623196214437485)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 701:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 701:\n",
      "2025-02-27 23:43:43 - INFO - Line 47: if ((in && out)  ||  iso_in || iso_out || int_in || int_out) (Score: 25.23912161588669)\n",
      "2025-02-27 23:43:43 - INFO - Line 37: endpoint_update(edi, &int_in, &int_out, e); (Score: 20.006311655044556)\n",
      "2025-02-27 23:43:43 - INFO - Line 24: for (ep = 0; ep < alt->desc.bNumEndpoints; ep++) { (Score: 19.674606680870056)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 711:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 711:\n",
      "2025-02-27 23:43:43 - INFO - Line 82: v.ling.l_onoff\t= !!sock_flag(sk, SOCK_LINGER); (Score: 26.8770689368248)\n",
      "2025-02-27 23:43:43 - INFO - Line 69: v.val = !!sock_flag(sk, SOCK_URGINLINE); (Score: 20.395419478416443)\n",
      "2025-02-27 23:43:43 - INFO - Line 47: v.val = !!sock_flag(sk, SOCK_KEEPOPEN); (Score: 19.55963534116745)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 721:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 721:\n",
      "2025-02-27 23:43:43 - INFO - Line 18:                                                    &PluginInstance::OnTimerCall, (Score: 44.55691123008728)\n",
      "2025-02-27 23:43:43 - INFO - Line 20:                                                    npp_, (Score: 40.9036480486393)\n",
      "2025-02-27 23:43:43 - INFO - Line 21:                                                    timer_id), (Score: 40.75174018740654)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 731:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 731:\n",
      "2025-02-27 23:43:43 - INFO - Line 15: \tdev->priv_flags &= ~IFF_XMIT_DST_RELEASE; (Score: 19.202064752578735)\n",
      "2025-02-27 23:43:43 - INFO - Line 8: dev->tx_queue_len = TX_Q_LIMIT; (Score: 15.905377388000488)\n",
      "2025-02-27 23:43:43 - INFO - Line 4: dev->netdev_ops = &ifb_netdev_ops; (Score: 14.77632850408554)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 741:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 741:\n",
      "2025-02-27 23:43:43 - INFO - Line 35:                           REQUEST_DEVICE_FROM_CROSS_ORIGIN_IFRAME); (Score: 41.644979774951935)\n",
      "2025-02-27 23:43:43 - INFO - Line 21:         blink::mojom::WebBluetoothResult::REQUEST_DEVICE_WITH_BLOCKLISTED_UUID); (Score: 29.281208902597427)\n",
      "2025-02-27 23:43:43 - INFO - Line 43: DVLOG(1) << \"Bluetooth Adapter not present. Can't serve requestDevice.\"; (Score: 22.571265935897827)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 751:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 751:\n",
      "2025-02-27 23:43:43 - INFO - Line 11:              \"%s -m %s --manager-address %s -f %s/.shadowsocks_%s.pid -c %s/.shadowsocks_%s.conf\", (Score: 44.49290144443512)\n",
      "2025-02-27 23:43:43 - INFO - Line 26: snprintf(cmd + len, BUF_SIZE - len, \" -n %d\", manager->nofile); (Score: 26.78371125459671)\n",
      "2025-02-27 23:43:43 - INFO - Line 31: snprintf(cmd + len, BUF_SIZE - len, \" -a %s\", manager->user); (Score: 26.56133282184601)\n",
      "2025-02-27 23:43:43 - INFO - Explanation for Positive Sample 761:\n",
      "2025-02-27 23:43:43 - INFO - Ranked lines for Positive Sample 761:\n",
      "2025-02-27 23:43:43 - INFO - Line 9:             if ((s->rregs[ESP_RSTAT] & STAT_PIO_MASK) == 0) { (Score: 36.20149880647659)\n",
      "2025-02-27 23:43:43 - INFO - Line 15:                 s->rregs[ESP_FIFO] = s->ti_buf[s->ti_rptr++]; (Score: 34.09433767199516)\n",
      "2025-02-27 23:43:43 - INFO - Line 12:                               \"esp: PIO data read not implemented\\n\"); (Score: 29.871258080005646)\n",
      "2025-02-27 23:43:44 - INFO - Explanation for Positive Sample 771:\n",
      "2025-02-27 23:43:44 - INFO - Ranked lines for Positive Sample 771:\n",
      "2025-02-27 23:43:44 - INFO - Line 12: (NO_ERROR != player->setDataSource(httpService, url, headers))) { (Score: 19.601488828659058)\n",
      "2025-02-27 23:43:44 - INFO - Line 8:         const sp<IMediaPlayerService>& service(getMediaPlayerService()); (Score: 18.99187833070755)\n",
      "2025-02-27 23:43:44 - INFO - Line 2: const char *url, const KeyedVector<String8, String8> *headers) (Score: 18.797007083892822)\n"
     ]
    }
   ],
   "source": [
    "all_ranked_lines = []\n",
    "all_lines = []\n",
    "for idx, explanation in enumerate(explanation_results):\n",
    "    if idx % 10 == 0:\n",
    "        logger.info(f\"Explanation for Positive Sample {idx + 1}:\")\n",
    "\n",
    "    if EXPLAINER == \"LIME\":\n",
    "        token_scores = explanation.as_list()\n",
    "    elif EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "        token_scores = explanation\n",
    "\n",
    "    if EXPLAINER == \"ATTENTION\":\n",
    "      token_scores_dict = {line_idx: score for line_idx, line, score in explanation}\n",
    "    elif EXPLAINER == \"LIME\":\n",
    "        token_scores_dict = {}\n",
    "        for token, score in token_scores:\n",
    "            token_scores_dict[token] = score\n",
    "    elif EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "        token_scores_dict = {line_idx: score for line_idx, line, score in token_scores}\n",
    "\n",
    "    # Get the corresponding function code\n",
    "    function_code = positive_samples[idx]\n",
    "    \n",
    "    # Tokenize the function into lines and tokens\n",
    "    lines, tokenized_lines = tokenize_function_to_lines_and_tokens(function_code, '\\n')\n",
    "    \n",
    "    # Compute values for each line\n",
    "    if EXPLAINER == \"DEEPLIFTSHAP\":\n",
    "        line_scores = compute_deepliftshap_values_per_line(tokenized_lines, token_scores_dict)\n",
    "    elif EXPLAINER == \"LIME\":\n",
    "        line_scores = compute_lime_values_per_line(tokenized_lines, token_scores_dict)\n",
    "    elif EXPLAINER == \"ATTENTION\":\n",
    "        line_scores = [token_scores_dict.get(line_idx, 0) for line_idx in range(len(lines))]\n",
    "    \n",
    "    # Create a list of tuples containing (line_index, line_text, lime_score)\n",
    "    line_scores_with_text = [(line_idx, line, line_scores[line_idx]) for line_idx, line in enumerate(lines)]\n",
    "    all_lines.append(line_scores_with_text)\n",
    "    \n",
    "    # Sort the lines by score in descending order\n",
    "    ranked_lines = sorted(line_scores_with_text, key=lambda x: x[2], reverse=True)\n",
    "    all_ranked_lines.append(ranked_lines)\n",
    "    \n",
    "    # Print the ranked lines\n",
    "    if idx % 10 == 0:  # Log every 10th sample\n",
    "        logger.info(f\"Ranked lines for Positive Sample {idx + 1}:\")\n",
    "        for line_idx, line_text, score in ranked_lines[:3]:  # Only print top 3 lines\n",
    "            logger.info(f\"Line {line_idx}: {line_text} (Score: {score})\")\n",
    "\n",
    "    # Optionally, show the explanation in a notebook\n",
    "    # if EXPLAINER == \"LIME\":\n",
    "    #     explanation.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25b902-1146-41dc-82df-e2754ca4485e",
   "metadata": {},
   "source": [
    "Line-level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921b8c7c-5c31-48c6-97c9-f24b291c0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "# Function to compute Top-X Accuracy for each function\n",
    "def compute_top_x_accuracy(ranked_lines, flaw_lines, top_x=10):\n",
    "    \"\"\"\n",
    "    Compute Top-X Accuracy: Measures whether at least one actual vulnerable line appears in the top-X ranking.\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: 1 if at least one vulnerable line is in the top-X, else 0.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "    # top_x_lines = []\n",
    "    # count = 0\n",
    "    # for line3 in ranked_lines:\n",
    "    #     line = line3[1]\n",
    "    #     if line != \"\":\n",
    "    #         top_x_lines.append(line3[0])\n",
    "    #         count+=1\n",
    "    #         if count>=top_x:\n",
    "    #             break\n",
    "    return 1 if any(line_index in flaw_lines for line_index, _, _ in top_x_lines) else 0\n",
    "    #return 1 if any(line_index in flaw_lines for line_index in top_x_lines) else 0\n",
    "\n",
    "def compute_reciprocal_rank(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Reciprocal Rank for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :return: Reciprocal rank for this function, or 0 if no vulnerable line is found in the ranking.\n",
    "    \"\"\"\n",
    "\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "    for i, (line_index, _, _) in enumerate(top_x_lines):\n",
    "        if line_index in flaw_lines:\n",
    "            return 1 / (i + 1)  # Reciprocal of the rank of the first relevant item\n",
    "    return 0  # If no relevant item is found\n",
    "\n",
    "# Helper function to parse flaw lines\n",
    "def parse_flaw_lines(flaw_line_str):\n",
    "    \"\"\"\n",
    "    Parse flaw_line string into a list of integers.\n",
    "    \n",
    "    :param flaw_line_str: A string of comma-separated line numbers (e.g., '36,37,40').\n",
    "    :return: List of integers representing the flaw lines.\n",
    "    \"\"\"\n",
    "    if pd.isna(flaw_line_str) or flaw_line_str == '':\n",
    "        return []\n",
    "    else:\n",
    "        return [int(x) for x in flaw_line_str.split(',')]\n",
    "        \n",
    "# Function to compute Initial False Alarm (IFA)\n",
    "def compute_ifa(ranked_lines, flaw_lines):\n",
    "    \"\"\"\n",
    "    Compute Initial False Alarm (IFA): Counts how many false alarms (non-vulnerable lines) occur before the first vulnerable line.\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices.\n",
    "    :return: Number of false alarms until the first vulnerable line is found.\n",
    "    \"\"\"\n",
    "    ifa = 0\n",
    "    for line_index, _, _ in ranked_lines:\n",
    "        if line_index not in flaw_lines:\n",
    "            ifa += 1\n",
    "        else:\n",
    "            break  # Stop counting when the first vulnerable line is found\n",
    "    return ifa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5e767eb-67d0-4400-9948-a4bdd01b6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Top-X Precision for each function\n",
    "def compute_top_x_precision(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Top-X Precision: Measures how many lines are indeed vulnerable in the top-X ranking.\n",
    "\n",
    "    Relevant retrieved instances divided by all retrieved instances\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: Number of the number of vulnerable lines included in the top-X ranking divided by X.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "\n",
    "    count = 0\n",
    "    for line_index, _, _ in top_x_lines:\n",
    "        if line_index in flaw_lines:\n",
    "            count += 1\n",
    "\n",
    "    return count / top_x\n",
    "\n",
    "# Function to compute Top-X Recall for each function\n",
    "def compute_top_x_recall(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Top-X Recall: Measures how many of the function's vulnerable lines can be found by searching in the top-X ranking.\n",
    "\n",
    "    Relevant retrieved instances divided by all relevant instances\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: Number of the number of vulnerable lines included in the top-X ranking divided by the total number of vulnerable lines in the function.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "\n",
    "    count = 0\n",
    "    for line_index, _, _ in top_x_lines:\n",
    "        if line_index in flaw_lines:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(flaw_lines) if len(flaw_lines)>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0c0449-48c2-4475-9963-5f20132972fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision_at_k(ranked_lines, flaw_lines, k):\n",
    "    \"\"\"\n",
    "    Compute Average Precision at K for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param k: The number of top lines to consider for AP@K.\n",
    "    :return: Average Precision at K for this function.\n",
    "    \"\"\"\n",
    "    relevant_found = 0\n",
    "    precision_sum = 0\n",
    "    top_k_lines = ranked_lines[:k]  # Consider only the top K lines\n",
    "\n",
    "    for i, (line_index, _, _) in enumerate(top_k_lines):\n",
    "        if line_index in flaw_lines:\n",
    "            relevant_found += 1\n",
    "            precision_sum += relevant_found / (i + 1)  # Precision at this rank\n",
    "\n",
    "    return precision_sum / relevant_found if relevant_found>0 else 0  # Avoid division by zero\n",
    "    #return precision_sum / min(k, len(flaw_lines)) if flaw_lines else 0  # Avoid division by zero\n",
    "\n",
    "\n",
    "def compute_average_recall_at_k(ranked_lines, flaw_lines, k):\n",
    "    \"\"\"\n",
    "    Compute Average Recall at K for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param k: The number of top lines to consider for AR@K.\n",
    "    :return: Average Recall at K for this function.\n",
    "    \"\"\"\n",
    "    relevant_found = 0\n",
    "    recall_sum = 0\n",
    "    top_k_lines = ranked_lines[:k]  # Consider only the top K lines\n",
    "\n",
    "    for i, (line_index, _, _) in enumerate(top_k_lines):\n",
    "        if line_index in flaw_lines:\n",
    "            relevant_found += 1\n",
    "            recall_sum += relevant_found / len(flaw_lines)  if len(flaw_lines)>0 else 0 # Recall at this rank\n",
    "\n",
    "    return recall_sum / relevant_found if relevant_found>0 else 0  # Avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b278701-ec70-40fa-b0bc-dc7788ab62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Effectiveness metrics\n",
    "\n",
    "# Helper functions\n",
    "# Compute total LOC of the testing set\n",
    "def compute_total_loc(all_total_locs):  \n",
    "    return sum(all_total_locs)\n",
    "\n",
    "def compute_total_flaw_lines(all_flaw_lines):\n",
    "    total_flaw_loc = 0\n",
    "    for line_str in all_flaw_lines:\n",
    "        line_int = parse_flaw_lines(line_str)\n",
    "        total_flaw_loc+=len(line_int)\n",
    "\n",
    "    return total_flaw_loc\n",
    "\n",
    "def find_effort_breakpoint(flaw_lines_num, x_percent):    \n",
    "    return max(1, ((x_percent/100) * flaw_lines_num))\n",
    "\n",
    "def find_recall_breakpoint(total_test_loc, x_percent):    \n",
    "    return max(1, ((x_percent/100) * total_test_loc))\n",
    "\n",
    "# Prepare data for Cost-Effectiveness calculation\n",
    "# Sort the ranked_lines based on their function proba\n",
    "def sort_all_ranked_lines(positive_probas, all_ranked_lines):\n",
    "    combined = list(zip(positive_probas, all_ranked_lines))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    all_ranked_lines_sorted = [item[1] for item in combined_sorted]\n",
    "    \n",
    "    return all_ranked_lines_sorted\n",
    "\n",
    "# Sort the flaw_lines based on their function proba\n",
    "def sort_all_flaw_lines(positive_probas, all_flaw_lines):\n",
    "\n",
    "    combined = list(zip(positive_probas, all_flaw_lines))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    all_flaw_lines_sorted = [item[1] for item in combined_sorted]\n",
    "\n",
    "    return all_flaw_lines_sorted\n",
    "    \n",
    "\n",
    "# Function to compute Effort@X%Recall by sorting functions\n",
    "def compute_effort_at_x_percent_recall_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=20):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_ranked_lines_sorted = sort_all_ranked_lines(positive_probas, all_ranked_lines)\n",
    "    all_flaw_lines_sorted = sort_all_flaw_lines(positive_probas, all_flaw_lines)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    effort_breakpoint = find_effort_breakpoint(flaw_lines_num, x_percent)\n",
    "\n",
    "    if flaw_lines_num == 0:\n",
    "        return 1.0  # If no vulnerable lines, maximum effort (full LOC inspected)\n",
    "\n",
    "    # Iterate over ranked lines to count how much effort (LOC) is spent to find X% of the vulnerable lines\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    found = False\n",
    "    for i, fun_lines in enumerate(all_ranked_lines_sorted):\n",
    "        fun_flaws = parse_flaw_lines(all_flaw_lines_sorted[i])\n",
    "        for line in fun_lines:\n",
    "            index = line[0]\n",
    "            inspected_lines += 1\n",
    "\n",
    "            if index in fun_flaws:\n",
    "                found_vulnerable_lines += 1\n",
    "\n",
    "            # Stop when we find X% of vulnerable lines\n",
    "            if found_vulnerable_lines >= effort_breakpoint:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    return inspected_lines / total_test_loc\n",
    "\n",
    "# Assign labels for all sorted lines\n",
    "def create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines):\n",
    "\n",
    "    all_lines_with_labels = []\n",
    "    for func_idx, ranked_lines in enumerate(all_ranked_lines):\n",
    "        flaw_lines = parse_flaw_lines(all_flaw_lines[func_idx])\n",
    "        \n",
    "        for line_idx, line_content, line_score in ranked_lines:\n",
    "            if line_idx in flaw_lines:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            all_lines_with_labels.append((line_idx, line_content, line_score, label))\n",
    "\n",
    "    sorted_lines_with_labels = sorted(all_lines_with_labels, key=lambda x: x[2], reverse=True)\n",
    "            \n",
    "    \n",
    "    return sorted_lines_with_labels\n",
    "\n",
    "# Function to compute Effort@X%Recall by sorting all lines\n",
    "def compute_effort_at_x_percent_recall_rankedLines(all_ranked_lines, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=20):\n",
    "    \n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_labels_lines_sorted = create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines) # contains the label (vulnerable or not) of each line in the sorted lines\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "\n",
    "    if flaw_lines_num == 0:\n",
    "        return 1.0  # If no vulnerable lines, maximum effort (full LOC inspected)\n",
    "\n",
    "    effort_breakpoint = find_effort_breakpoint(flaw_lines_num, x_percent)\n",
    "\n",
    "    # Iterate over ranked lines to count how much effort (LOC) is spent to find X% of the vulnerable lines\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    for i in range(0, len(all_labels_lines_sorted)):\n",
    "        _, _, _, line_label = all_labels_lines_sorted[i]\n",
    "        inspected_lines += 1\n",
    "        if line_label == 1:\n",
    "            found_vulnerable_lines += 1\n",
    "\n",
    "        # Stop when we find X% of vulnerable lines\n",
    "        if found_vulnerable_lines >= effort_breakpoint:\n",
    "            break\n",
    "\n",
    "    return inspected_lines / total_test_loc\n",
    "    \n",
    "\n",
    "# Function to compute Recall@1%LOC by sorting functions\n",
    "def compute_recall_at_x_percent_loc_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=1):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_ranked_lines_sorted = sort_all_ranked_lines(positive_probas, all_ranked_lines)\n",
    "    all_flaw_lines_sorted = sort_all_flaw_lines(positive_probas, all_flaw_lines)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    recall_breakpoint = find_recall_breakpoint(total_test_loc, x_percent)\n",
    "\n",
    "    # Count how many vulnerable lines are found within the top X% LOC\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    found = False\n",
    "    for i, fun_lines in enumerate(all_ranked_lines_sorted):\n",
    "        fun_flaws = parse_flaw_lines(all_flaw_lines_sorted[i])\n",
    "        for line in fun_lines:\n",
    "            index = line[0]\n",
    "            inspected_lines += 1\n",
    "\n",
    "            if index in fun_flaws:\n",
    "                found_vulnerable_lines += 1\n",
    "\n",
    "            # Stop when we find X% of vulnerable lines\n",
    "            if inspected_lines >= recall_breakpoint:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    return found_vulnerable_lines / flaw_lines_num\n",
    "\n",
    "# Function to compute Recall@1%LOC by sorting all lines\n",
    "def compute_recall_at_x_percent_loc_rankedLines(all_ranked_lines, all_neg_lines, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=1):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_labels_lines_sorted = create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines) # contains the label (vulnerable or not) of each line in the sorted lines\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    recall_breakpoint = find_recall_breakpoint(total_test_loc, x_percent)\n",
    "\n",
    "    # Count how many vulnerable lines are found within the top X% LOC\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    inspect_neg_lines = True\n",
    "    for i in range(0, len(all_labels_lines_sorted)):\n",
    "        inspected_lines += 1\n",
    "        _, _, _, line_label = all_labels_lines_sorted[i]\n",
    "\n",
    "        if line_label == 1:\n",
    "            found_vulnerable_lines += 1\n",
    "\n",
    "        if inspected_lines >= recall_breakpoint:\n",
    "            inspect_neg_lines = False\n",
    "            break\n",
    "\n",
    "    if inspect_neg_lines:\n",
    "        for neg_line in all_neg_lines:\n",
    "            inspected_lines += 1\n",
    "            if inspected_lines >= recall_breakpoint:\n",
    "                break\n",
    "            \n",
    "\n",
    "    return found_vulnerable_lines / flaw_lines_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9f53463-ce57-4584-ae5c-1f9bbffc9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_beyond_token_limit(lines_text, flaw_lines_text):\n",
    "\n",
    "#     if type(flaw_lines_text) != str:\n",
    "#         return False\n",
    "#     else:\n",
    "    \n",
    "#         beyond = True\n",
    "    \n",
    "#         _, flaw_tokens = tokenize_function_to_lines_and_tokens(flaw_lines_text, '/~/') \n",
    "    \n",
    "#         function_string = '\\n'.join([line_text for _, line_text, _ in lines_text])\n",
    "#         _, func_tokens = tokenize_function_to_lines_and_tokens(function_string, '\\n')\n",
    "        \n",
    "#         for encoded_flaw in flaw_tokens:    \n",
    "#             if encoded_flaw in func_tokens:\n",
    "#                 beyond = False\n",
    "#                 break\n",
    "    \n",
    "#     return beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d3e21dd-b21f-453b-974c-71ceb287f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate all metrics for each function\n",
    "def evaluate_vulnerability_detection(all_ranked_lines, all_flaw_lines, all_lines, all_flaw_lines_text, top_x=10):\n",
    "    \"\"\"\n",
    "    Evaluate the XAI methods using Top-X Accuracy, IFA, Effort@X%Recall, Recall@X%LOC for all functions.\n",
    "\n",
    "    :param all_ranked_lines: List of ranked lines for all functions.\n",
    "    :param all_flaw_lines: List of actual vulnerable line indices for all functions.\n",
    "    :param top_x: Number of top-ranked lines to consider for Top-X Accuracy.\n",
    "    :return: DataFrame with individual and average results for each function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, ranked_lines in enumerate(all_ranked_lines):\n",
    "\n",
    "        # check whether the flaws reside beyond the max_len of the model\n",
    "        #beyond = check_beyond_token_limit(all_lines[i], all_flaw_lines_text[i])      \n",
    "\n",
    "        #if beyond == False:\n",
    "        \n",
    "        flaw_line_index = all_flaw_lines[i]\n",
    "\n",
    "        # Even if there are no flaw lines, we still compute line-level evaluation for false positives\n",
    "        flaw_lines = parse_flaw_lines(flaw_line_index) if pd.notna(flaw_line_index) else []\n",
    "        \n",
    "        # Compute each metric\n",
    "        top_x_accuracy = compute_top_x_accuracy(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        top_x_precision = compute_top_x_precision(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        top_x_recall = compute_top_x_recall(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        rr = compute_reciprocal_rank(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        apk = compute_average_precision_at_k(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        ark = compute_average_recall_at_k(ranked_lines, flaw_lines, top_x)\n",
    "        \n",
    "        \n",
    "        ifa = compute_ifa(ranked_lines, flaw_lines)\n",
    "\n",
    "        result = {\n",
    "            f'Top-{top_x} Accuracy': top_x_accuracy,\n",
    "            f'Top-{top_x} Precision': top_x_precision,\n",
    "            f'Top-{top_x} Recall': top_x_recall,\n",
    "            f'Reciprocal Rank-{top_x}': rr,\n",
    "            f'AP@{top_x}': apk,\n",
    "            f'AR@{top_x}': ark,\n",
    "            'IFA': ifa\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Compute average results\n",
    "    average_results = results_df.mean().to_dict()\n",
    "    average_results['Type'] = 'Average'\n",
    "\n",
    "    # Compute median results\n",
    "    median_results = results_df.median().to_dict()\n",
    "    median_results['Type'] = 'Median'\n",
    "\n",
    "    # Add individual results and average to the final DataFrame\n",
    "    results_df['Type'] = 'Individual'\n",
    "    \n",
    "    average_results_df = pd.DataFrame([average_results])\n",
    "    median_results_df = pd.DataFrame([median_results])\n",
    "\n",
    "    # Combine individual and average results\n",
    "    final_results_df = pd.concat([results_df, average_results_df, median_results_df], ignore_index=True)\n",
    "    \n",
    "    return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7187ae5-9f3c-4c67-b7fb-8dce0700633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for line-level evaluation\n",
    "\n",
    "all_flaw_lines = [test_data['Line_Index'].tolist()[i] for i in positive_indices]  # Extract the flaw line indexes for each positive sample\n",
    "all_flaw_lines_text = [test_data['Lines'].tolist()[i] for i in positive_indices]  # Extract the flaw lines for each positive sample\n",
    "#all_total_locs = [len(test_data['Text'].tolist()[i].split('\\n')) for i in positive_indices]  # Compute total LOC for each positive sample\n",
    "\n",
    "test_all_flaw_lines = [test_data['Line_Index'].tolist()[i] for i in actual_positive_indices] # Extract the flaw line indexes for each actual positive sample\n",
    "test_all_total_locs = [len(test_data['Text'].tolist()[i].split('\\n')) for i in range(len(test_data))] # Compute total LOC for each sample in the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a09665f-1374-4fe2-8818-8b5c123d960d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Top-10 Accuracy  Top-10 Precision  Top-10 Recall  Reciprocal Rank-10  \\\n",
      "0           1.000000          0.100000       0.500000            0.333333   \n",
      "1           1.000000          0.200000       0.666667            0.500000   \n",
      "2           1.000000          0.100000       0.333333            0.100000   \n",
      "3           1.000000          1.000000       0.625000            1.000000   \n",
      "4           0.000000          0.000000       0.000000            0.000000   \n",
      "..               ...               ...            ...                 ...   \n",
      "775         0.000000          0.000000       0.000000            0.000000   \n",
      "776         1.000000          0.200000       1.000000            0.200000   \n",
      "777         1.000000          0.100000       0.100000            1.000000   \n",
      "778         0.717224          0.192159       0.581214            0.439712   \n",
      "779         1.000000          0.100000       0.703297            0.333333   \n",
      "\n",
      "        AP@10     AR@10        IFA        Type  \n",
      "0    0.333333  0.500000   2.000000  Individual  \n",
      "1    0.500000  0.500000   1.000000  Individual  \n",
      "2    0.100000  0.333333   9.000000  Individual  \n",
      "3    1.000000  0.343750   0.000000  Individual  \n",
      "4    0.000000  0.000000  14.000000  Individual  \n",
      "..        ...       ...        ...         ...  \n",
      "775  0.000000  0.000000  21.000000  Individual  \n",
      "776  0.211111  0.750000   4.000000  Individual  \n",
      "777  1.000000  0.100000   0.000000  Individual  \n",
      "778  0.414679  0.491779  45.611825     Average  \n",
      "779  0.333333  0.500000   2.000000      Median  \n",
      "\n",
      "[780 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Results based on per function accuracy\n",
    "\n",
    "# Usage:\n",
    "top_x = 10\n",
    "final_results_df = evaluate_vulnerability_detection(all_ranked_lines, all_flaw_lines, all_lines, all_flaw_lines_text, top_x)\n",
    "\n",
    "# Display Accuracy Results per Function\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1bc9972-59e6-420e-aefd-01703631a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifa_all = final_results_df[\"IFA\"]\n",
    "ifa_ = ifa_all.iloc[0:-2]\n",
    "ifa_.to_csv('ifa_self_attention.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f274079e-9ea1-4574-91a8-992fbc95171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results based on the total of lines\n",
    "\n",
    "# configure sorting choice\n",
    "sort_by_lines = True # False # True when sort lines by line score and False when sort functions by prediction proba (and then sort lines in each function)\n",
    "\n",
    "# Usage\n",
    "if sort_by_lines == False:\n",
    "    \n",
    "    effortXrecall = compute_effort_at_x_percent_recall_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=20)\n",
    "    recallXloc = compute_recall_at_x_percent_loc_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=1)\n",
    "else: #sort_by_lines == True\n",
    "    effortXrecall = compute_effort_at_x_percent_recall_rankedLines(all_ranked_lines, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=20)\n",
    "    recallXloc = compute_recall_at_x_percent_loc_rankedLines(all_ranked_lines, all_neg_lines, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a9842fe-e96e-4d22-a473-4790ea89ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Accuracy: 0.7172236503856041\n",
      "Top-10 Precision: 0.19215938303341903\n",
      "Top-10 Recall: 0.5812136735946082\n",
      "Top-10 Reciprocal Rank: 0.4397116130085281\n",
      "Top-10 MAP: 0.4146787962185731\n",
      "Top-10 MAR: 0.49177867240461764\n",
      "Median IFA: 2.0\n",
      "Effort@20%Recall: 0.0061688109612426534\n",
      "Recall@1%LOC: 0.28819969742813917\n"
     ]
    }
   ],
   "source": [
    "# Display Final Evaluation Results\n",
    "top10acc = final_results_df[f'Top-{top_x} Accuracy'].tolist()[-2]\n",
    "top_precision = final_results_df[f'Top-{top_x} Precision'].tolist()[-2]\n",
    "top_recall = final_results_df[f'Top-{top_x} Recall'].tolist()[-2]\n",
    "top_mrr = final_results_df[f'Reciprocal Rank-{top_x}'].tolist()[-2]\n",
    "top_map = final_results_df[f'AP@{top_x}'].tolist()[-2]\n",
    "top_mar = final_results_df[f'AR@{top_x}'].tolist()[-2]\n",
    "ifa = final_results_df[\"IFA\"].tolist()[-1]\n",
    "print(f\"Top-{top_x} Accuracy: {top10acc}\")\n",
    "print(f\"Top-{top_x} Precision: {top_precision}\")\n",
    "print(f\"Top-{top_x} Recall: {top_recall}\")\n",
    "print(f\"Top-{top_x} Reciprocal Rank: {top_mrr}\")\n",
    "print(f\"Top-{top_x} MAP: {top_map}\")\n",
    "print(f\"Top-{top_x} MAR: {top_mar}\")\n",
    "print(f\"Median IFA: {ifa}\")\n",
    "print(f\"Effort@20%Recall: {effortXrecall}\")\n",
    "print(f\"Recall@1%LOC: {recallXloc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bd023bb-c904-46ca-bdbc-db322c66a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy results:\n",
      "Top-10 Accuracy: 71.7%\n",
      "Top-10 Precision: 19.2%\n",
      "Top-10 Recall: 58.1%\n",
      "Top-10 MRR: 44.0%\n",
      "Top-10 MAP: 41.5%\n",
      "Top-10 MAR: 49.2%\n",
      "\n",
      "\n",
      "Cost-effectiveness results:\n",
      "Median IFA: 2.0\n",
      "Effort@20%Recall: 0.6%\n",
      "Recall@1%LOC: 28.8%\n"
     ]
    }
   ],
   "source": [
    "# Display Final Evaluation Results in Percentages\n",
    "print(\"Accuracy results:\")\n",
    "print(f\"Top-{top_x} Accuracy: {round(top10acc * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} Precision: {round(top_precision * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} Recall: {round(top_recall * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MRR: {round(top_mrr * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MAP: {round(top_map * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MAR: {round(top_mar * 100, 1)}%\")\n",
    "print(\"\\n\")\n",
    "print(\"Cost-effectiveness results:\")\n",
    "print(f\"Median IFA: {round(ifa, 1)}\")\n",
    "print(f\"Effort@20%Recall: {round(effortXrecall * 100, 1)}%\")\n",
    "print(f\"Recall@1%LOC: {round(recallXloc * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e575da-b06a-46fa-a958-f8219918c6a1",
   "metadata": {},
   "source": [
    "Comparison with sota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3d7b887-1377-4a05-ba28-448b27cb261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                    Ours            LineVul         Saliency        DeepLift        DeepLiftSHAP    LIG             GradientSHAP    CppCheck       \n",
      "=================================================================================================================================================\n",
      "Top-10 Accuracy           72%             65%             58%             57%             57%             53%             52%             15%            \n",
      "Median IFA                2               1               4               3               3               4               3               4              \n",
      "Effort@20%Recall          0.62%           0.75%           1.51%           1.51%           1.51%           1.06            1.60%           13%            \n",
      "Recall@1%LOC              29%             24%             13%             13%             13%             19%             13%             4%             \n"
     ]
    }
   ],
   "source": [
    "# Define metrics\n",
    "headers = [\"Metric\", \"Ours\", \"LineVul\", \"Saliency\", \"DeepLift\", \"DeepLiftSHAP\", \"LIG\", \"GradientSHAP\", \"CppCheck\"]\n",
    "\n",
    "# Metric names\n",
    "metrics = [\n",
    "    \"Top-10 Accuracy\",\n",
    "    \"Median IFA\",\n",
    "    \"Effort@20%Recall\",\n",
    "    \"Recall@1%LOC\"\n",
    "]\n",
    "\n",
    "# Metrics values\n",
    "your_implementation = [\n",
    "    f\"{int(round(top10acc * 100, 0))}%\",\n",
    "    f\"{int(round(ifa, 1))}\",\n",
    "    f\"{round(effortXrecall * 100, 2)}%\",\n",
    "    f\"{int(round(recallXloc * 100, 0))}%\"\n",
    "]\n",
    "\n",
    "linevul_results = [\n",
    "    \"65%\",\n",
    "    \"1\",\n",
    "    \"0.75%\",\n",
    "    \"24%\"\n",
    "]\n",
    "\n",
    "saliency = [\n",
    "    \"58%\",\n",
    "    \"4\",\n",
    "    \"1.51%\",\n",
    "    \"13%\"\n",
    "]\n",
    "\n",
    "deeplift = [\n",
    "    \"57%\",\n",
    "    \"3\",\n",
    "    \"1.51%\",\n",
    "    \"13%\"\n",
    "]\n",
    "\n",
    "deepliftshap = [\n",
    "    \"57%\",\n",
    "    \"3\",\n",
    "    \"1.51%\",\n",
    "    \"13%\"\n",
    "]\n",
    "\n",
    "lig = [\n",
    "    \"53%\",\n",
    "    \"4\",\n",
    "    \"1.06\",\n",
    "    \"19%\"\n",
    "]\n",
    "\n",
    "gradientshap = [\n",
    "    \"52%\",\n",
    "    \"3\",\n",
    "    \"1.60%\",\n",
    "    \"13%\"\n",
    "]\n",
    "\n",
    "cppcheck = [\n",
    "    \"15%\",\n",
    "    \"4\",\n",
    "    \"13%\",\n",
    "    \"4%\"\n",
    "]\n",
    "\n",
    "# Combine all results into a list of lists\n",
    "all_results = [\n",
    "    your_implementation,\n",
    "    linevul_results,\n",
    "    saliency,\n",
    "    deeplift,\n",
    "    deepliftshap,\n",
    "    lig,\n",
    "    gradientshap,\n",
    "    cppcheck\n",
    "]\n",
    "\n",
    "# Print table header with consistent column width (15 characters)\n",
    "column_width = 15\n",
    "print(f\"{headers[0]:<25} {headers[1]:<{column_width}} {headers[2]:<{column_width}} {headers[3]:<{column_width}} {headers[4]:<{column_width}} {headers[5]:<{column_width}} {headers[6]:<{column_width}} {headers[7]:<{column_width}} {headers[8]:<{column_width}}\")\n",
    "\n",
    "# Print separator\n",
    "print(\"=\" * (25 + column_width * (len(headers) - 1)))\n",
    "\n",
    "# Print metric rows\n",
    "for i, metric in enumerate(metrics):\n",
    "    row_values = [results[i] for results in all_results]\n",
    "    print(f\"{metric:<25} {' '.join([f'{value:<{column_width}}' for value in row_values])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439c414-9b68-4e27-ad4a-3076092c7d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
