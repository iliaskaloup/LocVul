{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbc9218-221c-4ed8-a089-6df0092466dd",
   "metadata": {},
   "source": [
    "<b>Evaluation Scheme for line-level Vulnerability Detection using Seq2Seq models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cd034d-45fd-4d6a-b662-37a4ce99b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "import statistics\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3d8b9-d126-44e2-ac53-667a99810cf6",
   "metadata": {},
   "source": [
    "Basic Configuration of logging and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6813592a-12cc-41df-ad4e-464bee19b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:19:58 - INFO - SEED: 680\n"
     ]
    }
   ],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# Define logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Specify a constant seeder for processes\n",
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 135, 680]\n",
    "seed = seeders[9]\n",
    "logger.info(f\"SEED: {seed}\")\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fc372-8cb1-443b-afdb-d0c1287555c1",
   "metadata": {},
   "source": [
    "Read data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7d570f-a6fa-40f3-bd07-4a13ade6e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "root_path = os.getcwd()\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4816ca-40b8-4ab7-863c-8f3d6aa1c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')\n",
    "\n",
    "max_len_lines = 512\n",
    "checkpoint_dir_seq2seq = './checkpoints_seq2seq_512'  # './checkpoints_seq2seq' for max_len_lines = 128 or './checkpoints_seq2seq_512' for max_len_lines = 512\n",
    "save_path_seq2seq = os.path.join(checkpoint_dir_seq2seq, 'best_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e822f02-9f9a-47d0-9750-d1dd867096e5",
   "metadata": {},
   "source": [
    "Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c342c5-0564-4cc2-bd5e-5bfba0c33aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliaskaloup/anaconda3/envs/torchenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430d3549-1451-4c53-b6f1-490c9f88f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variation_seq2seq = \"Salesforce/codet5-base\"\n",
    "tokenizer_seq2seq = AutoTokenizer.from_pretrained(model_variation_seq2seq, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e6ddf-2e41-4da0-a43e-83df566ebd85",
   "metadata": {},
   "source": [
    "Split data sets and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53300998-154d-4b22-af0c-07930b0b5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:20:37 - INFO - List of projects in BigVul: ['openssl' 'linux' 'Chrome' 'poppler' 'libpcap' 'gpac' 'libarchive'\n",
      " 'suricata' 'libexpat' 'mbedtls' 'unixODBC' 'libreport' 'keepalived'\n",
      " 'Android' 'radare2' 'savannah' 'lynx-snapshots' 'libming' 'php' 'qemu'\n",
      " 'tor' 'uncurl' 'ghostscript' 'ImageMagick' 'memcached' 'samba' 'miniupnp'\n",
      " 'FreeRDP' 'OpenSC' 'wireshark' 'ImageMagick6' 'FFmpeg' 'minisphere'\n",
      " 'jasper' 'OpenJK' 'Onigmo' 'imageworsener' 'gst-plugins-ugly'\n",
      " 'php-radius' 'polarssl' 'libssh' 'spice' 'libgd' 'mruby' 'neomutt' 'dbus'\n",
      " 'ovs' 'libimobiledevice' 'gnupg' 'oniguruma' 'mod_auth_openidc'\n",
      " 'nautilus' 'ppp' 'tcpdump' 'wolfssl' 'raptor' 'VeraCrypt' 'udisks' 'exim'\n",
      " 'kde' 'harfbuzz' 'curl' 'redis' 'h2o' 'lxc' 'libtiff' 'wayland'\n",
      " 'htcondor' 'mindrot' 'irssi' 'abrt' 'php-src' 'ntp' 'rdesktop' 'uzbl'\n",
      " 'mujs' 'profanity' 'file' 'jansson' 'cyrus-imapd' 'Espruino' 'postgres'\n",
      " 'heimdal' 'w3m' 'u-boot' 'netfilter' 'zstd' 'gstreamer' 'axtls-8266'\n",
      " 'launchpad' 'leptonica' 'postgresql' 'shibboleth' 'httpd' 'openvpn'\n",
      " 'xserver' 'krb5' 'libXtst' 'mapserver' 'busybox' 'gnulib' 'ippusbxd'\n",
      " 'nspluginwrapper' 'Little-CMS' 'systemd' 'exempi' 'tartarus'\n",
      " 'tcmu-runner' 'yara' 'kvm-guest-drivers-windows' 'accountsservice'\n",
      " 'mod_auth_mellon' 'weechat' 'openmpt' 'beanstalkd' 'haproxy' 'libgcrypt'\n",
      " 'libmspack' 'git' 'libgit2' 'slurm' 'squashfs-tools' 'src' 'jq'\n",
      " 'pengutronix' 'lxde' 'libx11' 'iperf' 'ext-http' 'zlib' 'libndp'\n",
      " 'libvirt' 'libcomps' 'texlive-source' 'virglrenderer' 'libxml2'\n",
      " 'pacemaker' 'json-c' 'cgminer' 'collectd' 'libav' 'NetworkManager'\n",
      " 'libXrandr' 'libdwarf' 'libXrender' 'rufus' 'sgminer' 'yubico-pam'\n",
      " 'cJSON' 'proftpd' 'fontconfig' 'libevent' 'didiwiki' 'lxcfs' 'openafs'\n",
      " 'ngiflib' 'libxsmm' 'gnome-session' 'libsndfile' 'evince' 'libxfont'\n",
      " 'bubblewrap' 'wildmidi' 'domoticz' 'optee_os' 'capstone' 'gpmf-parser'\n",
      " 'openbsd' 'WavPack' 'ssdp-responder' 'libxkbcommon' 'libreswan' 'stb'\n",
      " 'cups' 'openjpeg' 'opa-ff' 'viabtc_exchange_server' 'dosfstools'\n",
      " 'civetweb' 'libpng' 'util-linux' 'libetpan' 'faad2'\n",
      " 'LibRaw-demosaic-pack-GPL2' 'MAC-Telnet' 'illumos-gate' 'mongo-c-driver'\n",
      " 'acpica' 'Varnish-Cache' 'atheme' 'firejail' 'libplist' 'libXv' 'flatpak'\n",
      " 'hexchat' 'nmap' 'liblouis' 'jabberd2' 'uriparser' 'frr' 'lighttpd1.4'\n",
      " 'irssi-proxy' 'pdfresurrect' 'nbd' 'altlinux' 'netdata' 'strongswan'\n",
      " 'media-tree' 'libu2f-host' 'pgbouncer' 'matio' 'sthttpd' 'rawstudio'\n",
      " 'pigz' 'libvips' 'pam_p11' 'zfs' 'openssh-portable' 'bdwgc' 'libinfinity'\n",
      " 'enlightment' 'libass' 'libXfixes' 'mosquitto' 'libuv' 'libgsf'\n",
      " 'mongoose-os' 'feh' 'pango' 'pixman' 'libzip' 'xrdb' 'libmodbus'\n",
      " 'corosync' 'tnef' 'sleuthkit' 'monkey' 'mongoose' 'quagga' 'polkit'\n",
      " 'lhasa' 'ioq3' 'pngquant' 'mstdlib' 'libjpeg-turbo' 'nfdump' 'libfep'\n",
      " 'aircrack-ng' 'suhosin' 'PDFGen' 'pam-u2f' 't1utils' 'knc' 'libXpm'\n",
      " 'infradead' 'webserver' 'pupnp-code' 'rpm' 'libXvMC' 'das_watchdog'\n",
      " 'lysator' 'linux-nfs' 'ettercap' 'moodle' 'nefarious2' 'varnish-cache'\n",
      " 'tinc' 'xcursor' 'boa' 'musl' 'bzrtp' 'libevt' 'unrealircd' 'nagioscore'\n",
      " 'libtomcrypt' 'bitlbee' 'quassel' 'picocom' 'fontforge' 'gifsicle'\n",
      " 'jerryscript' 'radvd' 'tpm2.0-tools' 'libXi' 'rsyslog' 'thor' 'librsvg'\n",
      " 'kamailio' 'libXdmcp' 'shadowsocks-libev' 'tcpreplay' 'wpitchoune'\n",
      " '3proxy' 'hylafax' 'libbsd' 'pure-ftpd' 'exfat' 'gimp' 'libmysofa'\n",
      " 'Openswan' 'proxychains-ng' 'charybdis' 'libICE' 'pyfribidi' 'drm'\n",
      " 'uwsgi' 'nedmalloc' 'torque' 'yodl' 'libx12']\n",
      "2025-02-28 10:20:37 - INFO - Number of different projects in BigVul: 310\n",
      "2025-02-28 10:20:37 - INFO - Top-10 largest projects in BigVul and their size: project\n",
      "Chrome         77173\n",
      "linux          46855\n",
      "Android         8691\n",
      "qemu            3096\n",
      "php             2709\n",
      "ImageMagick     2520\n",
      "savannah        2176\n",
      "FFmpeg          1932\n",
      "ghostscript     1867\n",
      "openssl         1860\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View the largest projects\n",
    "\n",
    "logger.info(f\"List of projects in BigVul: {dataset['project'].unique()}\")\n",
    "logger.info(f\"Number of different projects in BigVul: {len(dataset['project'].unique())}\")\n",
    "\n",
    "project_counts = dataset['project'].value_counts().nlargest(10)\n",
    "logger.info(f\"Top-10 largest projects in BigVul and their size: {project_counts}\")\n",
    "\n",
    "# Choose the selected project to include in the test set.\n",
    "# default = \"all\"\n",
    "selected_project = \"all\" # all # Chrome # linux # Android # qemu # php # ImageMagick # savannah # FFmpeg # ghostscript # openssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf197575-0b5d-4f20-b485-fa26f6008e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:20:37 - INFO - Train data head:     index Access Gained Attack Origin Authentication Required Availability  \\\n",
      "0   48018           NaN         Local            Not required      Partial   \n",
      "1  177287           NaN        Remote            Not required     Complete   \n",
      "2  174089           NaN        Remote            Not required          NaN   \n",
      "3   31021           NaN         Local            Not required          NaN   \n",
      "4  120877           NaN        Remote            Not required          NaN   \n",
      "\n",
      "          CVE ID                                       CVE Page   CWE ID  \\\n",
      "0  CVE-2016-9588  https://www.cvedetails.com/cve/CVE-2016-9588/  CWE-388   \n",
      "1  CVE-2016-2476  https://www.cvedetails.com/cve/CVE-2016-2476/  CWE-119   \n",
      "2  CVE-2016-2460  https://www.cvedetails.com/cve/CVE-2016-2460/  CWE-200   \n",
      "3  CVE-2013-2635  https://www.cvedetails.com/cve/CVE-2013-2635/  CWE-399   \n",
      "4  CVE-2013-2879  https://www.cvedetails.com/cve/CVE-2013-2879/  CWE-200   \n",
      "\n",
      "  Complexity Confidentiality  ... parentID  \\\n",
      "0        Low             NaN  ...      NaN   \n",
      "1     Medium        Complete  ...      NaN   \n",
      "2     Medium         Partial  ...      NaN   \n",
      "3     Medium         Partial  ...      NaN   \n",
      "4     Medium         Partial  ...      NaN   \n",
      "\n",
      "                                               patch  project  \\\n",
      "0  @@ -1389,10 +1389,10 @@ static inline bool nes...    linux   \n",
      "1  @@ -2418,6 +2418,7 @@\\n\\n             : OMX_AU...  Android   \n",
      "2  @@ -349,7 +349,7 @@\\n\\n         }\\n         ca...  Android   \n",
      "3  @@ -979,6 +979,7 @@ static int rtnl_fill_ifinf...    linux   \n",
      "4  @@ -48,10 +48,12 @@ OneClickSigninSyncStarter:...   Chrome   \n",
      "\n",
      "                                       project_after  \\\n",
      "0           ef85b67385436ddc1998f45f1d6a210f935b3388   \n",
      "1  https://android.googlesource.com/platform/fram...   \n",
      "2  https://android.googlesource.com/platform/fram...   \n",
      "3           84d73cd3fb142bf1298a8c13fd4ca50fd2432372   \n",
      "4           afbc71b7a78ac99810a6b22b2b0a2e85dde18794   \n",
      "\n",
      "                                      project_before target  \\\n",
      "0           cc0d907c0907561f108b2f4d4da24e85f18d0ca5      0   \n",
      "1  https://android.googlesource.com/platform/fram...      0   \n",
      "2  https://android.googlesource.com/platform/fram...      0   \n",
      "3           c085c49920b2f900ba716b4ca1c1a55ece9872cc      0   \n",
      "4           1e46f230bd00678488d5b4fce546e965a00ba16b      0   \n",
      "\n",
      "                                   vul_func_with_fix  \\\n",
      "0  static inline bool fixed_bits_valid(u64 val, u...   \n",
      "1  void ACodec::onSignalEndOfInputStream() {\\n   ...   \n",
      "2   virtual status_t setMaxAcquiredBufferCount(in...   \n",
      "3  static void rtnetlink_rcv(struct sk_buff *skb)...   \n",
      "4  void OneClickSigninSyncStarter::SigninDialogDe...   \n",
      "\n",
      "                                      processed_func flaw_line flaw_line_index  \n",
      "0  static inline bool fixed_bits_valid(u64 val, u...       NaN             NaN  \n",
      "1  void ACodec::onSignalEndOfInputStream() {\\n   ...       NaN             NaN  \n",
      "2   virtual status_t setMaxAcquiredBufferCount(in...       NaN             NaN  \n",
      "3  static void rtnetlink_rcv(struct sk_buff *skb)...       NaN             NaN  \n",
      "4  void OneClickSigninSyncStarter::SigninDialogDe...       NaN             NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "2025-02-28 10:20:37 - INFO - Length of training data: 150910\n",
      "2025-02-28 10:20:37 - INFO - Maximum number of words: 15441\n",
      "2025-02-28 10:20:37 - INFO - Value counts of training data: target\n",
      "0    142174\n",
      "1      8736\n",
      "Name: count, dtype: int64\n",
      "2025-02-28 10:20:37 - INFO - Percentages of classes: (6.1445833978083195, '%')\n",
      "2025-02-28 10:20:37 - INFO - Number of categories: 2\n",
      "2025-02-28 10:20:37 - INFO - Train data length: 150910\n",
      "2025-02-28 10:20:37 - INFO - Validation data length: 18863\n",
      "2025-02-28 10:20:37 - INFO - Test data length: 18863\n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "val_ratio = 0.1\n",
    "num_of_ratio = int(val_ratio * len(dataset))\n",
    "data = dataset.iloc[0:-num_of_ratio, :]\n",
    "test_data = dataset.iloc[-num_of_ratio:, :]\n",
    "train_data = data.iloc[0:-num_of_ratio, :]\n",
    "val_data = data.iloc[-num_of_ratio:, :]\n",
    "\n",
    "# if selected_project==\"all\" continue with the whole test_set, else, if one specific project is selected keep only its samples\n",
    "if selected_project != \"all\":\n",
    "    test_data = test_data[test_data['project'] == selected_project]\n",
    "\n",
    "# Shuffle dataset\n",
    "train_data = train_data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "logger.info(f\"Train data head: {train_data.head()}\")\n",
    "logger.info(f\"Length of training data: {len(train_data)}\")\n",
    "\n",
    "train_data = train_data[[\"processed_func\", \"target\", \"flaw_line\", \"flaw_line_index\"]]\n",
    "\n",
    "# Explore data\n",
    "train_data = train_data.dropna(subset=[\"processed_func\"])\n",
    "\n",
    "word_counts = train_data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "logger.info(f\"Maximum number of words: {max_length}\")\n",
    "\n",
    "vc = train_data[\"target\"].value_counts()\n",
    "\n",
    "logger.info(f\"Value counts of training data: {vc}\")\n",
    "\n",
    "logger.info(f\"Percentages of classes: {(vc[1] / vc[0])*100, '%'}\")\n",
    "\n",
    "n_categories = len(vc)\n",
    "logger.info(f\"Number of categories: {n_categories}\")\n",
    "\n",
    "train_data = pd.DataFrame(({'Text': train_data['processed_func'], 'Labels': train_data['target'], 'Lines':train_data['flaw_line'], 'Line_Index':train_data['flaw_line_index']}))\n",
    "#train_data = train_data[0:100]\n",
    "train_data.head()\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Labels': val_data['target'], 'Lines':val_data['flaw_line'], 'Line_Index':val_data['flaw_line_index']}))\n",
    "val_data.head()\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Labels': test_data['target'], 'Lines':test_data['flaw_line'], 'Line_Index':test_data['flaw_line_index']}))\n",
    "\n",
    "logger.info(f\"Train data length: {len(train_data)}\")\n",
    "logger.info(f\"Validation data length: {len(val_data)}\")\n",
    "logger.info(f\"Test data length: {len(test_data)}\")\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9d0a2-db8f-4ab8-8e7f-8136a420cb09",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0ca122-0706-4c41-b12b-e0f23e2a6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing step: Under-sampling\n",
    "\n",
    "sampling = False\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 4 * class_counts[class_counts.idxmin()] # int(class_counts[class_counts.idxmax()] / 2) # 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2)\n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}\n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"])\n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f13f9-7834-48db-81e8-d94b17ca9c30",
   "metadata": {},
   "source": [
    "Get model and apply tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6a1956-7d7d-4f8f-87dd-2149bfc9e464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "# Resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "# # Compute maximum length\n",
    "\n",
    "# X = tokenizer(\n",
    "#         text=X_train.tolist(),\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         return_tensors='pt',\n",
    "#         return_token_type_ids=False,\n",
    "#         return_attention_mask=True,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "# max_len = getMaxLen(X)\n",
    "max_len = 512\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "X_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "X_val = tokenizer(\n",
    "    text=val_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=test_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8d903-df2a-4765-bbe3-03a4e2aff79f",
   "metadata": {},
   "source": [
    "Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837a858c-a15b-4338-9572-64c68150eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 2e-5 #5e-05\n",
    "batch_size = 8 #16\n",
    "patience = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd586096-239a-4e2a-b773-a2be9bce63b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:21:12 - INFO - Device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): RobertaForSequenceClassification(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "No. of trainable parameters:  124647170\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "\n",
    "Y_train = torch.LongTensor(Y_train.tolist())\n",
    "Y_val = torch.LongTensor(val_data[\"Labels\"].tolist())\n",
    "Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "Y_train.size(), Y_val.size(), Y_test.size()\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], Y_train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val[\"input_ids\"], X_val[\"attention_mask\"], Y_val)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "max_steps = len(train_dataloader)*n_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "            num_warmup_steps=max_steps // 5,\n",
    "            num_training_steps=max_steps)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# total_steps = len(train_dataloader) * n_epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py\n",
    "#                                             num_training_steps = total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Device {device}\")\n",
    "\n",
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357a794e-f5b7-4829-a648-413903415129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace \"/~/\" with \"\\n\" in the 'Lines' column\n",
    "def replace_delimiter_with_newline(data):\n",
    "    # Replace \"/~/\" with \"\\n\" in the 'Lines' column\n",
    "    data['Lines'] = data['Lines'].str.replace('/~/', '\\n')\n",
    "    return data\n",
    "\n",
    "test_data = replace_delimiter_with_newline(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e97266-9f34-475a-aa68-e40c26162506",
   "metadata": {},
   "source": [
    "Execution loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e1637e-fb40-426f-a38b-1e52f7b5f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1763810/4242508436.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path, map_location=device)\n",
      "2025-02-28 10:21:16 - INFO - Starting testing...\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2325/2325 [01:47<00:00, 21.61it/s]\n",
      "2025-02-28 10:23:04 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17808\n",
      "           1       0.95      0.93      0.94       788\n",
      "\n",
      "    accuracy                           0.99     18596\n",
      "   macro avg       0.97      0.97      0.97     18596\n",
      "weighted avg       0.99      0.99      0.99     18596\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perception time per sample: 0.005989310599881158\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "# If model is wrapped in DataParallel, load state_dict directly into the underlying model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.load_state_dict(checkpoint['model'])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)\n",
    "\n",
    "# Eliminate Test samples that are vulnerable (target=1) but they have missing line-level labels (flaw lines is nan)\n",
    "REMOVE_MISSING_LINE_LABELS = True # True # False\n",
    "\n",
    "test_time1 = time.time()\n",
    "\n",
    "if REMOVE_MISSING_LINE_LABELS:\n",
    "\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    test_data = test_data[~((test_data['Labels'] == 1) & (test_data['Line_Index'].isna()))]\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "    \n",
    "    \n",
    "    X_test = tokenizer(\n",
    "        text=test_data['Text'].tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # Make predictions\n",
    "    logger.info(\"Starting testing...\")\n",
    "    test_start_time = time.time()\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    test_probas_pred = []\n",
    "    actual_labels = []\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "    \n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "    \n",
    "            loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            logits_array = output.logits.cpu().detach().numpy()\n",
    "            #probs_array = softmax(logits_array, axis=1)\n",
    "            probs_array = torch.softmax(torch.tensor(logits_array), dim=-1).numpy()\n",
    "            \n",
    "            preds = np.argmax(probs_array , axis=-1)\n",
    "            test_pred+=list(preds)\n",
    "            actual_labels+=labels.cpu().numpy().tolist()\n",
    "    \n",
    "            probas = np.max(probs_array , axis=1)\n",
    "            test_probas_pred+=list(probas)\n",
    "    \n",
    "    # compute evaluation metrics\n",
    "    new_class_report = classification_report(actual_labels, test_pred)\n",
    "    logger.info(f\"Classification Report:\\n{new_class_report}\")\n",
    "\n",
    "    test_time2 = time.time()\n",
    "    testing_time = test_time2 - test_time1\n",
    "    print(\"Perception time per sample:\", (testing_time / len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20dfb0e5-4365-4227-b602-383dd02c1114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perception time per sample: 0.005989310599881158\n"
     ]
    }
   ],
   "source": [
    "print(\"Perception time per sample:\", (testing_time / len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83743fc-2ba1-48ab-a479-7fa2e7c05ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify negative predictions ie TN and FN\n",
    "negative_indices = [i for i, pred in enumerate(test_pred) if pred == 0]  # Indices of Negative predictions (TNs + FNs)\n",
    "\n",
    "# Collect lines of negative predictions\n",
    "negative_samples = [test_data['Text'].tolist()[i] for i in negative_indices]  # Extract Negative samples from test data\n",
    "\n",
    "# Flatten\n",
    "all_neg_lines = []\n",
    "for neg_func in negative_samples:\n",
    "    neg_lines = neg_func.split('\\n') #function_to_lines\n",
    "    for neg_line in neg_lines:\n",
    "        all_neg_lines.append(neg_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9cc0bd0-170f-4905-8fbd-4f3a559730eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_TP_Accuracy = True\n",
    "ONLY_TP_CostEffect = False\n",
    "\n",
    "ONLY_TP = ONLY_TP_CostEffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efb486d1-eb1f-4451-a895-b4a4d5238d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:23:05 - INFO - Generating explanations for 778 Positive predictions (TPs and FPs)...\n"
     ]
    }
   ],
   "source": [
    "# Identify True Positives (where the predicted label and actual label are both 1)\n",
    "true_positive_indices = [i for i, (pred, label) in enumerate(zip(test_pred, Y_test.tolist())) if pred == 1 and label == 1]\n",
    "if ONLY_TP:\n",
    "    positive_indices = true_positive_indices\n",
    "    logger.info(f\"Selected {len(true_positive_indices)} True Positives for explanations.\")\n",
    "else:\n",
    "    # Identify True Positives and False Positives\n",
    "    trueNfalse_positive_indices = [i for i, pred in enumerate(test_pred) if pred == 1]  # Indices of Positive predictions (TPs + FPs)\n",
    "    logger.info(f\"Generating explanations for {len(trueNfalse_positive_indices)} Positive predictions (TPs and FPs)...\")\n",
    "    positive_indices = trueNfalse_positive_indices\n",
    "\n",
    "actual_positive_indices = [i for i, label in enumerate(Y_test.tolist()) if label == 1]  # Indices of Actual Positive predictions (TPs + FNs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91bce8e-5a01-4262-be55-eacfe004ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [test_data['Text'].tolist()[i] for i in positive_indices]  # Extract Positive samples from test data\n",
    "\n",
    "positive_lines = [test_data['Lines'].tolist()[i] for i in positive_indices]\n",
    "positive_lines = [\"\" if isinstance(x, float) and math.isnan(x) else x for x in positive_lines]\n",
    "\n",
    "positive_probas = [test_probas_pred[i] for i in positive_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffbc62-bc21-4570-a970-ee3ff457ba74",
   "metadata": {},
   "source": [
    "Apply Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aa98a6b-e1b8-405c-a9d1-06be4fd11f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_data_without_labels(tokenizer, positive_samples):\n",
    "#     input_encodings = tokenizer(\n",
    "#         positive_samples,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding='max_length',\n",
    "#         return_tensors='pt',\n",
    "#         add_special_tokens=True\n",
    "#     )\n",
    "    \n",
    "#     return input_encodings\n",
    "\n",
    "# # Tokenize the test data without labels\n",
    "# test_encodings = tokenize_data_without_labels(tokenizer_seq2seq, positive_samples)\n",
    "\n",
    "# # Create a TensorDataset only with input_ids and attention_mask (no labels)\n",
    "# test_dataset_seq2seq = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n",
    "# test_loader_seq2seq = DataLoader(test_dataset_seq2seq, sampler=SequentialSampler(test_dataset_seq2seq), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e44e0fc-827f-4c93-b902-e2e6fab78489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, positive_samples, positive_lines, max_len_lines):\n",
    "    input_encodings = tokenizer(\n",
    "        positive_samples,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    target_encodings = tokenizer(\n",
    "        positive_lines,\n",
    "        max_length=max_len_lines,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    input_encodings['labels'] = target_encodings['input_ids']\n",
    "    \n",
    "    return input_encodings\n",
    "\n",
    "test_encodings = tokenize_data(tokenizer_seq2seq, positive_samples, positive_lines, max_len_lines)\n",
    "test_dataset_seq2seq = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "test_loader_seq2seq = DataLoader(test_dataset_seq2seq, sampler=SequentialSampler(test_dataset_seq2seq), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "858ca918-1ef4-4684-8442-d1afec2efbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1763810/3721508724.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path_seq2seq, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the CodeT5 model\n",
    "model_seq2seq = AutoModelForSeq2SeqLM.from_pretrained(model_variation_seq2seq)\n",
    "\n",
    "#load model\n",
    "checkpoint = torch.load(save_path_seq2seq, map_location=device)\n",
    "# If model is wrapped in DataParallel, load state_dict directly into the underlying model\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model_seq2seq.module.load_state_dict(checkpoint['model'])\n",
    "# else:\n",
    "#     model_seq2seq.load_state_dict(checkpoint['model'])\n",
    "model_seq2seq.load_state_dict(checkpoint['model'])\n",
    "print(model_seq2seq.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4b3ed46-076b-4adc-ba8c-67c644895c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:23:08 - INFO - Starting testing...\n",
      "Testing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [02:41<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed after 161.55168843269348\n",
      "Perception time per sample: 0.20764998513199676\n",
      "Perception time per sample: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing set\n",
    "logger.info(\"Starting testing...\")\n",
    "test_start_time = time.time()\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_loader_seq2seq, desc='Testing')):\n",
    "        input_ids, attention_mask, labels = [data.to(device) for data in batch_data]\n",
    "\n",
    "        # Generate predictions\n",
    "        # if torch.cuda.device_count() > 1:\n",
    "        #     outputs = model_seq2seq.module.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "        # else:\n",
    "        #     outputs = model_seq2seq.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "        outputs = model_seq2seq.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_len_lines)\n",
    "        \n",
    "        # Decode predicted sequences and actual labels\n",
    "        decoded_preds = tokenizer_seq2seq.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer_seq2seq.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        test_preds.extend(decoded_preds)\n",
    "        actual_labels.extend(decoded_labels)\n",
    "\n",
    "test_end_time = time.time()\n",
    "testing_time = test_end_time - test_start_time\n",
    "\n",
    "# Display the total testing time and average time per sample\n",
    "print(\"Testing completed after\", testing_time)\n",
    "print(\"Perception time per sample:\", (testing_time / len(test_preds)))\n",
    "print(\"Perception time per sample:\", int(testing_time / len(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed2effe-82bf-49ac-881f-0e2d6e124f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:25:50 - INFO - Mean predicted length: 6.574550128534704\n",
      "2025-02-28 10:25:50 - INFO - Median predicted length: 2.0\n"
     ]
    }
   ],
   "source": [
    "# compute the average number of lines predicted as vulnerable by the seq2seq model\n",
    "pred_lens = []\n",
    "for i in range(0, len(test_preds)):\n",
    "    pred_lens.append(len(test_preds[i].split('\\n')))\n",
    "mean_pred_len = statistics.mean(pred_lens)\n",
    "med_pred_len = statistics.median(pred_lens)\n",
    "logger.info(f\"Mean predicted length: {mean_pred_len}\")\n",
    "logger.info(f\"Median predicted length: {med_pred_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1b3905-ddbf-420c-bdeb-9f7b4275aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:25:50 - INFO - Mean actual flaw length: 4.769922879177378\n",
      "2025-02-28 10:25:50 - INFO - Median actual flaw length: 2.0\n"
     ]
    }
   ],
   "source": [
    "# compute the average number of lines that are actual vulnerable lines\n",
    "actual_vuln_lens = []\n",
    "for i in range(0, len(positive_lines)):\n",
    "    actual_vuln_lens.append(len(positive_lines[i].split('\\n')))\n",
    "mean_actual_vuln_len = statistics.mean(actual_vuln_lens)\n",
    "med_actual_vuln_len = statistics.median(actual_vuln_lens)\n",
    "logger.info(f\"Mean actual flaw length: {mean_actual_vuln_len}\")\n",
    "logger.info(f\"Median actual flaw length: {med_actual_vuln_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87cc5d92-410b-4060-81b6-0b9caa4def88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accurary(test_preds, real_positive_lines):\n",
    "    accuracy = 0\n",
    "    for i in range(0, len(test_preds)):\n",
    "        if test_preds[i] == real_positive_lines[i]:\n",
    "            accuracy += 1\n",
    "    accuracy = accuracy / len(test_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6c2d3c7-6416-4096-956e-224fb63419fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:25:50 - INFO - Accuracy: (58.354755784061695, '%')\n"
     ]
    }
   ],
   "source": [
    "# compute simple accuracy: In how many functions the seq2seq model identified the vulnerable lines 100%\n",
    "accuracy = calc_accurary(test_preds, positive_lines)\n",
    "logger.info(f\"Accuracy: {accuracy*100, '%'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60bfb475-a344-4390-98cd-d1e774853e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:25:50 - INFO - Accuracy on truncated labels: (65.93830334190231, '%')\n"
     ]
    }
   ],
   "source": [
    "# compute simple accuracy with truncated output: In how many functions the seq2seq model identified the vulnerable lines 100%, \n",
    "#considering the actual vulnerable lines truncated in max_len\n",
    "accuracy_trunc = calc_accurary(test_preds, actual_labels)\n",
    "logger.info(f\"Accuracy on truncated labels: {accuracy_trunc*100, '%'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17c2d94a-ceeb-4e76-a747-84a5bd1cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy metrics using the most similar lines of the predicted to handle hallucinations\n",
    "\n",
    "def get_line_embeddings(lines, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Get the embeddings for a list of lines using a CodeT5 model.\n",
    "    \n",
    "    Args:\n",
    "    lines (list of str): The lines of code to embed.\n",
    "    tokenizer: The tokenizer for the CodeT5 model.\n",
    "    model: The CodeT5 model.\n",
    "    \n",
    "    Returns:\n",
    "    embeddings (torch.Tensor): A tensor containing the embeddings for each line.\n",
    "    \"\"\"\n",
    "    # Tokenize the input lines\n",
    "    inputs = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get the model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    \n",
    "    # Extract the last hidden state\n",
    "    hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "    # To get a single embedding per line, we can mean-pool the hidden states across the sequence dimension\n",
    "    # or use just the first token's representation, depending on your task.\n",
    "    # Here, we'll use mean-pooling:\n",
    "    embeddings = hidden_states.mean(dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "    \n",
    "    return embeddings.cpu().numpy()  # Return the embeddings as a NumPy array\n",
    "    \n",
    "\n",
    "def get_most_similar_line(predicted_line, original_lines, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Find the most similar line from original lines based on cosine similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    predicted_embedding = get_line_embeddings([predicted_line], tokenizer, model)[0]\n",
    "    original_embeddings = get_line_embeddings(original_lines, tokenizer, model)\n",
    "\n",
    "    cosine_similarities = cosine_similarity([predicted_embedding], original_embeddings).flatten()\n",
    "    \n",
    "    # lines = [predicted_line] + original_lines  # Combine predicted with original lines\n",
    "    \n",
    "    # # Compute TF-IDF matrix\n",
    "    # vectorizer = TfidfVectorizer().fit_transform(lines)\n",
    "    # vectors = vectorizer.toarray()\n",
    "    \n",
    "    # # Calculate cosine similarity between the first line (predicted) and the rest\n",
    "    # cosine_similarities = cosine_similarity([vectors[0]], vectors[1:])\n",
    "    \n",
    "    most_similar_idx = np.argmax(cosine_similarities)  # Find the index of the most similar line\n",
    "    \n",
    "    return original_lines[most_similar_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897bb3a-ee9d-4bf6-9e0f-b61f23ac586d",
   "metadata": {},
   "source": [
    "Choose whether to replace predicted lines with the most similar lines in the function to handle hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aaa678c3-498c-46bd-a3a2-b560b903f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:32:54 - INFO - Accuracy: (61.43958868894601, '%')\n",
      "2025-02-28 10:32:54 - INFO - Accuracy on truncated labels: (60.9254498714653, '%')\n"
     ]
    }
   ],
   "source": [
    "SIMILARITY_REPLACEMENT = True\n",
    "\n",
    "if SIMILARITY_REPLACEMENT:\n",
    "\n",
    "    test_preds_similar = []\n",
    "    for i, pred in enumerate(test_preds):\n",
    "        predicted_lines  = pred.split('\\n')\n",
    "        original_lines  = positive_samples[i].split('\\n')\n",
    "        similar_str = ''\n",
    "        for j, predicted_line in enumerate(predicted_lines):\n",
    "            if predicted_line not in original_lines and j < len(predicted_lines)-1: # to avoid to spoil a correct line AND to just to avoid a difference with the actual_labels in the evaluation\n",
    "                similar_line = get_most_similar_line(predicted_line, original_lines, tokenizer_seq2seq, model_seq2seq)\n",
    "            else:\n",
    "                similar_line = predicted_line\n",
    "            if j == 0:\n",
    "                similar_str += similar_line\n",
    "            else:\n",
    "                similar_str += '\\n' + similar_line\n",
    "                \n",
    "        test_preds_similar.append(similar_str) \n",
    "    \n",
    "    accuracy_similar = calc_accurary(test_preds_similar, positive_lines)\n",
    "    logger.info(f\"Accuracy: {accuracy_similar*100, '%'}\")\n",
    "    \n",
    "    accuracy_similar_trunc = calc_accurary(test_preds_similar, actual_labels)\n",
    "    logger.info(f\"Accuracy on truncated labels: {accuracy_similar_trunc*100, '%'}\")\n",
    "    \n",
    "    test_preds = test_preds_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db9f8c9a-6652-413b-a856-f27b1e50c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb1b79cb-c074-4bc9-ba30-003360f65587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n",
      "const char *type,\n",
      "const char *description,\n",
      "const void *payload,\n",
      "size_t plen,\n",
      "key_perm_t perm,\n",
      "unsigned long flags)\n",
      "{\n",
      "struct keyring_index_key index_key = {\n",
      ".description\t= description,\n",
      "};\n",
      "struct key_preparsed_payload prep;\n",
      "struct assoc_array_edit *edit;\n",
      "const struct cred *cred = current_cred();\n",
      "struct key *keyring, *key = NULL;\n",
      "key_ref_t key_ref;\n",
      "int ret;\n",
      "\n",
      "/* look up the key type to see if it's one of the registered kernel\n",
      "* types */\n",
      "index_key.type = key_type_lookup(type);\n",
      "if (IS_ERR(index_key.type)) {\n",
      "key_ref = ERR_PTR(-ENODEV);\n",
      "goto error;\n",
      "}\n",
      "\n",
      "key_ref = ERR_PTR(-EINVAL);\n",
      "\tif (!index_key.type->match || !index_key.type->instantiate ||\n",
      "(!index_key.description && !index_key.type->preparse))\n",
      "goto error_put_type;\n",
      "\n",
      "keyring = key_ref_to_ptr(keyring_ref);\n",
      "\n",
      "key_check(keyring);\n",
      "\n",
      "key_ref = ERR_PTR(-ENOTDIR);\n",
      "if (keyring->type != &key_type_keyring)\n",
      "goto error_put_type;\n",
      "\n",
      "memset(&prep, 0, sizeof(prep));\n",
      "prep.data = payload;\n",
      "prep.datalen = plen;\n",
      "prep.quotalen = index_key.type->def_datalen;\n",
      "prep.trusted = flags & KEY_ALLOC_TRUSTED;\n",
      "prep.expiry = TIME_T_MAX;\n",
      "if (index_key.type->preparse) {\n",
      "ret = index_key.type->preparse(&prep);\n",
      "if (ret < 0) {\n",
      "key_ref = ERR_PTR(ret);\n",
      "goto error_free_prep;\n",
      "}\n",
      "if (!index_key.description)\n",
      "index_key.description = prep.description;\n",
      "key_ref = ERR_PTR(-EINVAL);\n",
      "if (!index_key.description)\n",
      "goto error_free_prep;\n",
      "}\n",
      "index_key.desc_len = strlen(index_key.description);\n",
      "\n",
      "key_ref = ERR_PTR(-EPERM);\n",
      "if (!prep.trusted && test_bit(KEY_FLAG_TRUSTED_ONLY, &keyring->flags))\n",
      "goto error_free_prep;\n",
      "flags |= prep.trusted ? KEY_ALLOC_TRUSTED : 0;\n",
      "\n",
      "ret = __key_link_begin(keyring, &index_key, &edit);\n",
      "if (ret < 0) {\n",
      "key_ref = ERR_PTR(ret);\n",
      "goto error_free_prep;\n",
      "}\n",
      "\n",
      "/* if we're going to allocate a new key, we're going to have\n",
      "* to modify the keyring */\n",
      "ret = key_permission(keyring_ref, KEY_NEED_WRITE);\n",
      "if (ret < 0) {\n",
      "key_ref = ERR_PTR(ret);\n",
      "goto error_link_end;\n",
      "}\n",
      "\n",
      "/* if it's possible to update this type of key, search for an existing\n",
      "* key of the same type and description in the destination keyring and\n",
      "* update that instead if possible\n",
      "*/\n",
      "if (index_key.type->update) {\n",
      "key_ref = find_key_to_update(keyring_ref, &index_key);\n",
      "if (key_ref)\n",
      "goto found_matching_key;\n",
      "}\n",
      "\n",
      "/* if the client doesn't provide, decide on the permissions we want */\n",
      "if (perm == KEY_PERM_UNDEF) {\n",
      "perm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n",
      "perm |= KEY_USR_VIEW;\n",
      "\n",
      "if (index_key.type->read)\n",
      "perm |= KEY_POS_READ;\n",
      "\n",
      "if (index_key.type == &key_type_keyring ||\n",
      "index_key.type->update)\n",
      "perm |= KEY_POS_WRITE;\n",
      "}\n",
      "\n",
      "/* allocate a new key */\n",
      "key = key_alloc(index_key.type, index_key.description,\n",
      "cred->fsuid, cred->fsgid, cred, perm, flags);\n",
      "if (IS_ERR(key)) {\n",
      "key_ref = ERR_CAST(key);\n",
      "goto error_link_end;\n",
      "}\n",
      "\n",
      "/* instantiate it and link it into the target keyring */\n",
      "ret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n",
      "if (ret < 0) {\n",
      "key_put(key);\n",
      "key_ref = ERR_PTR(ret);\n",
      "goto error_link_end;\n",
      "}\n",
      "\n",
      "key_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n",
      "\n",
      "error_link_end:\n",
      "__key_link_end(keyring, &index_key, edit);\n",
      "error_free_prep:\n",
      "if (index_key.type->preparse)\n",
      "index_key.type->free_preparse(&prep);\n",
      "error_put_type:\n",
      "key_type_put(index_key.type);\n",
      "error:\n",
      "return key_ref;\n",
      "\n",
      "found_matching_key:\n",
      "/* we found a matching key, so we're going to try to update it\n",
      "* - we can drop the locks first as we have the key pinned\n",
      "*/\n",
      "__key_link_end(keyring, &index_key, edit);\n",
      "\n",
      "key_ref = __key_update(key_ref, &prep);\n",
      "goto error_free_prep;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(positive_samples[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "058880ac-f79a-4f95-82cd-11058fc90e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tif (!index_key.type->match || !index_key.type->instantiate ||\n"
     ]
    }
   ],
   "source": [
    "print(positive_lines[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0c96491-6338-495d-a0d2-dc356e03e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tif (!index_key.type->match ||!index_key.type->instantiate ||\n"
     ]
    }
   ],
   "source": [
    "print(actual_labels[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4104567b-2489-4923-998e-f60ba11fc722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tif (!index_key.type->match ||!index_key.type->instantiate ||\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0eba140b-9d20-42a8-887c-25d56bdefd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[num] == actual_labels[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "147230b9-f443-4e21-a774-2037a8fefd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(test_preds[num] == positive_lines[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fb5f2-c8bb-4e68-b45b-3ba33625247a",
   "metadata": {},
   "source": [
    "Rank the lines based on the predictions of the seq2seq model and their position in the original functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95a7fa38-492a-48ed-95f4-669af959e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranked_lines = []\n",
    "all_predicted_lines_number = []\n",
    "\n",
    "# the ranked list per function will contain first the predicted lines by the seq2seq model and then the rest lines of the original function \n",
    "# with order based on their original position\n",
    "all_predicted_lines = []\n",
    "for pred in test_preds:\n",
    "    predicted_lines = pred.split('\\n')\n",
    "    all_predicted_lines.append(predicted_lines)\n",
    "    all_ranked_lines.append(predicted_lines.copy())\n",
    "    all_predicted_lines_number.append(len(predicted_lines))\n",
    "\n",
    "for i, pos_sample in enumerate(positive_samples):\n",
    "    original_lines = pos_sample.split('\\n')\n",
    "    for orig_line in original_lines:\n",
    "        if orig_line not in all_predicted_lines[i]:\n",
    "            all_ranked_lines[i].append(orig_line)\n",
    "\n",
    "# restructure labels\n",
    "str_labels = positive_lines # actual_labels # positive_lines\n",
    "all_flaw_lines = []\n",
    "for label in str_labels:\n",
    "    label_lines = label.split('\\n')\n",
    "    all_flaw_lines.append(label_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8819f-ad5c-436a-9de9-ad9e0442684c",
   "metadata": {},
   "source": [
    "Line-level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bf62942-8670-4cc2-87fd-a1b97cda20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "# Function to compute Top-X Accuracy for each function\n",
    "def compute_top_x_accuracy(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Top-X Accuracy: Measures whether at least one actual vulnerable line appears in the top-X ranking.\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: 1 if at least one vulnerable line is in the top-X, else 0.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "\n",
    "    return 1 if any(line in flaw_lines for line in top_x_lines) else 0\n",
    "\n",
    "\n",
    "def compute_reciprocal_rank(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Reciprocal Rank for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :return: Reciprocal rank for this function, or 0 if no vulnerable line is found in the ranking.\n",
    "    \"\"\"\n",
    "\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "    for i, line in enumerate(top_x_lines):\n",
    "        if line in flaw_lines:\n",
    "            return 1 / (i + 1)  # Reciprocal of the rank of the first relevant item\n",
    "    return 0  # If no relevant item is found\n",
    "    \n",
    "\n",
    "# Function to compute Initial False Alarm (IFA)\n",
    "def compute_ifa(ranked_lines, flaw_lines):\n",
    "    \"\"\"\n",
    "    Compute Initial False Alarm (IFA): Counts how many false alarms (non-vulnerable lines) occur before the first vulnerable line.\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices.\n",
    "    :return: Number of false alarms until the first vulnerable line is found.\n",
    "    \"\"\"\n",
    "    ifa = 0\n",
    "    for line in ranked_lines:\n",
    "        if line not in flaw_lines:\n",
    "            ifa += 1\n",
    "        else:\n",
    "            break  # Stop counting when the first vulnerable line is found\n",
    "    return ifa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bedfc2ff-98d8-4fa9-b83f-fba03af2c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Top-X Precision for each function\n",
    "def compute_top_x_precision(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Top-X Precision: Measures how many lines are indeed vulnerable in the top-X ranking.\n",
    "\n",
    "    Relevant retrieved instances divided by all retrieved instances\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: Number of the number of vulnerable lines included in the top-X ranking divided by X.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "\n",
    "    count = 0\n",
    "    for line in top_x_lines:\n",
    "        if line in flaw_lines:\n",
    "            count += 1\n",
    "\n",
    "    return count / top_x\n",
    "\n",
    "\n",
    "# Function to compute Top-X Recall for each function\n",
    "def compute_top_x_recall(ranked_lines, flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Compute Top-X Recall: Measures how many of the function's vulnerable lines can be found by searching in the top-X ranking.\n",
    "\n",
    "    Relevant retrieved instances divided by all relevant instances\n",
    "    \n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param top_x: The number of top lines to consider (default is 10).\n",
    "    :return: Number of the number of vulnerable lines included in the top-X ranking divided by the total number of vulnerable lines in the function.\n",
    "    \"\"\"\n",
    "    top_x_lines = ranked_lines[:top_x]  # Get the top-X ranked lines\n",
    "\n",
    "    count = 0\n",
    "    for line in top_x_lines:\n",
    "        if line in flaw_lines:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(flaw_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7ce96b0-083f-4aa2-9f9d-a16f0e796827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision_at_k(ranked_lines, flaw_lines, k):\n",
    "    \"\"\"\n",
    "    Compute Average Precision at K for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param k: The number of top lines to consider for AP@K.\n",
    "    :return: Average Precision at K for this function.\n",
    "    \"\"\"\n",
    "    relevant_found = 0\n",
    "    precision_sum = 0\n",
    "    top_k_lines = ranked_lines[:k]  # Consider only the top K lines\n",
    "\n",
    "    for i, line in enumerate(top_k_lines):\n",
    "        if line in flaw_lines:\n",
    "            relevant_found += 1\n",
    "            precision_sum += relevant_found / (i + 1)  # Precision at this rank\n",
    "\n",
    "    return precision_sum / relevant_found if relevant_found>0 else 0  # Avoid division by zero\n",
    "    #return precision_sum / min(k, len(flaw_lines)) if flaw_lines else 0  # Avoid division by zero\n",
    "\n",
    "\n",
    "def compute_average_recall_at_k(ranked_lines, flaw_lines, k):\n",
    "    \"\"\"\n",
    "    Compute Average Recall at K for a single function.\n",
    "\n",
    "    :param ranked_lines: List of tuples (line_index, line_text, score) sorted by score.\n",
    "    :param flaw_lines: List of actual vulnerable line indices (integers).\n",
    "    :param k: The number of top lines to consider for AR@K.\n",
    "    :return: Average Recall at K for this function.\n",
    "    \"\"\"\n",
    "    relevant_found = 0\n",
    "    precision_sum = 0\n",
    "    top_k_lines = ranked_lines[:k]  # Consider only the top K lines\n",
    "\n",
    "    for i, line in enumerate(top_k_lines):\n",
    "        if line in flaw_lines:\n",
    "            relevant_found += 1\n",
    "            precision_sum += relevant_found / len(flaw_lines)  # Precision at this rank\n",
    "\n",
    "    return precision_sum / relevant_found if relevant_found>0 else 0  # Avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b225fcad-892d-4e93-9dd1-ab9791694b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Effectiveness metrics\n",
    "\n",
    "# Helper functions\n",
    "# Compute total LOC of the testing set\n",
    "def compute_total_loc(all_total_locs):  \n",
    "    return sum(all_total_locs)\n",
    "\n",
    "def compute_total_flaw_lines(all_flaw_lines):\n",
    "    total_flaw_loc = 0\n",
    "    for flaw_lines in all_flaw_lines:\n",
    "        total_flaw_loc+=len(flaw_lines)\n",
    "\n",
    "    return total_flaw_loc\n",
    "\n",
    "def find_effort_breakpoint(flaw_lines_num, x_percent):    \n",
    "    return max(1, ((x_percent/100) * flaw_lines_num))\n",
    "\n",
    "def find_recall_breakpoint(total_test_loc, x_percent):    \n",
    "    return max(1, ((x_percent/100) * total_test_loc))\n",
    "\n",
    "# Prepare data for Cost-Effectiveness calculation\n",
    "# Sort the ranked_lines based on their function proba\n",
    "def sort_all_ranked_lines(positive_probas, all_ranked_lines):\n",
    "    combined = list(zip(positive_probas, all_ranked_lines))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    all_ranked_lines_sorted = [item[1] for item in combined_sorted]\n",
    "    \n",
    "    return all_ranked_lines_sorted\n",
    "\n",
    "# # Sort the flaw_lines based on their function proba\n",
    "# def sort_all_flaw_lines(positive_probas, all_flaw_lines):\n",
    "\n",
    "#     combined = list(zip(positive_probas, all_flaw_lines))\n",
    "#     combined_sorted = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "#     all_flaw_lines_sorted = [item[1] for item in combined_sorted]\n",
    "\n",
    "#     return all_flaw_lines_sorted\n",
    "    \n",
    "\n",
    "# Function to compute Effort@X%Recall by sorting functions\n",
    "def compute_effort_at_x_percent_recall_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=20):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_ranked_lines_sorted = sort_all_ranked_lines(positive_probas, all_ranked_lines)\n",
    "    #all_flaw_lines_sorted = sort_all_flaw_lines(positive_probas, all_flaw_lines)\n",
    "    all_flaw_lines_sorted = sort_all_ranked_lines(positive_probas, all_flaw_lines)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    effort_breakpoint = find_effort_breakpoint(flaw_lines_num, x_percent)\n",
    "\n",
    "    if flaw_lines_num == 0:\n",
    "        return 1.0  # If no vulnerable lines, maximum effort (full LOC inspected)\n",
    "\n",
    "    # Iterate over ranked lines to count how much effort (LOC) is spent to find X% of the vulnerable lines\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    found = False\n",
    "    for i, fun_lines in enumerate(all_ranked_lines_sorted):\n",
    "        fun_flaws = all_flaw_lines_sorted[i]\n",
    "        for line in fun_lines:\n",
    "            inspected_lines += 1\n",
    "\n",
    "            if line in fun_flaws:\n",
    "                found_vulnerable_lines += 1\n",
    "\n",
    "            # Stop when we find X% of vulnerable lines\n",
    "            if found_vulnerable_lines >= effort_breakpoint:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    return inspected_lines / total_test_loc\n",
    "\n",
    "# Assign labels for all sorted lines\n",
    "def create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines, all_predicted_lines_number):\n",
    "\n",
    "    all_lines_with_labels = []\n",
    "    for func_idx, ranked_lines in enumerate(all_ranked_lines):\n",
    "        flaw_lines = all_flaw_lines[func_idx]\n",
    "        \n",
    "        for line_idx, line_content in enumerate(ranked_lines):\n",
    "            if line_idx < all_predicted_lines_number[func_idx]:\n",
    "                line_score = 1 # lines predicted as vulnerable\n",
    "            else:\n",
    "                line_score = 0 # lines not predicted as vulnerable\n",
    "                \n",
    "            if line_content in flaw_lines:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            all_lines_with_labels.append((line_content, line_score, label))\n",
    "\n",
    "    sorted_lines_with_labels = sorted(all_lines_with_labels, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_lines_with_labels\n",
    "\n",
    "# Assign labels for all sorted lines and sorted functions\n",
    "def create_sorted_lines_with_labels_and_probas(all_ranked_lines, positive_probas, all_flaw_lines, all_predicted_lines_number):\n",
    "\n",
    "    all_ranked_lines_sorted = sort_all_ranked_lines(positive_probas, all_ranked_lines)\n",
    "    all_flaw_lines_sorted = sort_all_ranked_lines(positive_probas, all_flaw_lines)\n",
    "    all_predicted_lines_number_sorted = sort_all_ranked_lines(positive_probas, all_predicted_lines_number)\n",
    "    \n",
    "    all_lines_with_labels = []\n",
    "    for func_idx, ranked_lines in enumerate(all_ranked_lines_sorted):\n",
    "        flaw_lines = all_flaw_lines_sorted[func_idx]\n",
    "        \n",
    "        for line_idx, line_content in enumerate(ranked_lines):\n",
    "            if line_idx < all_predicted_lines_number_sorted[func_idx]:\n",
    "                line_score = 1 # lines predicted as vulnerable\n",
    "            else:\n",
    "                line_score = 0 # lines not predicted as vulnerable\n",
    "                \n",
    "            if line_content in flaw_lines:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            all_lines_with_labels.append((line_content, line_score, label))\n",
    "\n",
    "    # # Separate lines with line_score=0 and line_score=1\n",
    "    # vulnerable_lines = [line for line in all_lines_with_labels if line[1] == 1]\n",
    "    # non_vulnerable_lines = [line for line in all_lines_with_labels if line[1] == 0]\n",
    "\n",
    "    # # Shuffle only the non-vulnerable lines\n",
    "    # random.shuffle(non_vulnerable_lines)\n",
    "\n",
    "    # # Combine the shuffled non-vulnerable lines with the vulnerable lines\n",
    "    # sorted_lines_with_labels = vulnerable_lines + non_vulnerable_lines\n",
    "    \n",
    "    sorted_lines_with_labels = sorted(all_lines_with_labels, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_lines_with_labels\n",
    "\n",
    "# Function to compute Effort@X%Recall by sorting all lines\n",
    "def compute_effort_at_x_percent_recall_rankedLines(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, all_total_locs, all_predicted_lines_number, x_percent=20):\n",
    "    \n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    #all_labels_lines_sorted = create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines, all_predicted_lines_number) # contains the label (vulnerable or not) of each line in the sorted lines\n",
    "    all_labels_lines_sorted = create_sorted_lines_with_labels_and_probas(all_ranked_lines, positive_probas, all_flaw_lines, all_predicted_lines_number)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "\n",
    "    if flaw_lines_num == 0:\n",
    "        return 1.0  # If no vulnerable lines, maximum effort (full LOC inspected)\n",
    "\n",
    "    effort_breakpoint = find_effort_breakpoint(flaw_lines_num, x_percent)\n",
    "\n",
    "    # Iterate over ranked lines to count how much effort (LOC) is spent to find X% of the vulnerable lines\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    for i in range(0, len(all_labels_lines_sorted)):\n",
    "        _, _, line_label = all_labels_lines_sorted[i]\n",
    "        inspected_lines += 1\n",
    "        if line_label == 1:\n",
    "            found_vulnerable_lines += 1\n",
    "\n",
    "        # Stop when we find X% of vulnerable lines\n",
    "        if found_vulnerable_lines >= effort_breakpoint:\n",
    "            break\n",
    "\n",
    "    return inspected_lines / total_test_loc\n",
    "    \n",
    "\n",
    "# Function to compute Recall@1%LOC by sorting functions\n",
    "def compute_recall_at_x_percent_loc_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, all_total_locs, x_percent=1):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    all_ranked_lines_sorted = sort_all_ranked_lines(positive_probas, all_ranked_lines)\n",
    "    #all_flaw_lines_sorted = sort_all_flaw_lines(positive_probas, all_flaw_lines)\n",
    "    all_flaw_lines_sorted = sort_all_ranked_lines(positive_probas, all_flaw_lines)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    recall_breakpoint = find_recall_breakpoint(total_test_loc, x_percent)\n",
    "\n",
    "    # Count how many vulnerable lines are found within the top X% LOC\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    found = False\n",
    "    for i, fun_lines in enumerate(all_ranked_lines_sorted):\n",
    "        fun_flaws = all_flaw_lines_sorted[i]\n",
    "        for line in fun_lines:\n",
    "            inspected_lines += 1\n",
    "\n",
    "            if line in fun_flaws:\n",
    "                found_vulnerable_lines += 1\n",
    "\n",
    "            # Stop when we find X% of vulnerable lines\n",
    "            if inspected_lines >= recall_breakpoint:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    return found_vulnerable_lines / flaw_lines_num\n",
    "\n",
    "# Function to compute Recall@1%LOC by sorting all lines\n",
    "def compute_recall_at_x_percent_loc_rankedLines(all_ranked_lines, positive_probas, all_neg_lines, all_flaw_lines, test_all_flaw_lines, all_total_locs, all_predicted_lines_number, x_percent=1):\n",
    "\n",
    "    # Prepare data for Cost-Effectiveness calculation\n",
    "    #all_labels_lines_sorted = create_sorted_lines_with_labels(all_ranked_lines, all_flaw_lines, all_predicted_lines_number) # contains the label (vulnerable or not) of each line in the sorted lines\n",
    "    all_labels_lines_sorted = create_sorted_lines_with_labels_and_probas(all_ranked_lines, positive_probas, all_flaw_lines, all_predicted_lines_number)\n",
    "    \n",
    "    total_test_loc = compute_total_loc(all_total_locs)\n",
    "\n",
    "    flaw_lines_num = compute_total_flaw_lines(test_all_flaw_lines)\n",
    "    \n",
    "    recall_breakpoint = find_recall_breakpoint(total_test_loc, x_percent)\n",
    "\n",
    "    # Count how many vulnerable lines are found within the top X% LOC\n",
    "    inspected_lines = 0\n",
    "    found_vulnerable_lines = 0\n",
    "    inspect_neg_lines = True\n",
    "    for i in range(0, len(all_labels_lines_sorted)):\n",
    "        inspected_lines += 1\n",
    "        _, _, line_label = all_labels_lines_sorted[i]\n",
    "\n",
    "        if line_label == 1:\n",
    "            found_vulnerable_lines += 1\n",
    "\n",
    "        if inspected_lines >= recall_breakpoint:\n",
    "            inspect_neg_lines = False\n",
    "            break\n",
    "\n",
    "    if inspect_neg_lines:\n",
    "        for neg_line in all_neg_lines:\n",
    "            inspected_lines += 1\n",
    "            if inspected_lines >= recall_breakpoint:\n",
    "                break\n",
    "            \n",
    "    return found_vulnerable_lines / flaw_lines_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8b5eb83-17d7-47d9-9255-936244c22377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate all metrics for each function\n",
    "def evaluate_vulnerability_detection(all_ranked_lines, all_flaw_lines, top_x):\n",
    "    \"\"\"\n",
    "    Evaluate the XAI methods using Top-X Accuracy, IFA, Effort@X%Recall, Recall@X%LOC for all functions.\n",
    "\n",
    "    :param all_ranked_lines: List of ranked lines for all functions.\n",
    "    :param all_flaw_lines: List of actual vulnerable line indices for all functions.\n",
    "    :param top_x: Number of top-ranked lines to consider for Top-X Accuracy.\n",
    "    :return: DataFrame with individual and average results for each function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, ranked_lines in enumerate(all_ranked_lines):\n",
    "        \n",
    "        flaw_lines = all_flaw_lines[i]\n",
    "        \n",
    "        # Compute each metric\n",
    "        top_x_accuracy = compute_top_x_accuracy(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        top_x_precision = compute_top_x_precision(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        top_x_recall = compute_top_x_recall(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        rr = compute_reciprocal_rank(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        apk = compute_average_precision_at_k(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        ark = compute_average_recall_at_k(ranked_lines, flaw_lines, top_x)\n",
    "\n",
    "        \n",
    "        ifa = compute_ifa(ranked_lines, flaw_lines)\n",
    "\n",
    "        result = {\n",
    "            f'Top-{top_x} Accuracy': top_x_accuracy,\n",
    "            f'Top-{top_x} Precision': top_x_precision,\n",
    "            f'Top-{top_x} Recall': top_x_recall,\n",
    "            f'Reciprocal Rank-{top_x}': rr,\n",
    "            f'AP@{top_x}': apk,\n",
    "            f'AR@{top_x}': ark,\n",
    "            'IFA': ifa\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Compute average results\n",
    "    average_results = results_df.mean().to_dict()\n",
    "    average_results['Type'] = 'Average'\n",
    "\n",
    "    # Compute median results\n",
    "    median_results = results_df.median().to_dict()\n",
    "    median_results['Type'] = 'Median'\n",
    "\n",
    "    # Add individual results and average to the final DataFrame\n",
    "    results_df['Type'] = 'Individual'\n",
    "    \n",
    "    average_results_df = pd.DataFrame([average_results])\n",
    "    median_results_df = pd.DataFrame([median_results])\n",
    "\n",
    "    # Combine individual and average results\n",
    "    final_results_df = pd.concat([results_df, average_results_df, median_results_df], ignore_index=True)\n",
    "    \n",
    "    return final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d41f6d4-bbae-4cb4-97ac-e656b1e88531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Top-10 Accuracy  Top-10 Precision  Top-10 Recall  Reciprocal Rank-10  \\\n",
      "0           0.000000          0.000000       0.000000            0.000000   \n",
      "1           1.000000          0.300000       1.000000            1.000000   \n",
      "2           1.000000          0.300000       1.000000            1.000000   \n",
      "3           1.000000          1.000000       0.625000            1.000000   \n",
      "4           1.000000          0.200000       1.000000            1.000000   \n",
      "..               ...               ...            ...                 ...   \n",
      "775         1.000000          0.200000       1.000000            1.000000   \n",
      "776         1.000000          0.200000       1.000000            1.000000   \n",
      "777         1.000000          0.200000       0.200000            1.000000   \n",
      "778         0.822622          0.270566       0.779379            0.789611   \n",
      "779         1.000000          0.100000       1.000000            1.000000   \n",
      "\n",
      "       AP@10     AR@10        IFA        Type  \n",
      "0    0.00000  0.000000  60.000000  Individual  \n",
      "1    1.00000  0.666667   0.000000  Individual  \n",
      "2    1.00000  0.666667   0.000000  Individual  \n",
      "3    1.00000  0.343750   0.000000  Individual  \n",
      "4    1.00000  0.750000   0.000000  Individual  \n",
      "..       ...       ...        ...         ...  \n",
      "775  1.00000  0.750000   0.000000  Individual  \n",
      "776  1.00000  0.750000   0.000000  Individual  \n",
      "777  1.00000  0.150000   0.000000  Individual  \n",
      "778  0.78761  0.629749  24.992288     Average  \n",
      "779  1.00000  0.666667   0.000000      Median  \n",
      "\n",
      "[780 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Results based on per function accuracy\n",
    "\n",
    "# Usage:\n",
    "top_x = 10\n",
    "final_results_df = evaluate_vulnerability_detection(all_ranked_lines, all_flaw_lines, top_x)\n",
    "\n",
    "# Display Accuracy Results per Function\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "652c23dd-933c-4fd9-b36d-b1b73771d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifa_all = final_results_df[\"IFA\"]\n",
    "ifa_ = ifa_all.iloc[0:-2]\n",
    "ifa_.to_csv('ifa_locvul.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c235164-ff8e-4b75-8517-b0c2dc4895e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for line-level evaluation of cost-effectiveness\n",
    "test_all_flaw_lines = [test_data['Line_Index'].tolist()[i] for i in actual_positive_indices] # Extract the flaw line indexes for each actual positive sample\n",
    "test_all_total_locs = [len(test_data['Text'].tolist()[i].split('\\n')) for i in range(len(test_data))] # Compute total LOC for each sample in the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bec214ef-212a-4ad1-861b-693fc4d50203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results based on the total of lines\n",
    "\n",
    "# configure sorting choice\n",
    "sort_by_lines = True # False # True when sort lines by line score and False when sort functions by prediction proba (and then sort lines in each function)\n",
    "\n",
    "# Usage\n",
    "if sort_by_lines == False:\n",
    "    effortXrecall = compute_effort_at_x_percent_recall_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=20)\n",
    "    recallXloc = compute_recall_at_x_percent_loc_rankedFuncs(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, x_percent=1)\n",
    "else: #sort_by_lines == True\n",
    "    effortXrecall = compute_effort_at_x_percent_recall_rankedLines(all_ranked_lines, positive_probas, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, all_predicted_lines_number, x_percent=20)\n",
    "    recallXloc = compute_recall_at_x_percent_loc_rankedLines(all_ranked_lines, positive_probas, all_neg_lines, all_flaw_lines, test_all_flaw_lines, test_all_total_locs, all_predicted_lines_number, x_percent=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b90aa470-84a6-4306-804f-665a8cee7abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Accuracy: 0.8226221079691517\n",
      "Top-10 Precision: 0.2705655526992288\n",
      "Top-10 Recall: 0.7793793571045597\n",
      "Top-10 Reciprocal Rank: 0.7896111315134451\n",
      "Top-10 MAP: 0.7876103950458598\n",
      "Top-10 MAR: 0.6297485625582099\n",
      "Median IFA: 0.0\n",
      "Effort@20%Recall: 0.005309907618988343\n",
      "Recall@1%LOC: 0.2910569105691057\n"
     ]
    }
   ],
   "source": [
    "# Display Final Evaluation Results\n",
    "top10acc = final_results_df[f'Top-{top_x} Accuracy'].tolist()[-2]\n",
    "top_precision = final_results_df[f'Top-{top_x} Precision'].tolist()[-2]\n",
    "top_recall = final_results_df[f'Top-{top_x} Recall'].tolist()[-2]\n",
    "top_mrr = final_results_df[f'Reciprocal Rank-{top_x}'].tolist()[-2]\n",
    "top_map = final_results_df[f'AP@{top_x}'].tolist()[-2]\n",
    "top_mar = final_results_df[f'AR@{top_x}'].tolist()[-2]\n",
    "ifa = final_results_df[\"IFA\"].tolist()[-1]\n",
    "print(f\"Top-{top_x} Accuracy: {top10acc}\")\n",
    "print(f\"Top-{top_x} Precision: {top_precision}\")\n",
    "print(f\"Top-{top_x} Recall: {top_recall}\")\n",
    "print(f\"Top-{top_x} Reciprocal Rank: {top_mrr}\")\n",
    "print(f\"Top-{top_x} MAP: {top_map}\")\n",
    "print(f\"Top-{top_x} MAR: {top_mar}\")\n",
    "print(f\"Median IFA: {ifa}\")\n",
    "print(f\"Effort@20%Recall: {effortXrecall}\")\n",
    "print(f\"Recall@1%LOC: {recallXloc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e578b2e4-eba7-4135-8f98-469896d2b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy results:\n",
      "Top-10 Accuracy: 82.3%\n",
      "Top-10 Precision: 27.1%\n",
      "Top-10 Recall: 77.9%\n",
      "Top-10 MRR: 79.0%\n",
      "Top-10 MAP: 78.8%\n",
      "Top-10 MAR: 63.0%\n",
      "\n",
      "\n",
      "Cost-effectiveness results:\n",
      "Median IFA: 0.0\n",
      "Effort@20%Recall: 0.5%\n",
      "Recall@1%LOC: 29.1%\n"
     ]
    }
   ],
   "source": [
    "# Display Final Evaluation Results in Percentages\n",
    "print(\"Accuracy results:\")\n",
    "print(f\"Top-{top_x} Accuracy: {round(top10acc * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} Precision: {round(top_precision * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} Recall: {round(top_recall * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MRR: {round(top_mrr * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MAP: {round(top_map * 100, 1)}%\")\n",
    "print(f\"Top-{top_x} MAR: {round(top_mar * 100, 1)}%\")\n",
    "print(\"\\n\")\n",
    "print(\"Cost-effectiveness results:\")\n",
    "print(f\"Median IFA: {round(ifa, 1)}\")\n",
    "print(f\"Effort@20%Recall: {round(effortXrecall * 100, 1)}%\")\n",
    "print(f\"Recall@1%LOC: {round(recallXloc * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67c63d-be14-4445-9d45-67d0faef08a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
