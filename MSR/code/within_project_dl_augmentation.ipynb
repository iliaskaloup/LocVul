{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# from tensorflow.keras.callbacks import CSVLogger\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "# from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "# from tensorflow.keras.layers import GRU\n",
    "# from tensorflow.keras.layers import Masking\n",
    "# from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "# from tensorflow.keras.callbacks import CSVLogger\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.constraints import max_norm\n",
    "# from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "# from tensorflow.keras.layers import LeakyReLU\n",
    "# from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "# from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from tensorflow.keras.layers import Conv1D\n",
    "# from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Flatten\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "#from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e81a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"w2v\", \"ft\", \"bert\", \"codebert\"]\n",
    "embedding = embeddings[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among Chrome and Linux or the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'full_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'chrome_data_reduced.csv'))\n",
    "#data = pd.read_csv(os.path.join('..','data', 'linux_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b1cf",
   "metadata": {},
   "source": [
    "Shuffle the dataset before starting operating on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60a95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8597edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0    midimanagerusbtest() : message_loop_(new bas...    0      14\n",
      "1  void renderframehostimpl::cancelblockedrequest...    0       8\n",
      "2  void renderthreadimpl::removeobserver(renderth...    0       7\n",
      "3  void webstorestandaloneinstaller::initinstalld...    0       7\n",
      "4  omniboxstate::omniboxstate(const omniboxeditmo...    0      15\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baebd02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "485b09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 80\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    70232\n",
      "1     3000\n",
      "Name: vul, dtype: int64\n",
      "Vulnerability Percentage:  4.271557124957284 %\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Vulnerability Percentage: \", (vc[1] / vc[0])*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251f04f",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ebe5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and then train into train and val (90% train, 10% test and then 90% train and 10% val)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(data[\"func\"].tolist(), data[\"vul\"].tolist(), stratify = data[\"vul\"].tolist(), test_size=0.1, random_state=shuffle_seeder)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, stratify = y_train_val, test_size=0.1, random_state=shuffle_seeder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c0c5a",
   "metadata": {},
   "source": [
    "<b>Handling imbalanced data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931d2fe",
   "metadata": {},
   "source": [
    "Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b76ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:1}\n",
    "#class_weights = {0:len(data) / (len(vc) * vc[0]), 1:len(data) / (len(vc) * vc[1])} #total observations / (number of classes * observations in class)\n",
    "#class_weights = dict(enumerate(compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f96c2e",
   "metadata": {},
   "source": [
    "Under-sampling of the clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4ffcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59317, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb3a86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    56887\n",
      "1     2430\n",
      "dtype: int64\n",
      "Majority class  0\n",
      "Targeted number of majority class 28443\n",
      "Class distribution after augmentation 0    28443\n",
      "1     2430\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply under-sampling with the specified strategy\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(\"Class distribution \", class_counts)\n",
    "majority_class = class_counts.idxmax()\n",
    "print(\"Majority class \", majority_class)\n",
    "target_count = int(class_counts.iloc[0] / 2) \n",
    "print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "# under\n",
    "sampling_strategy = {majority_class: target_count}        \n",
    "rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train) \n",
    "print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30a62d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_resampled = x_train_resampled.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e3c87",
   "metadata": {},
   "source": [
    "Random word augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5782f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.RandomWordAug(action='delete', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n",
    "#                         target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    "\n",
    "# num_minority_samples = pd.Series(y_train_resampled).value_counts()[1]\n",
    "# num_majority_samples = pd.Series(y_train_resampled).value_counts()[0]\n",
    "# difference  = num_majority_samples - num_minority_samples\n",
    "# num_augmentations_per_sentence = difference // num_minority_samples\n",
    "\n",
    "# x_augm = []\n",
    "# y_augm = []\n",
    "# for i in range(0, len(x_train_resampled)):\n",
    "#     # check for minority class\n",
    "#     if y_train_resampled[i] == 1:\n",
    "#         sentence = x_train_resampled[i]\n",
    "        \n",
    "#         for _ in range(num_augmentations_per_sentence):\n",
    "#             # Apply synonym augmentation\n",
    "#             augmented_sentence = aug.augment(sentence[0])\n",
    "#             # Store the augmented sentence\n",
    "#             x_augm.append(augmented_sentence)\n",
    "#             y_augm.append(1)\n",
    "            \n",
    "# x_train_resampled = x_train_resampled + x_augm\n",
    "# y_train_resampled = y_train_resampled + y_augm\n",
    "\n",
    "# print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc44591",
   "metadata": {},
   "source": [
    "Synonym word augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57cb5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n",
    "#                      stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n",
    "#                      verbose=0)\n",
    "\n",
    "# num_minority_samples = pd.Series(y_train_resampled).value_counts()[1]\n",
    "# num_majority_samples = pd.Series(y_train_resampled).value_counts()[0]\n",
    "# difference  = num_majority_samples - num_minority_samples\n",
    "# num_augmentations_per_sentence = difference // num_minority_samples\n",
    "\n",
    "# x_augm = []\n",
    "# y_augm = []\n",
    "# for i in range(0, len(x_train_resampled)):\n",
    "#     # check for minority class\n",
    "#     if y_train_resampled[i] == 1:\n",
    "#         sentence = x_train_resampled[i]\n",
    "        \n",
    "#         for _ in range(num_augmentations_per_sentence):\n",
    "#             # Apply synonym augmentation\n",
    "#             augmented_sentence = aug.augment(sentence[0])\n",
    "#             # Store the augmented sentence\n",
    "#             x_augm.append(augmented_sentence)\n",
    "#             y_augm.append(1)\n",
    "            \n",
    "# x_train_resampled = x_train_resampled + x_augm\n",
    "# y_train_resampled = y_train_resampled + y_augm\n",
    "\n",
    "# print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911c241",
   "metadata": {},
   "source": [
    "Word2vec similar words augmentation - substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd0d8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DownloadUtil.download_word2vec(dest_dir='.') # Download word2vec model\n",
    "#DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # Download GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b64eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # aug_w2v = naw.WordEmbsAug(\n",
    "# #     model_type='word2vec', model_path='GoogleNews-vectors-negative300.bin',\n",
    "# #     action=\"substitute\")\n",
    "\n",
    "# aug_w2v = naw.WordEmbsAug(\n",
    "#     model_type='word2vec', model_path='w2v_model.model',\n",
    "#     action=\"substitute\")\n",
    "\n",
    "# num_minority_samples = pd.Series(y_train_resampled).value_counts()[1]\n",
    "# num_majority_samples = pd.Series(y_train_resampled).value_counts()[0]\n",
    "# difference  = num_majority_samples - num_minority_samples\n",
    "# num_augmentations_per_sentence = difference // num_minority_samples\n",
    "\n",
    "# x_augm = []\n",
    "# y_augm = []\n",
    "# for i in range(0, len(x_train_resampled)):\n",
    "#     # check for minority class\n",
    "#     if y_train_resampled[i] == 1:\n",
    "#         sentence = x_train_resampled[i]\n",
    "        \n",
    "#         for _ in range(num_augmentations_per_sentence):\n",
    "#             # Apply synonym augmentation\n",
    "#             augmented_sentence = aug_w2v.augment(sentence[0])\n",
    "#             # Store the augmented sentence\n",
    "#             x_augm.append(augmented_sentence)\n",
    "#             y_augm.append(1)\n",
    "            \n",
    "# x_train_resampled = x_train_resampled + x_augm\n",
    "# y_train_resampled = y_train_resampled + y_augm\n",
    "\n",
    "# print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51c27c",
   "metadata": {},
   "source": [
    "FastText similar words augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a36a29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # Download fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "422bfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_w2v = naw.WordEmbsAug(\n",
    "#     model_type='fasttext', model_path='.',\n",
    "#     action=\"substitute\")\n",
    "\n",
    "# num_minority_samples = pd.Series(y_train_resampled).value_counts()[1]\n",
    "# num_majority_samples = pd.Series(y_train_resampled).value_counts()[0]\n",
    "# difference  = num_majority_samples - num_minority_samples\n",
    "# num_augmentations_per_sentence = difference // num_minority_samples\n",
    "\n",
    "# x_augm = []\n",
    "# y_augm = []\n",
    "# for i in range(0, len(x_train_resampled)):\n",
    "#     # check for minority class\n",
    "#     if y_train_resampled[i] == 1:\n",
    "#         sentence = x_train_resampled[i]\n",
    "        \n",
    "#         for _ in range(num_augmentations_per_sentence):\n",
    "#             # Apply synonym augmentation\n",
    "#             augmented_sentence = aug_w2v.augment(sentence[0])\n",
    "#             # Store the augmented sentence\n",
    "#             x_augm.append(augmented_sentence)\n",
    "#             y_augm.append(1)\n",
    "\n",
    "# x_train_resampled = x_train_resampled + x_augm\n",
    "# y_train_resampled = y_train_resampled + y_augm\n",
    "\n",
    "# print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d6af8",
   "metadata": {},
   "source": [
    "Contextual augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "892ddc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28443\n",
      "28444\n",
      "28445\n",
      "28446\n",
      "28447\n",
      "28448\n",
      "28449\n",
      "28450\n",
      "28451\n",
      "28452\n",
      "28453\n",
      "28454\n",
      "28455\n",
      "28456\n",
      "28457\n",
      "28458\n",
      "28459\n",
      "28460\n",
      "28461\n",
      "28462\n",
      "28463\n",
      "28464\n",
      "28465\n",
      "28466\n",
      "28467\n",
      "28468\n",
      "28469\n",
      "28470\n",
      "28471\n",
      "28472\n",
      "28473\n",
      "28474\n",
      "28475\n",
      "28476\n",
      "28477\n",
      "28478\n",
      "28479\n",
      "28480\n",
      "28481\n",
      "28482\n",
      "28483\n",
      "28484\n",
      "28485\n",
      "28486\n",
      "28487\n",
      "28488\n",
      "28489\n",
      "28490\n",
      "28491\n",
      "28492\n",
      "28493\n",
      "28494\n",
      "28495\n",
      "28496\n",
      "28497\n",
      "28498\n",
      "28499\n",
      "28500\n",
      "28501\n",
      "28502\n",
      "28503\n",
      "28504\n",
      "28505\n",
      "28506\n",
      "28507\n",
      "28508\n",
      "28509\n",
      "28510\n",
      "28511\n",
      "28512\n",
      "28513\n",
      "28514\n",
      "28515\n",
      "28516\n",
      "28517\n",
      "28518\n",
      "28519\n",
      "28520\n",
      "28521\n",
      "28522\n",
      "28523\n",
      "28524\n",
      "28525\n",
      "28526\n",
      "28527\n",
      "28528\n",
      "28529\n",
      "28530\n",
      "28531\n",
      "28532\n",
      "28533\n",
      "28534\n",
      "28535\n",
      "28536\n",
      "28537\n",
      "28538\n",
      "28539\n",
      "28540\n",
      "28541\n",
      "28542\n",
      "28543\n",
      "28544\n",
      "28545\n",
      "28546\n",
      "28547\n",
      "28548\n",
      "28549\n",
      "28550\n",
      "28551\n",
      "28552\n",
      "28553\n",
      "28554\n",
      "28555\n",
      "28556\n",
      "28557\n",
      "28558\n",
      "28559\n",
      "28560\n",
      "28561\n",
      "28562\n",
      "28563\n",
      "28564\n",
      "28565\n",
      "28566\n",
      "28567\n",
      "28568\n",
      "28569\n",
      "28570\n",
      "28571\n",
      "28572\n",
      "28573\n",
      "28574\n",
      "28575\n",
      "28576\n",
      "28577\n",
      "28578\n",
      "28579\n",
      "28580\n",
      "28581\n",
      "28582\n",
      "28583\n",
      "28584\n",
      "28585\n",
      "28586\n",
      "28587\n",
      "28588\n",
      "28589\n",
      "28590\n",
      "28591\n",
      "28592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m sentence \u001b[38;5;241m=\u001b[39m x_train_resampled[i]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_augmentations_per_sentence):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Apply synonym augmentation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     augmented_sentence \u001b[38;5;241m=\u001b[39m \u001b[43maug_bert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Store the augmented sentence\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     x_augm\u001b[38;5;241m.\u001b[39mappend(augmented_sentence)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\nlpaug\\base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, data, n, num_thread)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[1;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\nlpaug\\augmenter\\word\\context_word_embs.py:471\u001b[0m, in \u001b[0;36mContextualWordEmbsAug.substitute\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(masked_texts):\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# Update doc\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original_token, aug_input_pos, output, masked_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(original_tokens, aug_input_poses, outputs, masked_texts):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\nlpaug\\model\\lang_models\\bert.py:113\u001b[0m, in \u001b[0;36mBert.predict\u001b[1;34m(self, texts, target_words, n)\u001b[0m\n\u001b[0;32m    111\u001b[0m seed \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_p}\n\u001b[0;32m    112\u001b[0m target_token_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_randomness(target_token_logits, seed)\n\u001b[1;32m--> 113\u001b[0m target_token_logits, target_token_idxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiltering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_token_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_token_idxes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    115\u001b[0m     new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpick(target_token_logits, target_token_idxes, target_word\u001b[38;5;241m=\u001b[39mtarget_token, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\nlpaug\\model\\lang_models\\language_models.py:145\u001b[0m, in \u001b[0;36mLanguageModels.filtering\u001b[1;34m(self, logits, seed)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# TODO: Externalize to util for checking\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m--> 145\u001b[0m         idxes \u001b[38;5;241m=\u001b[39m \u001b[43midxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     idxes \u001b[38;5;241m=\u001b[39m idxes\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TOPK = 50 #default=100 # 20\n",
    "ACT = 'substitute' #\"substitute\" #\"insert\"\n",
    " \n",
    "aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='microsoft/codebert-base-mlm', \n",
    "    device='cuda',\n",
    "    action=ACT, top_k=TOPK)\n",
    "\n",
    "num_minority_samples = pd.Series(y_train_resampled).value_counts()[1]\n",
    "num_majority_samples = pd.Series(y_train_resampled).value_counts()[0]\n",
    "difference  = num_majority_samples - num_minority_samples\n",
    "num_augmentations_per_sentence = difference // num_minority_samples\n",
    "\n",
    "x_augm = []\n",
    "y_augm = []\n",
    "for i in range(0, len(x_train_resampled)):\n",
    "    # check for minority class\n",
    "    if y_train_resampled[i] == 1:\n",
    "        print(i)\n",
    "        sentence = x_train_resampled[i]\n",
    "        for _ in range(num_augmentations_per_sentence):\n",
    "            # Apply synonym augmentation\n",
    "            augmented_sentence = aug_bert.augment(sentence[0])\n",
    "            # Store the augmented sentence\n",
    "            x_augm.append(augmented_sentence)\n",
    "            y_augm.append(1)\n",
    "            \n",
    "x_train_resampled = x_train_resampled + x_augm\n",
    "y_train_resampled = y_train_resampled + y_augm\n",
    "\n",
    "print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df = pd.DataFrame({'func': x_train_resampled, 'vul': y_train_resampled})\n",
    "resampled_df.to_csv('augmented_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "# rename\n",
    "X_train = x_train_resampled\n",
    "Y_train = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f5db",
   "metadata": {},
   "source": [
    "BERT pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f89d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef02bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab68807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer.encode(sente[0], truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_train = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_train.append(seq[0])\n",
    "\n",
    "    lines_pad_x_train = pad_sequences(lines_pad_x_train, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63675629",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_val = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_val.append(seq[0])\n",
    "\n",
    "    lines_pad_x_val = pad_sequences(lines_pad_x_val, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7efba",
   "metadata": {},
   "source": [
    "CodeBERT pre-trained CPP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    model_variation = \"microsoft/codebert-base-mlm\" # \"neulab/codebert-cpp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea203cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer(sente[0], return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc259d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(sequences):\n",
    "    max_len = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "            max_len = len(seq['input_ids'].numpy()[0])\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    max_len = get_max_len(sequences)\n",
    "    print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1277690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_train = padSequences(sequences, max_len)\n",
    "    lines_pad_x_train = [arr.tolist() for arr in lines_pad_x_train]\n",
    "    lines_pad_x_train = np.array(lines_pad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_val = padSequences(sequences, max_len)\n",
    "    lines_pad_x_val = [arr.tolist() for arr in lines_pad_x_val]\n",
    "    lines_pad_x_val = np.array(lines_pad_x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fa37",
   "metadata": {},
   "source": [
    "Functions for Deep Learnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2\n",
    "\n",
    "def f2_loss(y_true, y_pred):\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    #tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    f2 = tf.where(tf.math.is_nan(f2), tf.zeros_like(f2), f2)\n",
    "    \n",
    "    return 1 - K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def buildLstmWithBahdanauAttention(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=max_len, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) \n",
    "\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Add BahdanauAttention layer before the final output layer\n",
    "    model.add(attention())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b991073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate) # default 0.001\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[f2_metric])\n",
    "    return cnn_model\n",
    "\n",
    "def buildLstmCNN(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "\n",
    "    # Embedding Layer\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "\n",
    "    # LSTM/GRU Layers\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    # CNN Layers\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', kernel_initializer=kernel_initializer)) \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    #model.add(Dense(64, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    # Classification layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a63be2",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe521ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(Y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d232a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "BS = 64\n",
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"lstmCnn\"#\"lstm\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix)\n",
    "elif userModel == \"lstmCnn\":\n",
    "    myModel = buildLstmCNN(max_len, num_words, dim, seed, embedding_matrix)\n",
    "elif userModel == \"lstmAtt\":\n",
    "    myModel = buildLstmWithBahdanauAttention(max_len, num_words, dim, seed, embedding_matrix)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f2_metric', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f2_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(lines_pad_x_train, y_train, validation_data=(lines_pad_x_val, y_val), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc], class_weight=class_weights)\n",
    "\n",
    "#load best model\n",
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "scores = myModel.evaluate(lines_pad_x_val, y_val, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(lines_pad_x_val)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(y_val, predictions)\n",
    "precision=precision_score(y_val, predictions)\n",
    "recall=recall_score(y_val, predictions)\n",
    "f1=f1_score(y_val, predictions)\n",
    "roc_auc=roc_auc_score(y_val, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(y_val, predictions, labels=[0, 1])\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e815aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
