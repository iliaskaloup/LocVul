{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.regularizers import l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a24011",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"w2v\", \"ft\", \"bert\", \"codebert\"]\n",
    "embedding = embeddings[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among Chrome and Linux or the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'full_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'chrome_data_reduced.csv'))\n",
    "#data = pd.read_csv(os.path.join('..','data', 'linux_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b1cf",
   "metadata": {},
   "source": [
    "Shuffle the dataset before starting operating on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8597edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0    midimanagerusbtest() : message_loop_(new bas...    0      14\n",
      "1  void renderframehostimpl::cancelblockedrequest...    0       8\n",
      "2  void renderthreadimpl::removeobserver(renderth...    0       7\n",
      "3  void webstorestandaloneinstaller::initinstalld...    0       7\n",
      "4  omniboxstate::omniboxstate(const omniboxeditmo...    0      15\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baebd02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 80\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    70232\n",
      "1     3000\n",
      "Name: vul, dtype: int64\n",
      "Vulnerability Percentage:  4.271557124957284 %\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Vulnerability Percentage: \", (vc[1] / vc[0])*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251f04f",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebe5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and then train into train and val (90% train, 10% test and then 90% train and 10% val)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(data[\"func\"].tolist(), data[\"vul\"].tolist(), stratify = data[\"vul\"].tolist(), test_size=0.1, random_state=shuffle_seeder)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, stratify = y_train_val, test_size=0.1, random_state=shuffle_seeder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c0c5a",
   "metadata": {},
   "source": [
    "<b>Handling imbalanced data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931d2fe",
   "metadata": {},
   "source": [
    "Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b76ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:1}\n",
    "#class_weights = {0:len(data) / (len(vc) * vc[0]), 1:len(data) / (len(vc) * vc[1])} #total observations / (number of classes * observations in class)\n",
    "#class_weights = dict(enumerate(compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f96c2e",
   "metadata": {},
   "source": [
    "Under-sampling of the clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ffcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59317, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3a86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    56887\n",
      "1     2430\n",
      "dtype: int64\n",
      "Majority class  0\n",
      "Targeted number of majority class 28443\n",
      "Class distribution after under-sampling 0    28443\n",
      "1     2430\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply under-sampling with the specified strategy\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(\"Class distribution \", class_counts)\n",
    "majority_class = class_counts.idxmax()\n",
    "print(\"Majority class \", majority_class)\n",
    "target_count = int(class_counts.iloc[0] / 2) \n",
    "print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "# under\n",
    "sampling_strategy = {majority_class: target_count}        \n",
    "rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train) \n",
    "print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1312fc",
   "metadata": {},
   "source": [
    "Synthetic Minority Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1ce39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at microsoft/codebert-base-mlm were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base-mlm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_variation = \"microsoft/codebert-base-mlm\" # \"neulab/codebert-cpp\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7578ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = x_train_resampled.tolist()\n",
    "\n",
    "sequences = [tokenizer.encode(sente[0], truncation=True, max_length=512, padding=\"max_length\", add_special_tokens=False, return_tensors=\"tf\") for sente in sentences]\n",
    "embeddings = [codebert(sequence)[0][:, 0, :].numpy() for sequence in sequences]  # Extract embeddings from CodeBERT's output\n",
    "extracted_embeddings = [arr[0] for arr in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318e0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE 0    28443\n",
      "1    28443\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(extracted_embeddings, y_train_resampled)\n",
    "\n",
    "# Class distribution after SMOTE\n",
    "class_counts_after_smote = pd.Series(y_train_resampled).value_counts()\n",
    "print(\"Class distribution after SMOTE\", class_counts_after_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41404d2b",
   "metadata": {},
   "source": [
    "Adaptive Synthetic Sampling with ADASYN - Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a12ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adasyn = ADASYN(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "# x_train_resampled, y_train_resampled = adasyn.fit_resample(extracted_embeddings, y_train_resampled)\n",
    "\n",
    "# # Class distribution after ADASYN\n",
    "# class_counts_after_adasyn = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution after SMOTE\", class_counts_after_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65e6517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "# rename\n",
    "X_train = x_train_resampled\n",
    "Y_train = y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dcc9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codebert encoding also in test/val set\n",
    "sequences_val = [tokenizer.encode(sente, truncation=True, max_length=512, padding=\"max_length\", add_special_tokens=False, return_tensors=\"tf\") for sente in x_val]\n",
    "embeddings_val = [codebert(sequence)[0][:, 0, :].numpy() for sequence in sequences_val]  # Extract embeddings from CodeBERT's output\n",
    "extracted_embeddings_val = [arr[0] for arr in embeddings_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "733efc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = extracted_embeddings_val\n",
    "Y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ec709d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(X_train[0])\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fa37",
   "metadata": {},
   "source": [
    "Functions for Deep Learnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cbeaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2\n",
    "\n",
    "def f2_loss(y_true, y_pred):\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    #tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    f2 = tf.where(tf.math.is_nan(f2), tf.zeros_like(f2), f2)\n",
    "    \n",
    "    return 1 - K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13eb0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def buildLstmWithBahdanauAttention(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=max_len, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) \n",
    "\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Add BahdanauAttention layer before the final output layer\n",
    "    model.add(attention())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5b991073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(dim, seed, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "\n",
    "    # LSTM/GRU Layers\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, input_shape=(1, dim), kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    #model.add(Dense(64, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    # Classification layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[f2_metric])\n",
    "    return cnn_model\n",
    "\n",
    "def buildLstmCNN(dim, seed, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "\n",
    "    # LSTM/GRU Layers\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, input_shape=(1, dim), kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # CNN Layers\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', kernel_initializer=kernel_initializer)) \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    #model.add(Dense(64, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    # Classification layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a63be2",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2846e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(Y_train)\n",
    "y_val = np.array(Y_val)\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_val = np.expand_dims(y_val, axis=1)\n",
    "\n",
    "x_train = np.array(X_train)\n",
    "x_val = np.array(X_val)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=1)\n",
    "x_val = np.expand_dims(x_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0d232a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv1d_11. Consider increasing the input size. Received input shape [None, 1, 200] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [116], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     myModel \u001b[38;5;241m=\u001b[39m buildLstm(dim, seed)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m userModel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstmCnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m     myModel \u001b[38;5;241m=\u001b[39m \u001b[43mbuildLstmCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m userModel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstmAtt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     15\u001b[0m     myModel \u001b[38;5;241m=\u001b[39m buildLstmWithBahdanauAttention(max_len, num_words, dim, seed, embedding_matrix)\n",
      "Cell \u001b[1;32mIn [115], line 50\u001b[0m, in \u001b[0;36mbuildLstmCNN\u001b[1;34m(dim, seed, learning_rate)\u001b[0m\n\u001b[0;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39madd(BatchNormalization())\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# CNN Layers\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39madd(GlobalMaxPool1D())\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Fully Connected Layers\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#model.add(Dense(64, activation='relu', kernel_initializer=kernel_initializer))\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Classification layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py:307\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[0;32m    303\u001b[0m         input_shape[:batch_rank] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]))\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    309\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    310\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    311\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    312\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    313\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv1d_11. Consider increasing the input size. Received input shape [None, 1, 200] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "BS = 64\n",
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"lstm\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(dim, seed)\n",
    "elif userModel == \"lstmCnn\":\n",
    "    myModel = buildLstmCNN(dim, seed)\n",
    "elif userModel == \"lstmAtt\":\n",
    "    myModel = buildLstmWithBahdanauAttention(max_len, num_words, dim, seed, embedding_matrix)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f2_metric', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f2_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc], class_weight=class_weights)\n",
    "\n",
    "#load best model\n",
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "scores = myModel.evaluate(x_val, y_val, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(x_val)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(y_val, predictions)\n",
    "precision=precision_score(y_val, predictions)\n",
    "recall=recall_score(y_val, predictions)\n",
    "f1=f1_score(y_val, predictions)\n",
    "roc_auc=roc_auc_score(y_val, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(y_val, predictions, labels=[0, 1])\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
