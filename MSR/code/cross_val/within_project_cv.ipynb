{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a24011",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"w2v\", \"ft\", \"bert\", \"codebert\"]\n",
    "embedding = embeddings[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among Chrome and Linux or the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'full_data_reduced.csv'))\n",
    "#data = pd.read_csv(os.path.join('..','data', 'chrome_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'linux_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b1cf",
   "metadata": {},
   "source": [
    "Shuffle the dataset before starting operating on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8597edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0  static inline void __ap_schedule_poll_timer(vo...    0      30\n",
      "1  static int sctp_autobind(struct sock *sk)\\n{\\n...    0      36\n",
      "2  static ssize_t ucma_init_qp_attr(struct ucma_f...    0      88\n",
      "3  static void __blk_mq_requeue_request(struct re...    0      24\n",
      "4  static void vhost_net_flush(struct vhost_net *...    0      37\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baebd02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 237\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    43024\n",
      "1     1439\n",
      "Name: vul, dtype: int64\n",
      "3.344644849386389 %\n"
     ]
    }
   ],
   "source": [
    "print(data[\"vul\"].value_counts())\n",
    "\n",
    "print((data[\"vul\"].value_counts()[1]/data[\"vul\"].value_counts()[0])*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffa700",
   "metadata": {},
   "source": [
    "PRE_PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4acbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9302ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaskaloup\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "tokenized_list = [word_tokenize(sentence) for sentence in data[\"func\"].tolist()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f5db",
   "metadata": {},
   "source": [
    "BERT pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f89d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef02bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab68807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = data[\"func\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d09c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad = []\n",
    "    for seq in sequences:\n",
    "        lines_pad.append(seq[0])\n",
    "\n",
    "    lines_pad = pad_sequences(lines_pad, padding = 'post', maxlen = 512)\n",
    "    lines_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7efba",
   "metadata": {},
   "source": [
    "CodeBERT pre-trained CPP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c8c38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at microsoft/codebert-base-mlm were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base-mlm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"codebert\":\n",
    "    model_variation = \"microsoft/codebert-base-mlm\" # \"neulab/codebert-cpp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea203cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"codebert\":\n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd3d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = data[\"func\"].tolist()\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7586bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(sequences):\n",
    "    max_len = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "            max_len = len(seq['input_ids'].numpy()[0])\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c1e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_len = get_max_len(sequences)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1277690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad = padSequences(sequences, max_len)\n",
    "    lines_pad = [arr.tolist() for arr in lines_pad]\n",
    "    lines_pad = np.array(lines_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d708d8b",
   "metadata": {},
   "source": [
    "CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "841ea528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0e4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[f1_metric])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ba094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCrossVal(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel):\n",
    "    ############## cross validation\n",
    "    scores=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'f2']\n",
    "    values = [np.array([]) for i in range(0, len(scores))]\n",
    "    score_dict = OrderedDict(zip(scores, values))\n",
    "    k=10\n",
    "    f=0\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nb_epoch = 100\n",
    "    BS = 64\n",
    "    print(\"Training...\")\n",
    "    milli_sec1 = int(round(time.time() * 1000))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        f = f + 1\n",
    "        print('fold number= ',f)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        \n",
    "#         #sampling\n",
    "#         X_res, Y_res = RandomOverSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "#         #X_res, Y_res = RandomUnderSampler(random_state=seed, sampling_strategy=0.5).fit_resample(X_train, Y_train)\n",
    "    \n",
    "#         #shuffle dataset\n",
    "#         X_resampled=pd.DataFrame(X_res)\n",
    "#         Y_resampled=pd.DataFrame(Y_res)\n",
    "#         newTrain=X_resampled.assign(Label=Y_resampled.values)\n",
    "#         newTrain = shuffle(newTrain,random_state=seed)\n",
    "#         X_train=np.array(newTrain.iloc[:, 0:-1 ])\n",
    "#         X_train=pd.DataFrame(X_train)\n",
    "#         Y_train=np.array(newTrain.iloc[:, -1 ])\n",
    "#         Y_train=pd.DataFrame(Y_train)\n",
    "        \n",
    "        if userModel == \"cnn\":\n",
    "            myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "        elif userModel == \"lstm\":\n",
    "            myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix)\n",
    "\n",
    "        print(\"model summary\\m\", myModel.summary())\n",
    "        csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "        es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "        print(type(X_train))\n",
    "        history = myModel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "        \n",
    "        #load best model\n",
    "        #model = load_model('best_model.h5')\n",
    "        myModel.load_weights(\"best_model.h5\")\n",
    "        \n",
    "        scores = myModel.evaluate(X_test, Y_test, verbose=0)\n",
    "        #predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "        predScores = myModel.predict(X_test)\n",
    "        predictions = (predScores > 0.5).astype(\"int32\")\n",
    "        \n",
    "        accuracy=accuracy_score(Y_test, predictions)\n",
    "        precision=precision_score(Y_test, predictions)\n",
    "        recall=recall_score(Y_test, predictions)\n",
    "        f1=f1_score(Y_test, predictions)\n",
    "        roc_auc=roc_auc_score(Y_test, predictions)\n",
    "        f2=5*precision*recall / (4*precision+recall)\n",
    "        \n",
    "        cm = confusion_matrix(Y_test, predictions, labels=[0, 1])\n",
    "        #print(cm)\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        print(\"TP=\",tp)\n",
    "        print(\"TN=\",tn)\n",
    "        print(\"FP=\",fp)\n",
    "        print(\"FN=\",fn)\n",
    "        \n",
    "        acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "        \n",
    "        print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "        print(\"Precision:%.2f%%\"%(precision*100))\n",
    "        print(\"Recall:%.2f%%\"%(recall*100))\n",
    "        print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "        print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "        print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "        print(classification_report(Y_test, predictions))\n",
    "        \n",
    "        del myModel\n",
    "        \n",
    "        score_dict['accuracy'] = np.append(score_dict['accuracy'], accuracy)\n",
    "        score_dict['precision'] = np.append(score_dict['precision'], precision)\n",
    "        score_dict['recall'] = np.append(score_dict['recall'], recall)\n",
    "        score_dict['f1'] = np.append(score_dict['f1'], f1)\n",
    "        score_dict['roc_auc'] = np.append(score_dict['roc_auc'], roc_auc)\n",
    "        score_dict['f2'] = np.append(score_dict['f2'], f2)\n",
    "        \n",
    "    milli_sec2 = int(round(time.time() * 1000))\n",
    "    print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)\n",
    "    \n",
    "    print(\"accuracy: %.2f%% (%.2f%%)\" % (score_dict['accuracy'].mean()*100, score_dict['accuracy'].std()*100))\n",
    "    print(\"precision: %.2f%% (%.2f%%)\" % (score_dict['precision'].mean()*100, score_dict['precision'].std()*100))\n",
    "    print(\"recall: %.2f%% (%.2f%%)\" % (score_dict['recall'].mean()*100, score_dict['recall'].std()*100))\n",
    "    print(\"f1: %.2f%% (%.2f%%)\" % (score_dict['f1'].mean()*100, score_dict['f1'].std()*100))\n",
    "    print(\"roc_auc: %.2f%% (%.2f%%)\" % (score_dict['roc_auc'].mean()*100, score_dict['roc_auc'].std()*100))\n",
    "    print(\"f2: %.2f%% (%.2f%%)\" % (score_dict['f2'].mean()*100, score_dict['f2'].std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dfb69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "fold number=  1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 768)         38603520  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 100)         347600    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 38,981,571\n",
      "Trainable params: 377,951\n",
      "Non-trainable params: 38,603,620\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/100\n",
      "626/626 [==============================] - 68s 94ms/step - loss: 0.1973 - f1_metric: 0.1728 - val_loss: 0.1258 - val_f1_metric: 0.1763\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.17633, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.1084 - f1_metric: 0.2257 - val_loss: 0.0643 - val_f1_metric: 0.5054\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.17633 to 0.50537, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0688 - f1_metric: 0.3470 - val_loss: 0.0598 - val_f1_metric: 0.3226\n",
      "\n",
      "Epoch 00003: val_f1_metric did not improve from 0.50537\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0651 - f1_metric: 0.3506 - val_loss: 0.0612 - val_f1_metric: 0.2408\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.50537\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0618 - f1_metric: 0.3791 - val_loss: 0.0616 - val_f1_metric: 0.1310\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.50537\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0601 - f1_metric: 0.3926 - val_loss: 0.0618 - val_f1_metric: 0.6511\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.50537 to 0.65107, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0592 - f1_metric: 0.4037 - val_loss: 0.0599 - val_f1_metric: 0.4496\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.65107\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0568 - f1_metric: 0.4103 - val_loss: 0.0679 - val_f1_metric: 0.6581\n",
      "\n",
      "Epoch 00008: val_f1_metric improved from 0.65107 to 0.65808, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0544 - f1_metric: 0.4390 - val_loss: 0.0674 - val_f1_metric: 0.1390\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.65808\n",
      "Epoch 10/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0525 - f1_metric: 0.4622 - val_loss: 0.0612 - val_f1_metric: 0.4288\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.65808\n",
      "Epoch 11/100\n",
      "626/626 [==============================] - 57s 91ms/step - loss: 0.0505 - f1_metric: 0.5080 - val_loss: 0.0685 - val_f1_metric: 0.6094\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.65808\n",
      "Epoch 12/100\n",
      "626/626 [==============================] - 56s 90ms/step - loss: 0.0482 - f1_metric: 0.4971 - val_loss: 0.0720 - val_f1_metric: 0.6334\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.65808\n",
      "Epoch 13/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0472 - f1_metric: 0.5137 - val_loss: 0.0747 - val_f1_metric: 0.5819\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.65808\n",
      "Epoch 14/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0457 - f1_metric: 0.5260 - val_loss: 0.0667 - val_f1_metric: 0.5292\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.65808\n",
      "Epoch 15/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0444 - f1_metric: 0.5641 - val_loss: 0.0864 - val_f1_metric: 0.1750\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.65808\n",
      "Epoch 16/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0425 - f1_metric: 0.5591 - val_loss: 0.0810 - val_f1_metric: 0.4574\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.65808\n",
      "Epoch 17/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0422 - f1_metric: 0.5663 - val_loss: 0.0771 - val_f1_metric: 0.5193\n",
      "\n",
      "Epoch 00017: val_f1_metric did not improve from 0.65808\n",
      "Epoch 18/100\n",
      "626/626 [==============================] - 54s 86ms/step - loss: 0.0408 - f1_metric: 0.5628 - val_loss: 0.0834 - val_f1_metric: 0.5505\n",
      "\n",
      "Epoch 00018: val_f1_metric did not improve from 0.65808\n",
      "Epoch 00018: early stopping\n",
      "TP= 130\n",
      "TN= 4192\n",
      "FP= 111\n",
      "FN= 14\n",
      "Accuracy:97.19%\n",
      "Precision:53.94%\n",
      "Recall:90.28%\n",
      "F1 score:67.53%\n",
      "Roc_Auc score:93.85%\n",
      "F2 score:79.56%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99      4303\n",
      "           1       0.54      0.90      0.68       144\n",
      "\n",
      "    accuracy                           0.97      4447\n",
      "   macro avg       0.77      0.94      0.83      4447\n",
      "weighted avg       0.98      0.97      0.98      4447\n",
      "\n",
      "fold number=  2\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 768)         38603520  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 100)         347600    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 38,981,571\n",
      "Trainable params: 377,951\n",
      "Non-trainable params: 38,603,620\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/100\n",
      "626/626 [==============================] - 65s 95ms/step - loss: 0.1969 - f1_metric: 0.1504 - val_loss: 0.1171 - val_f1_metric: 0.1714\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.17143, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 55s 87ms/step - loss: 0.1037 - f1_metric: 0.2253 - val_loss: 0.0685 - val_f1_metric: 0.1971\n",
      "\n",
      "Epoch 00002: val_f1_metric improved from 0.17143 to 0.19714, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 55s 87ms/step - loss: 0.0667 - f1_metric: 0.3494 - val_loss: 0.0733 - val_f1_metric: 0.5446\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.19714 to 0.54464, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0640 - f1_metric: 0.3805 - val_loss: 0.0666 - val_f1_metric: 0.5345\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.54464\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 53s 85ms/step - loss: 0.0617 - f1_metric: 0.3659 - val_loss: 0.0684 - val_f1_metric: 0.1714\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.54464\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 54s 87ms/step - loss: 0.0608 - f1_metric: 0.3959 - val_loss: 0.0689 - val_f1_metric: 0.5531\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.54464 to 0.55307, saving model to best_model.h5\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626/626 [==============================] - 54s 85ms/step - loss: 0.0588 - f1_metric: 0.4037 - val_loss: 0.0682 - val_f1_metric: 0.2491\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.55307\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 55s 87ms/step - loss: 0.0570 - f1_metric: 0.4566 - val_loss: 0.0695 - val_f1_metric: 0.1571\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.55307\n",
      "Epoch 9/100\n",
      "626/626 [==============================] - 621s 994ms/step - loss: 0.0554 - f1_metric: 0.4485 - val_loss: 0.0695 - val_f1_metric: 0.4361\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.55307\n",
      "Epoch 10/100\n",
      "626/626 [==============================] - 800s 1s/step - loss: 0.0525 - f1_metric: 0.4771 - val_loss: 0.0835 - val_f1_metric: 0.5064\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.55307\n",
      "Epoch 11/100\n",
      "626/626 [==============================] - 800s 1s/step - loss: 0.0506 - f1_metric: 0.4874 - val_loss: 0.0815 - val_f1_metric: 0.4526\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.55307\n",
      "Epoch 12/100\n",
      "626/626 [==============================] - 832s 1s/step - loss: 0.0495 - f1_metric: 0.4987 - val_loss: 0.0822 - val_f1_metric: 0.4652\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.55307\n",
      "Epoch 13/100\n",
      "626/626 [==============================] - 801s 1s/step - loss: 0.0464 - f1_metric: 0.5368 - val_loss: 0.0841 - val_f1_metric: 0.2144\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.55307\n",
      "Epoch 14/100\n",
      "626/626 [==============================] - 814s 1s/step - loss: 0.0449 - f1_metric: 0.5424 - val_loss: 0.0867 - val_f1_metric: 0.4140\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.55307\n",
      "Epoch 15/100\n",
      "626/626 [==============================] - 839s 1s/step - loss: 0.0433 - f1_metric: 0.5578 - val_loss: 0.0935 - val_f1_metric: 0.1719\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.55307\n",
      "Epoch 16/100\n",
      "626/626 [==============================] - 777s 1s/step - loss: 0.0409 - f1_metric: 0.5859 - val_loss: 0.1057 - val_f1_metric: 0.1722\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.55307\n",
      "Epoch 00016: early stopping\n",
      "TP= 117\n",
      "TN= 4175\n",
      "FP= 128\n",
      "FN= 27\n",
      "Accuracy:96.51%\n",
      "Precision:47.76%\n",
      "Recall:81.25%\n",
      "F1 score:60.15%\n",
      "Roc_Auc score:89.14%\n",
      "F2 score:71.25%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      4303\n",
      "           1       0.48      0.81      0.60       144\n",
      "\n",
      "    accuracy                           0.97      4447\n",
      "   macro avg       0.74      0.89      0.79      4447\n",
      "weighted avg       0.98      0.97      0.97      4447\n",
      "\n",
      "fold number=  3\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 768)         38603520  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 100)         347600    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 38,981,571\n",
      "Trainable params: 377,951\n",
      "Non-trainable params: 38,603,620\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/100\n",
      "626/626 [==============================] - 686s 1s/step - loss: 0.1992 - f1_metric: 0.1799 - val_loss: 0.1188 - val_f1_metric: 0.3120\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.31204, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 668s 1s/step - loss: 0.1083 - f1_metric: 0.2321 - val_loss: 0.0686 - val_f1_metric: 0.1143\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.31204\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 660s 1s/step - loss: 0.0681 - f1_metric: 0.3567 - val_loss: 0.0781 - val_f1_metric: 0.5704\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.31204 to 0.57044, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 672s 1s/step - loss: 0.0640 - f1_metric: 0.3892 - val_loss: 0.0840 - val_f1_metric: 0.1143\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.57044\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 667s 1s/step - loss: 0.0611 - f1_metric: 0.4000 - val_loss: 0.0654 - val_f1_metric: 0.1143\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.57044\n",
      "Epoch 6/100\n",
      "626/626 [==============================] - 547s 874ms/step - loss: 0.0594 - f1_metric: 0.4060 - val_loss: 0.0823 - val_f1_metric: 0.5939\n",
      "\n",
      "Epoch 00006: val_f1_metric improved from 0.57044 to 0.59389, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "626/626 [==============================] - 331s 529ms/step - loss: 0.0575 - f1_metric: 0.4232 - val_loss: 0.0629 - val_f1_metric: 0.4537\n",
      "\n",
      "Epoch 00007: val_f1_metric did not improve from 0.59389\n",
      "Epoch 8/100\n",
      "626/626 [==============================] - 334s 534ms/step - loss: 0.0553 - f1_metric: 0.4445 - val_loss: 0.0653 - val_f1_metric: 0.5351\n",
      "\n",
      "Epoch 00008: val_f1_metric did not improve from 0.59389\n",
      "Epoch 9/100\n",
      "626/626 [==============================] - 321s 513ms/step - loss: 0.0532 - f1_metric: 0.4839 - val_loss: 0.0698 - val_f1_metric: 0.2116\n",
      "\n",
      "Epoch 00009: val_f1_metric did not improve from 0.59389\n",
      "Epoch 10/100\n",
      "626/626 [==============================] - 313s 501ms/step - loss: 0.0515 - f1_metric: 0.4964 - val_loss: 0.0751 - val_f1_metric: 0.4457\n",
      "\n",
      "Epoch 00010: val_f1_metric did not improve from 0.59389\n",
      "Epoch 11/100\n",
      "626/626 [==============================] - 314s 502ms/step - loss: 0.0493 - f1_metric: 0.5410 - val_loss: 0.1034 - val_f1_metric: 0.5751\n",
      "\n",
      "Epoch 00011: val_f1_metric did not improve from 0.59389\n",
      "Epoch 12/100\n",
      "626/626 [==============================] - 321s 513ms/step - loss: 0.0493 - f1_metric: 0.5315 - val_loss: 0.0866 - val_f1_metric: 0.5107\n",
      "\n",
      "Epoch 00012: val_f1_metric did not improve from 0.59389\n",
      "Epoch 13/100\n",
      "626/626 [==============================] - 321s 513ms/step - loss: 0.0454 - f1_metric: 0.5527 - val_loss: 0.0843 - val_f1_metric: 0.2504\n",
      "\n",
      "Epoch 00013: val_f1_metric did not improve from 0.59389\n",
      "Epoch 14/100\n",
      "626/626 [==============================] - 324s 517ms/step - loss: 0.0435 - f1_metric: 0.5816 - val_loss: 0.1180 - val_f1_metric: 0.5187\n",
      "\n",
      "Epoch 00014: val_f1_metric did not improve from 0.59389\n",
      "Epoch 15/100\n",
      "626/626 [==============================] - 322s 515ms/step - loss: 0.0424 - f1_metric: 0.5904 - val_loss: 0.1009 - val_f1_metric: 0.1210\n",
      "\n",
      "Epoch 00015: val_f1_metric did not improve from 0.59389\n",
      "Epoch 16/100\n",
      "626/626 [==============================] - 332s 530ms/step - loss: 0.0428 - f1_metric: 0.5846 - val_loss: 0.0988 - val_f1_metric: 0.1386\n",
      "\n",
      "Epoch 00016: val_f1_metric did not improve from 0.59389\n",
      "Epoch 00016: early stopping\n",
      "TP= 130\n",
      "TN= 4151\n",
      "FP= 152\n",
      "FN= 14\n",
      "Accuracy:96.27%\n",
      "Precision:46.10%\n",
      "Recall:90.28%\n",
      "F1 score:61.03%\n",
      "Roc_Auc score:93.37%\n",
      "F2 score:75.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      4303\n",
      "           1       0.46      0.90      0.61       144\n",
      "\n",
      "    accuracy                           0.96      4447\n",
      "   macro avg       0.73      0.93      0.80      4447\n",
      "weighted avg       0.98      0.96      0.97      4447\n",
      "\n",
      "fold number=  4\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 768)         38603520  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 100)         347600    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 38,981,571\n",
      "Trainable params: 377,951\n",
      "Non-trainable params: 38,603,620\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626/626 [==============================] - 344s 541ms/step - loss: 0.1935 - f1_metric: 0.1353 - val_loss: 0.1223 - val_f1_metric: 0.1946\n",
      "\n",
      "Epoch 00001: val_f1_metric improved from -inf to 0.19456, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "626/626 [==============================] - 328s 523ms/step - loss: 0.1210 - f1_metric: 0.1705 - val_loss: 0.1189 - val_f1_metric: 0.1286\n",
      "\n",
      "Epoch 00002: val_f1_metric did not improve from 0.19456\n",
      "Epoch 3/100\n",
      "626/626 [==============================] - 328s 523ms/step - loss: 0.0766 - f1_metric: 0.3052 - val_loss: 0.1095 - val_f1_metric: 0.5408\n",
      "\n",
      "Epoch 00003: val_f1_metric improved from 0.19456 to 0.54078, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "626/626 [==============================] - 334s 533ms/step - loss: 0.0654 - f1_metric: 0.3683 - val_loss: 0.0644 - val_f1_metric: 0.5352\n",
      "\n",
      "Epoch 00004: val_f1_metric did not improve from 0.54078\n",
      "Epoch 5/100\n",
      "626/626 [==============================] - 351s 560ms/step - loss: 0.0636 - f1_metric: 0.3820 - val_loss: 0.0628 - val_f1_metric: 0.2103\n",
      "\n",
      "Epoch 00005: val_f1_metric did not improve from 0.54078\n",
      "Epoch 6/100\n",
      " 66/626 [==>...........................] - ETA: 4:41 - loss: 0.0695 - f1_metric: 0.3862"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22044\\3344113554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrunCrossVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines_pad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vul\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lstm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22044\\3373738043.py\u001b[0m in \u001b[0;36mrunCrossVal\u001b[1;34m(X, y, max_len, num_words, dim, seed, embedding_matrix, userModel)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_f1_metric'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#load best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "runCrossVal(lines_pad, data[\"vul\"].values, max_len, num_words, dim, seed, embedding_matrix, \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ec372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
