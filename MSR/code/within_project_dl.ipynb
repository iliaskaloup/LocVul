{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30a24011",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"w2v\", \"ft\", \"bert\", \"codebert\"]\n",
    "embedding = embeddings[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among Chrome and Linux or the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'full_data_reduced.csv'))\n",
    "#data = pd.read_csv(os.path.join('..','data', 'chrome_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'linux_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b1cf",
   "metadata": {},
   "source": [
    "Shuffle the dataset before starting operating on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60a95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8597edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0  static inline void __ap_schedule_poll_timer(vo...    0      30\n",
      "1  static int sctp_autobind(struct sock *sk)\\n{\\n...    0      36\n",
      "2  static ssize_t ucma_init_qp_attr(struct ucma_f...    0      88\n",
      "3  static void __blk_mq_requeue_request(struct re...    0      24\n",
      "4  static void vhost_net_flush(struct vhost_net *...    0      37\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baebd02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "485b09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 237\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    43024\n",
      "1     1439\n",
      "Name: vul, dtype: int64\n",
      "Vulnerability Percentage:  3.344644849386389 %\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Vulnerability Percentage: \", (vc[1] / vc[0])*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251f04f",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ebe5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and then train into train and val (90% train, 10% test and then 90% train and 10% val)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(data[\"func\"].tolist(), data[\"vul\"].tolist(), stratify = data[\"vul\"].tolist(), test_size=0.1, random_state=shuffle_seeder)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, stratify = y_train_val, test_size=0.1, random_state=shuffle_seeder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860c1b3",
   "metadata": {},
   "source": [
    "<b>Handling imbalanced data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b9279",
   "metadata": {},
   "source": [
    "Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efc443f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.516723224246932, 1: 15.449270326615705}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total observations / (number of classes * observations in class)\n",
    "class_weights = {0:len(data) / (len(vc) * vc[0]), 1:len(data) / (len(vc) * vc[1])}\n",
    "#class_weights = {0:1, 1:1}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1c91e",
   "metadata": {},
   "source": [
    "Under-sampling of the clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e731e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    34849\n",
      "1     1165\n",
      "dtype: int64\n",
      "Majority class  0\n",
      "Targeted number of majority class 17424\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['syscall_define6(mbind, unsigned long, start, unsigned long, len,\\n\\t\\tunsigned long, mode, const unsigned long __user *, nmask,\\n\\t\\tunsigned long, maxnode, unsigned, flags)\\n{\\n\\tnodemask_t nodes;\\n\\tint err;\\n\\tunsigned short mode_flags;\\n\\tmode_flags = mode & mpol_mode_flags;\\n\\tmode &= ~mpol_mode_flags;\\n\\tif (mode >= mpol_max)\\n\\t\\treturn -einval;\\n\\tif ((mode_flags & mpol_f_static_nodes) &&\\n\\t    (mode_flags & mpol_f_relative_nodes))\\n\\t\\treturn -einval;\\n\\terr = get_nodes(&nodes, nmask, maxnode);\\n\\tif (err)\\n\\t\\treturn err;\\n\\treturn do_mbind(start, len, mode, mode_flags, &nodes, flags);\\n}\\n'\n 'int sk_filter(struct sock *sk, struct sk_buff *skb)\\n{\\n\\tint err;\\n\\tstruct sk_filter *filter;\\n\\t/*\\n\\t * if the skb was allocated from pfmemalloc reserves, only\\n\\t * allow sock_memalloc sockets to use it as this socket is\\n\\t * helping free memory\\n\\t */\\n\\tif (skb_pfmemalloc(skb) && !sock_flag(sk, sock_memalloc))\\n\\t\\treturn -enomem;\\n\\terr = security_sock_rcv_skb(sk, skb);\\n\\tif (err)\\n\\t\\treturn err;\\n\\trcu_read_lock();\\n\\tfilter = rcu_dereference(sk->sk_filter);\\n\\tif (filter) {\\n\\t\\tunsigned int pkt_len = sk_run_filter(filter, skb);\\n\\t\\terr = pkt_len ? pskb_trim(skb, pkt_len) : -eperm;\\n\\t}\\n\\trcu_read_unlock();\\n\\treturn err;\\n}\\n'\n 'nfsd4_decode_stateid(struct nfsd4_compoundargs *argp, stateid_t *sid)\\n{\\n\\tdecode_head;\\n\\tread_buf(sizeof(stateid_t));\\n\\tsid->si_generation = be32_to_cpup(p++);\\n\\tcopymem(&sid->si_opaque, sizeof(stateid_opaque_t));\\n\\tdecode_tail;\\n}\\n'\n ...\n 'void qeth_tx_timeout(struct net_device *dev)\\n{\\n\\tstruct qeth_card *card;\\n\\tcard = dev->ml_priv;\\n\\tqeth_card_text(card, 4, \"txtimeo\");\\n\\tcard->stats.tx_errors++;\\n\\tqeth_schedule_recovery(card);\\n}\\n'\n 'static void kvm_sync_pages(struct kvm_vcpu *vcpu,  gfn_t gfn)\\n{\\n\\tstruct kvm_mmu_page *s;\\n\\tlist_head(invalid_list);\\n\\tbool flush = false;\\n\\tfor_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {\\n\\t\\tif (!s->unsync)\\n\\t\\t\\tcontinue;\\n\\t\\twarn_on(s->role.level != pt_page_table_level);\\n\\t\\tkvm_unlink_unsync_page(vcpu->kvm, s);\\n\\t\\tif ((s->role.cr4_pae != !!is_pae(vcpu)) ||\\n\\t\\t\\t(vcpu->arch.mmu.sync_page(vcpu, s))) {\\n\\t\\t\\tkvm_mmu_prepare_zap_page(vcpu->kvm, s, &invalid_list);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tflush = true;\\n\\t}\\n\\tkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\\n\\tif (flush)\\n\\t\\tkvm_mmu_flush_tlb(vcpu);\\n}\\n'\n 'static int __init ptrace_break_init(void)\\n{\\n\\tregister_undef_hook(&arm_break_hook);\\n\\tregister_undef_hook(&thumb_break_hook);\\n\\tregister_undef_hook(&thumb2_break_hook);\\n\\treturn 0;\\n}\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20000\\1651882894.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msampling_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmajority_class\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_count\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mrus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx_train_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Class distribution after under-sampling\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_resampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_random_under_sampler.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         )\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    771\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                     \u001b[1;34m\"if it contains a single sample.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 )\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['syscall_define6(mbind, unsigned long, start, unsigned long, len,\\n\\t\\tunsigned long, mode, const unsigned long __user *, nmask,\\n\\t\\tunsigned long, maxnode, unsigned, flags)\\n{\\n\\tnodemask_t nodes;\\n\\tint err;\\n\\tunsigned short mode_flags;\\n\\tmode_flags = mode & mpol_mode_flags;\\n\\tmode &= ~mpol_mode_flags;\\n\\tif (mode >= mpol_max)\\n\\t\\treturn -einval;\\n\\tif ((mode_flags & mpol_f_static_nodes) &&\\n\\t    (mode_flags & mpol_f_relative_nodes))\\n\\t\\treturn -einval;\\n\\terr = get_nodes(&nodes, nmask, maxnode);\\n\\tif (err)\\n\\t\\treturn err;\\n\\treturn do_mbind(start, len, mode, mode_flags, &nodes, flags);\\n}\\n'\n 'int sk_filter(struct sock *sk, struct sk_buff *skb)\\n{\\n\\tint err;\\n\\tstruct sk_filter *filter;\\n\\t/*\\n\\t * if the skb was allocated from pfmemalloc reserves, only\\n\\t * allow sock_memalloc sockets to use it as this socket is\\n\\t * helping free memory\\n\\t */\\n\\tif (skb_pfmemalloc(skb) && !sock_flag(sk, sock_memalloc))\\n\\t\\treturn -enomem;\\n\\terr = security_sock_rcv_skb(sk, skb);\\n\\tif (err)\\n\\t\\treturn err;\\n\\trcu_read_lock();\\n\\tfilter = rcu_dereference(sk->sk_filter);\\n\\tif (filter) {\\n\\t\\tunsigned int pkt_len = sk_run_filter(filter, skb);\\n\\t\\terr = pkt_len ? pskb_trim(skb, pkt_len) : -eperm;\\n\\t}\\n\\trcu_read_unlock();\\n\\treturn err;\\n}\\n'\n 'nfsd4_decode_stateid(struct nfsd4_compoundargs *argp, stateid_t *sid)\\n{\\n\\tdecode_head;\\n\\tread_buf(sizeof(stateid_t));\\n\\tsid->si_generation = be32_to_cpup(p++);\\n\\tcopymem(&sid->si_opaque, sizeof(stateid_opaque_t));\\n\\tdecode_tail;\\n}\\n'\n ...\n 'void qeth_tx_timeout(struct net_device *dev)\\n{\\n\\tstruct qeth_card *card;\\n\\tcard = dev->ml_priv;\\n\\tqeth_card_text(card, 4, \"txtimeo\");\\n\\tcard->stats.tx_errors++;\\n\\tqeth_schedule_recovery(card);\\n}\\n'\n 'static void kvm_sync_pages(struct kvm_vcpu *vcpu,  gfn_t gfn)\\n{\\n\\tstruct kvm_mmu_page *s;\\n\\tlist_head(invalid_list);\\n\\tbool flush = false;\\n\\tfor_each_gfn_indirect_valid_sp(vcpu->kvm, s, gfn) {\\n\\t\\tif (!s->unsync)\\n\\t\\t\\tcontinue;\\n\\t\\twarn_on(s->role.level != pt_page_table_level);\\n\\t\\tkvm_unlink_unsync_page(vcpu->kvm, s);\\n\\t\\tif ((s->role.cr4_pae != !!is_pae(vcpu)) ||\\n\\t\\t\\t(vcpu->arch.mmu.sync_page(vcpu, s))) {\\n\\t\\t\\tkvm_mmu_prepare_zap_page(vcpu->kvm, s, &invalid_list);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tflush = true;\\n\\t}\\n\\tkvm_mmu_commit_zap_page(vcpu->kvm, &invalid_list);\\n\\tif (flush)\\n\\t\\tkvm_mmu_flush_tlb(vcpu);\\n}\\n'\n 'static int __init ptrace_break_init(void)\\n{\\n\\tregister_undef_hook(&arm_break_hook);\\n\\tregister_undef_hook(&thumb_break_hook);\\n\\tregister_undef_hook(&thumb2_break_hook);\\n\\treturn 0;\\n}\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Apply under-sampling with the specified strategy\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(\"Class distribution \", class_counts)\n",
    "majority_class = class_counts.idxmax()\n",
    "print(\"Majority class \", majority_class)\n",
    "target_count = int(class_counts.iloc[0] / 2) \n",
    "print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "# under\n",
    "sampling_strategy = {majority_class: target_count}        \n",
    "rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train) \n",
    "print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0ecd6",
   "metadata": {},
   "source": [
    "Random Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply random over-sampling with the specified strategy\n",
    "# class_counts = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution \", class_counts)\n",
    "# minority_class = class_counts.idxmin()\n",
    "# print(\"Minority class \", minority_class)\n",
    "# target_count = class_counts.iloc[0] #int(class_counts.iloc[1] * 2) \n",
    "# print(\"Targeted number of minority class\", target_count)\n",
    "\n",
    "# # over\n",
    "# sampling_strategy = {minority_class: target_count}        \n",
    "# ros = RandomOverSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "# x_train_resampled, y_train_resampled = ros.fit_resample(x_train_resampled, y_train_resampled) \n",
    "# print(\"Class distribution after over-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc40334",
   "metadata": {},
   "source": [
    "Synthetic Minority Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "# x_train_resampled, y_train_resampled = smote.fit_resample(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Class distribution after SMOTE\n",
    "# class_counts_after_smote = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution after SMOTE\", class_counts_after_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662afe8b",
   "metadata": {},
   "source": [
    "Adaptive Synthetic Sampling with ADASYN - Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce20afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adasyn = ADASYN(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "# x_train_resampled, y_train_resampled = adasyn.fit_resample(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Class distribution after ADASYN\n",
    "# class_counts_after_adasyn = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution after ADASYN\", class_counts_after_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "# rename\n",
    "X_train = x_train_resampled\n",
    "Y_train = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f5db",
   "metadata": {},
   "source": [
    "BERT pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f89d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef02bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab68807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = x_train\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_train = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_train.append(seq[0])\n",
    "\n",
    "    lines_pad_x_train = pad_sequences(lines_pad_x_train, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63675629",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_val = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_val.append(seq[0])\n",
    "\n",
    "    lines_pad_x_val = pad_sequences(lines_pad_x_val, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7efba",
   "metadata": {},
   "source": [
    "CodeBERT pre-trained CPP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    model_variation = \"microsoft/codebert-base-mlm\" # \"neulab/codebert-cpp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea203cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = x_train\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc259d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(sequences):\n",
    "    max_len = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "            max_len = len(seq['input_ids'].numpy()[0])\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = get_max_len(sequences)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1277690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_train = padSequences(sequences, max_len)\n",
    "    lines_pad_x_train = [arr.tolist() for arr in lines_pad_x_train]\n",
    "    lines_pad_x_train = np.array(lines_pad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_val = padSequences(sequences, max_len)\n",
    "    lines_pad_x_val = [arr.tolist() for arr in lines_pad_x_val]\n",
    "    lines_pad_x_val = np.array(lines_pad_x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fa37",
   "metadata": {},
   "source": [
    "Functions for Deep Learnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b991073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    model=Sequential()\n",
    "    #model.add(Embedding(input_dim=top_words+1, output_dim=dim, input_length=None, mask_zero=True))\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    model.add(LSTM(100, dropout=0.2, return_sequences=True, stateful=False))\n",
    "    model.add(LSTM(50, dropout=0.1, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    model.add(BatchNormalization(momentum=0.0))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[f1_metric])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a63be2",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe521ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d232a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "BS = 64\n",
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"lstm\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(lines_pad_x_train, y_train, validation_data=(lines_pad_x_val, y_val), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc], class_weight=class_weights)\n",
    "\n",
    "#load best model\n",
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "scores = myModel.evaluate(lines_pad_x_val, y_val, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(lines_pad_x_val)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(y_val, predictions)\n",
    "precision=precision_score(y_val, predictions)\n",
    "recall=recall_score(y_val, predictions)\n",
    "f1=f1_score(y_val, predictions)\n",
    "roc_auc=roc_auc_score(y_val, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(y_val, predictions, labels=[0, 1])\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b9942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
