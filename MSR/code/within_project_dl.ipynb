{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75d92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.regularizers import l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cbca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a24011",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\"w2v\", \"ft\", \"bert\", \"codebert\"]\n",
    "embedding = embeddings[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059895e",
   "metadata": {},
   "source": [
    "Choose a project among Chrome and Linux or the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d86c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(os.path.join('..','data', 'full_data_reduced.csv'))\n",
    "data = pd.read_csv(os.path.join('..','data', 'chrome_data_reduced.csv'))\n",
    "#data = pd.read_csv(os.path.join('..','data', 'linux_data_reduced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5b1cf",
   "metadata": {},
   "source": [
    "Shuffle the dataset before starting operating on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a95914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8597edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0    midimanagerusbtest() : message_loop_(new bas...    0      14\n",
      "1  void renderframehostimpl::cancelblockedrequest...    0       8\n",
      "2  void renderthreadimpl::removeobserver(renderth...    0       7\n",
      "3  void webstorestandaloneinstaller::initinstalld...    0       7\n",
      "4  omniboxstate::omniboxstate(const omniboxeditmo...    0      15\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baebd02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b09ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 80\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69061a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    70232\n",
      "1     3000\n",
      "Name: vul, dtype: int64\n",
      "Vulnerability Percentage:  4.271557124957284 %\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Vulnerability Percentage: \", (vc[1] / vc[0])*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251f04f",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebe5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and then train into train and val (90% train, 10% test and then 90% train and 10% val)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(data[\"func\"].tolist(), data[\"vul\"].tolist(), stratify = data[\"vul\"].tolist(), test_size=0.1, random_state=shuffle_seeder)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, stratify = y_train_val, test_size=0.1, random_state=shuffle_seeder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c0c5a",
   "metadata": {},
   "source": [
    "<b>Handling imbalanced data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931d2fe",
   "metadata": {},
   "source": [
    "Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b76ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0:1, 1:1}\n",
    "#class_weights = {0:len(data) / (len(vc) * vc[0]), 1:len(data) / (len(vc) * vc[1])} #total observations / (number of classes * observations in class)\n",
    "#class_weights = dict(enumerate(compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f96c2e",
   "metadata": {},
   "source": [
    "Under-sampling of the clean samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ffcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59317, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3a86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    56887\n",
      "1     2430\n",
      "dtype: int64\n",
      "Majority class  0\n",
      "Targeted number of majority class 28443\n",
      "Class distribution after under-sampling 0    28443\n",
      "1     2430\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply under-sampling with the specified strategy\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(\"Class distribution \", class_counts)\n",
    "majority_class = class_counts.idxmax()\n",
    "print(\"Majority class \", majority_class)\n",
    "target_count = int(class_counts.iloc[0] / 2) \n",
    "print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "# under\n",
    "sampling_strategy = {majority_class: target_count}        \n",
    "rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train) \n",
    "print(\"Class distribution after under-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc44591",
   "metadata": {},
   "source": [
    "Random Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b82781c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    28443\n",
      "1     2430\n",
      "dtype: int64\n",
      "Minority class  1\n",
      "Targeted number of minority class 28443\n",
      "Class distribution after over-sampling 0    28443\n",
      "1    28443\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply random over-sampling with the specified strategy\n",
    "class_counts = pd.Series(y_train_resampled).value_counts()\n",
    "print(\"Class distribution \", class_counts)\n",
    "minority_class = class_counts.idxmin()\n",
    "print(\"Minority class \", minority_class)\n",
    "target_count = class_counts.iloc[0] #int(class_counts.iloc[1] * 2) \n",
    "print(\"Targeted number of minority class\", target_count)\n",
    "\n",
    "# over\n",
    "sampling_strategy = {minority_class: target_count}        \n",
    "ros = RandomOverSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "x_train_resampled, y_train_resampled = ros.fit_resample(x_train_resampled, y_train_resampled) \n",
    "print(\"Class distribution after over-sampling\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1312fc",
   "metadata": {},
   "source": [
    "Synthetic Minority Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "318e0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "# x_train_resampled, y_train_resampled = smote.fit_resample(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Class distribution after SMOTE\n",
    "# class_counts_after_smote = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution after SMOTE\", class_counts_after_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41404d2b",
   "metadata": {},
   "source": [
    "Adaptive Synthetic Sampling with ADASYN - Over-sampling of the vulnerable samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6b054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adasyn = ADASYN(random_state=seed, sampling_strategy='auto')  # 'auto' means it will resample to have the same number of samples as the majority class\n",
    "# x_train_resampled, y_train_resampled = adasyn.fit_resample(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# # Class distribution after ADASYN\n",
    "# class_counts_after_adasyn = pd.Series(y_train_resampled).value_counts()\n",
    "# print(\"Class distribution after ADASYN\", class_counts_after_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e6517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "# rename\n",
    "X_train = x_train_resampled\n",
    "Y_train = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f5db",
   "metadata": {},
   "source": [
    "BERT pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f89d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    model_variation = \"bert-base-uncased\" # \"roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation)\n",
    "    bert = TFAutoModel.from_pretrained(model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef02bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab68807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer.encode(sente[0], truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d09c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_train = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_train.append(seq[0])\n",
    "\n",
    "    lines_pad_x_train = pad_sequences(lines_pad_x_train, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f66a1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63675629",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"bert\":\n",
    "    lines_pad_x_val = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_val.append(seq[0])\n",
    "\n",
    "    lines_pad_x_val = pad_sequences(lines_pad_x_val, padding = 'post', maxlen = 512)\n",
    "    lines_pad_x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7efba",
   "metadata": {},
   "source": [
    "CodeBERT pre-trained CPP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c8c38dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at microsoft/codebert-base-mlm were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base-mlm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"codebert\":\n",
    "    model_variation = \"microsoft/codebert-base-mlm\" # \"neulab/codebert-cpp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=False) # , do_lower_case=True\n",
    "    codebert = TFAutoModel.from_pretrained(model_variation) # , from_pt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea203cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"codebert\":\n",
    "    codebert_embeddings = codebert.get_input_embeddings()\n",
    "    embedding_matrix = codebert_embeddings.weights[0].numpy()\n",
    "    num_words = len(embedding_matrix)\n",
    "    print(num_words)\n",
    "    dim = len(embedding_matrix[0])\n",
    "    print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd3d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer(sente[0], return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc259d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSequences(sequences, max_len):\n",
    "    lines_pad = []\n",
    "    for sequence in sequences:\n",
    "        seq = sequence['input_ids'].numpy()[0]\n",
    "        if len(seq) < max_len:\n",
    "            for i in range(len(seq), max_len):\n",
    "                seq = np.append(seq, 0)\n",
    "        lines_pad.append(seq)\n",
    "    return lines_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7586bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(sequences):\n",
    "    max_len = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "            max_len = len(seq['input_ids'].numpy()[0])\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83c1e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"codebert\":\n",
    "    max_len = get_max_len(sequences)\n",
    "    print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1277690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_train = padSequences(sequences, max_len)\n",
    "    lines_pad_x_train = [arr.tolist() for arr in lines_pad_x_train]\n",
    "    lines_pad_x_train = np.array(lines_pad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e01ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    sentences = x_val\n",
    "    sequences = [tokenizer(sente, return_tensors=\"tf\", truncation=True, add_special_tokens=False) for sente in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e1a627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"codebert\":\n",
    "    lines_pad_x_val = padSequences(sequences, max_len)\n",
    "    lines_pad_x_val = [arr.tolist() for arr in lines_pad_x_val]\n",
    "    lines_pad_x_val = np.array(lines_pad_x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fa37",
   "metadata": {},
   "source": [
    "Functions for Deep Learnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cbeaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2\n",
    "\n",
    "def f2_loss(y_true, y_pred):\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    #tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2 = 5*p*r / (4*p+r+K.epsilon())\n",
    "    f2 = tf.where(tf.math.is_nan(f2), tf.zeros_like(f2), f2)\n",
    "    \n",
    "    return 1 - K.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b991073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    #model.add(SimpleRNN(300, dropout=0.3, stateful=False))\n",
    "    #model.add(Bidirectional(LSTM(300, dropout=0.3, stateful=False)))\n",
    "    #model.add(GRU(300, dropout=0.3, stateful=False))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate) # default 0.001\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    cnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[f2_metric])\n",
    "    return cnn_model\n",
    "\n",
    "def buildLstmCNN(max_len, top_words, dim, seed, embedding_matrix, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    kernel_initializer = glorot_uniform()\n",
    "\n",
    "    # Embedding Layer\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "\n",
    "    # LSTM/GRU Layers\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    # CNN Layers\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', kernel_initializer=kernel_initializer)) \n",
    "    model.add(GlobalMaxPool1D())\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    #model.add(Dense(64, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    # Classification layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f2_metric])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a63be2",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe521ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(Y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d232a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 768)         38603520  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 500)         1905000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 100)         180600    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 200)         181200    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         76928     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 40,947,377\n",
      "Trainable params: 2,343,857\n",
      "Non-trainable params: 38,603,520\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/100\n",
      "889/889 [==============================] - 191s 201ms/step - loss: 0.3473 - f2_metric: 0.8446 - val_loss: 0.2496 - val_f2_metric: 0.62078 - loss: 0. - ETA: 2:23 - loss: 0.5544 - f2_metric: - ETA: 2:22 - loss: 0.5508 - - ETA: 2:18 - loss: 0.5425  - ETA: 2:14 - loss: 0.534 - ETA: 1:53 - loss: 0.4866 - f2_me - ETA: 1:51 - loss: 0.4824 - f2_me - ETA: 1:48 - loss: 0.4796 - f2_metric - ETA: 1:47 - loss: 0.4772 - f2_metric: - ETA: 1:46 - loss: 0.4754 - f2_metric: 0.75 - ETA: 1:45 - loss: 0.4747 - f2_m - E - ETA: 1:35 - los - ETA: 1:29 - loss: 0.4506 - f2_metric - ETA: 1:27 - loss: 0.4490 - f2_m - ETA: 1:25 - loss: 0.4458 - f2_metric: 0.7 - ETA: 1:25 - loss: 0.4453 - f2_metric: 0.777 - ETA: 1:24 - loss: 0.4451 - f2_metric - ETA: 1:23 - loss: 0.4432 - f2_met - ETA: 1:21 - loss: 0.4409 - f2_metric: 0.78 - ETA: 1:20 - loss: 0.4404 - f2_metric: 0 - ETA: 1:19 - loss: 0.4392 - f2_metric: 0 - ETA: 1:18 - loss: 0.4382 - f2_metric: - ETA: 1:08 - loss: 0.4239 - f2_metric - ETA: 1:07 - loss: 0.4221 - f2_metric: 0.793 - ETA: 1:06 - loss: 0.4217 - f2 - ETA: 13s - loss: 0.3611 - f2_metric:  - ETA: 1 - ETA: 9s - los - ETA: 3s - loss: 0.350\n",
      "\n",
      "Epoch 00001: val_f2_metric improved from -inf to 0.62075, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "889/889 [==============================] - 177s 199ms/step - loss: 0.1391 - f2_metric: 0.9689 - val_loss: 0.1939 - val_f2_metric: 0.6769 2:42 - loss: 0.1973 - f2_metric: 0 - ETA: 2:43 - loss: 0.1 - ETA: 2:39 - loss: 0.1962 - f2_metric: 0. - ETA: 2:38 - loss: 0.1935 - f2_metric: 0. - ETA: - ETA: 2:30 - loss: 0.1794 - f2_metri - ETA: 2:29  - ETA: 2:23 - loss: 0.1856 - f2_metric: 0.94 - ETA: 2:23 - loss: 0.1854 - f2_metric: 0.948 - ETA: 2:22 - loss: 0.1854 - ETA: 2:19 - loss: 0.1813 - f2_metric: 0.95 - ETA: 2 - ETA: 2:11 - loss: 0.1741 - f2_met - ETA: 2:09 - loss:  - ETA: 2:05 - loss: 0 - ETA: 2:00 - loss: 0.1 - ETA: 1:56 - loss: 0.1629 - ETA: 1:52 - loss -  - ETA: 6s - loss: 0.1403 - f2_metric: 0.96 - ETA: 6s -\n",
      "\n",
      "Epoch 00002: val_f2_metric improved from 0.62075 to 0.67685, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "889/889 [==============================] - 177s 199ms/step - loss: 0.1012 - f2_metric: 0.9824 - val_loss: 0.1838 - val_f2_metric: 0.6904loss: 0.0898 - - ETA: 2:43 - loss: 0.0934 - f2_me - ETA: 2:40 - loss: 0.0950 - - ETA: 2:36 - loss: 0.0932 - f2_metric: 0.985 - ETA: 2:36 - loss: 0.09 - ETA: 2:31 - loss: 0.0964 - f2 - ETA: 2:28 - loss: 0.0979 - f2_metric: 0 - ETA: 2:19 - loss: 0.1054 - f2_metric: 0. - - ETA: 2:09 - loss: 0.1108 - f2_metric: 0. - ETA - ETA: 2:01 - loss: 0 - ETA: 1:56 - loss: 0.1131 - - ETA: 1:52 - loss: - ETA: 1:47 - loss - ETA: 1:42 - loss - ETA: 1:37 - loss: 0.1139 - f2_metric: 0 - ETA: 1:36 - - ETA: 1:29 - loss: 0.1121 - f2_metric: - ETA: 1:28 - loss: 0.1119 - f2_me - ETA: 1:26 - loss: 0.1118 -  - ETA: 1:22 - - ETA: 1:16 - loss: 0.1096 - f2_metric: 0 - ETA: 1:15 - loss: 0.1100 - f2_metric: 0.98 - ETA: 1:15 - loss: 0.1101 - f2_metric: 0.980 - ETA: 1:14 - loss: - ETA: 4s - loss: 0.1020 - f2_metric: 0. - ETA: 3s - loss: 0.1018 - f2 - ETA: 0s - loss: 0.1014 - f2_metric: 0\n",
      "\n",
      "Epoch 00003: val_f2_metric improved from 0.67685 to 0.69045, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "889/889 [==============================] - 177s 199ms/step - loss: 0.0823 - f2_metric: 0.9861 - val_loss: 0.1658 - val_f2_metric: 0.6639- ETA: 2:38 - loss: 0.0875 - f2_me - ETA: 2:36 - loss - ETA: 2:31 - loss: 0.0 - ET - ETA: 2:08 - los - ETA: 2:03 -  - ETA: 1:57 - loss: 0.0805 - f2_metr - ETA: 1:55 - loss: 0.08 - ETA: 1:51 - loss: 0.0810 - f2_met - ETA: 1:49 - loss: 0.0815 -  - ETA: 1:45 - loss: 0.0821  - ETA: 1:42 - loss: 0.0823  - ETA: 1:38 - loss: 0.0837 - f2_metric: 0.984 - ETA: 1:38 - loss: 0.0837 - f2_me -  - ETA: 16s - loss: 0.0830 - f2_metric: 0.98 - ETA: 16s - loss: 0.08 - ETA: 13s - loss: 0.0831 - f - ETA: 12s - loss: 0.0831 - f2_me - ETA: 11s - loss: 0.083\n",
      "\n",
      "Epoch 00004: val_f2_metric did not improve from 0.69045\n",
      "Epoch 5/100\n",
      "889/889 [==============================] - 178s 201ms/step - loss: 0.0695 - f2_metric: 0.9882 - val_loss: 0.1533 - val_f2_metric: 0.6370- loss: 0.0553 - f2_ - ETA: 2:36 - l - - ETA: 2:22 - loss: 0.0595 - f2_metric: 0. - ETA: 2: - ETA: 2:14 - loss: 0.0628 - f2_metric: 0.99 - ETA: 2:14  - E - ETA: 1:41 - loss: 0.0642 - f2_metric: 0 - ETA: 1:40 - loss: 0.0642 - f - ETA: 1:36 - loss: 0.0647 - f2_metric: - ETA: 1:35 - loss: 0.0647 - f2_m - ETA: 1:33 - loss: 0.06 - ETA: 1:28 - loss: 0.0651 - f2_me - ETA: 1:26 - loss: 0. - ETA: 1:22 - loss: 0.0653 - f2_metric: 0. - ETA: 1:21  - ETA: 1:14 - loss: 0.0653 - f2_metric: 0.989 - ETA: 1:05 - loss: 0.0677 - ETA: 1:01 - loss: 0.0688 - f2_m - ETA: 59s - loss: 0.0690 - f2_metric: 0.98 - ETA: 59s - loss: 0.0690 - f2_metric:  - ETA: 58s - loss: 0.0694 - f2_ - ETA: 13s - loss:  - ETA: 2s - loss: 0.0691 - f2_\n",
      "\n",
      "Epoch 00005: val_f2_metric did not improve from 0.69045\n",
      "Epoch 6/100\n",
      "889/889 [==============================] - 177s 199ms/step - loss: 0.0613 - f2_metric: 0.9891 - val_loss: 0.2071 - val_f2_metric: 0.64810 - ETA: 2:20 - loss - ETA: 2:15 - loss: 0.05 - ETA: 1:44 - loss: 0.0589 - f2_metric: - ETA: 1:4 - ETA: 1:36 - loss: 0.0587 - f2_metric - ETA: 1:35 - loss: 0.0591 - f - ETA: 1:32 - loss: 0.0585 - f2_met - ETA: 1:30 - loss: 0.0588  - ETA: 1:26 -  - ETA: 17s - loss: 0.0617 - f2_metr - ETA: 16s  - E\n",
      "\n",
      "Epoch 00006: val_f2_metric did not improve from 0.69045\n",
      "Epoch 7/100\n",
      "889/889 [==============================] - 177s 200ms/step - loss: 0.0525 - f2_metric: 0.9903 - val_loss: 0.1702 - val_f2_metric: 0.6443 0.0501 - f2_m - ETA: 2:39 - loss: 0.0515 - f2_metri - ETA: 2:37 - loss: 0.0486 - f2_metric: 0.99 - ETA: 2:37 - loss: 0.04 - ETA: 2:33 - loss: 0.0489 -  - ETA: 2:29 - loss: 0.0491 - f - ETA: 2:26 - loss: 0.0 - E - ETA: 1:47 - lo - ETA: 1:41 - loss - ETA: 1:36 - loss: 0.0493 - f2 - ETA: 1:33 - loss: 0.0485 - f2 - ETA: 1:30 - loss: 0.0486 - f2_metr - ETA: 1:2 - ETA: 1:21 - loss: 0.0496 - - ETA: 1:18 - loss: 0.0499 - ETA: 1:14 - ETA: 1:07  - ETA: 51s - loss: 0.0525 - f2_me - ETA: 50s - loss: 0.0524 - f2_ - ETA: 44s - loss: 0.0518 - f2_metr - ETA: 43s - loss: 0.0520 - f2_metric - ETA: 43s - loss: 0.0520 - f - ETA: 41s - loss: 0.0518 - ETA: 35s - loss - ETA: 32s - loss: 0.05 - ETA: 30s - loss: 0.0528 - f2_ - ETA:  - ETA: 25s - loss:  - ETA: 23s  - ETA: 19 - ETA:  - - ETA\n",
      "\n",
      "Epoch 00007: val_f2_metric did not improve from 0.69045\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889/889 [==============================] - 178s 200ms/step - loss: 0.0460 - f2_metric: 0.9903 - val_loss: 0.2004 - val_f2_metric: 0.6299 - ETA: 2:34 - loss: 0.027 - ETA: 2:31 - loss: 0.0297 - f2_metric:  - ETA: 2:29 - loss: 0.0307  - ETA: 2:26 - loss: 0.030 - ETA: 2:23 - loss: 0.0344 - f2_metric: - ETA: 2:22 - lo - ETA: 2:17 - loss: 0.0330 - f2_ - ETA: 2:14 -  - ETA: 2:08 - loss: 0.0356 - f2_metr - ETA: 2:06 - loss: 0.0354 - ETA: 1:54 - loss: - ETA: 1:49 - loss:  - ETA:  - ETA: 1:37 - loss: 0.0420 - f2_metric: 0. - ETA: 1:36 - loss: 0.0422 - f2_metric: 0.99 - ETA: 1:35 - loss: 0.0421 - f2_metric: 0.991 - ETA: 1:35 - loss: - ETA: 1:30 - loss: 0. - ETA: 1:25 - loss: 0.0451 -  - ETA: 1:22 - loss: 0.04 - ETA: 1:18 -  - ETA: 1:03 - loss: 0.0465 - f2_metric: 0 - ETA:  - E - ETA: 48 - ETA: 45s - loss: 0.04 - ETA: 43s - loss: 0.0461 - f2_metric:  - ETA: 42s - loss: 0.046 - ETA: 1s - loss: 0.0462 - f2_metr\n",
      "\n",
      "Epoch 00008: val_f2_metric did not improve from 0.69045\n",
      "Epoch 9/100\n",
      "889/889 [==============================] - 177s 199ms/step - loss: 0.0412 - f2_metric: 0.9914 - val_loss: 0.2625 - val_f2_metric: 0.6482c: - E - ETA: 12s - loss: 0.0413 - f2_metric: 0.99 - E - ETA: 7s - loss: 0.0413 - ETA: 3s - loss: 0.0414 \n",
      "\n",
      "Epoch 00009: val_f2_metric did not improve from 0.69045\n",
      "Epoch 10/100\n",
      "889/889 [==============================] - 178s 200ms/step - loss: 0.0344 - f2_metric: 0.9924 - val_loss: 0.1968 - val_f2_metric: 0.538948 - loss: 0.0318 - f2_met - ETA: 2:45 - loss: 0.0314 - f2_metric: 0.99 - ETA: 2:44 - loss: 0.0290 - f2_ - ETA: 2:42 - loss: 0.0277 - f2_metric: 0.99 - ETA: 2:42 - loss: 0.0307 - f2_metric: 0.9 - ETA: 2:41 - loss: 0.0334 - ETA: 2:38 - loss: 0.0329 - f2_metri - ETA: 2:36 - loss: 0.0337 - f2_metr - ETA: 2:07 - loss: 0.0345 - f2_metric: 0.99 - - ETA: 1:58 - loss: 0.0367 - f2_ - ETA: 1:56 - loss: 0.0369 - f2_metric:  - ETA: 1:27 - loss:\n",
      "\n",
      "Epoch 00010: val_f2_metric did not improve from 0.69045\n",
      "Epoch 11/100\n",
      "889/889 [==============================] - 178s 200ms/step - loss: 0.0284 - f2_metric: 0.9935 - val_loss: 0.2363 - val_f2_metric: 0.6328 ETA: 2:30 - loss: 0.0332 - f - ETA: 2:27 - loss: 0.0316 - - ETA: 2:23 - loss: 0.0308 - f2_metric: 0.99 - ETA: 2:23 - loss: 0.0304 - f2_metric: - ETA: 2:22 - ETA: 2:06 - loss: 0.0276 - f2_metric: 0. - ETA: 2:05 - loss: 0.0277 - f2_metric: 0 - ETA: 2:05 - loss: 0 - ETA: 2:00 - loss: 0.0281 - f2_metric: 0.99 - ETA: 1:59 - loss: 0.0280 - f2_met - ETA: 1:57 - loss: 0.0277 - - ETA: 1:54 - loss: 0.0287 - f2_metric: 0. - ETA: 1:53 - loss: 0.0284 - f2_metric: 0. - ETA: - ETA: 1:45 - loss - ETA: 1:39 - loss: 0.0271  - ETA: 1:35 - loss: 0.0268 - f2_metri - ETA: 1:34 - loss: 0 - ETA: 1:29 - loss - ETA: 1:15 - loss: 0.0264 - - ETA: 1: - ETA: 53s - loss: 0.0274 - f2_me - ETA: 52 - E - - ETA: 18s - loss: 0.0276 - ETA: 12s - lo - ETA: 8s - loss: 0.0278 - f2_metric: - ETA:  - ETA: 0s - loss: 0.0283 - f2_metric: 0.9\n",
      "\n",
      "Epoch 00011: val_f2_metric did not improve from 0.69045\n",
      "Epoch 12/100\n",
      "889/889 [==============================] - 179s 201ms/step - loss: 0.0267 - f2_metric: 0.9935 - val_loss: 0.2054 - val_f2_metric: 0.576145 - f2_metric: 0 - ETA: 1:37 - loss: 0.0243 - f2_ - ETA: 1:35 - loss: 0.0243 - f2 - ETA: 1:32 - loss: 0.0242 -  - ETA: 1:19 - ETA: 1:13 - loss: 0.0239 - f2_metric: 0. - ETA: 1:12 - loss: 0.0238 -  - ETA: 1 - ETA:  - ETA: 57s - loss:  - ETA: 54s - loss: 0. - ETA: 38s - loss - ETA: 36s - loss: 0.02 - ETA: 25s - loss\n",
      "\n",
      "Epoch 00012: val_f2_metric did not improve from 0.69045\n",
      "Epoch 13/100\n",
      "889/889 [==============================] - 179s 201ms/step - loss: 0.0241 - f2_metric: 0.9944 - val_loss: 0.2513 - val_f2_metric: 0.6272loss: 0.0234 - f2_metric:  - ETA - ETA: 2:31 - loss: 0. - ETA: 2:27 - loss:  - ETA: 2:04 - lo - ETA: 1:58 - loss: 0.0205 - ETA: 1:54 - loss: 0.0203 - - ETA: 1:51 -  - ETA:  - ETA: 1:28 - loss: 0.0203 - f2_metri - ETA: 1:26 - loss: 0 - ET - ETA: 17s - loss: 0.0230 - f2_ - ETA: 16s - loss - ETA: 13s - loss: 0.0231 - f2_metri - ETA: 6s\n",
      "\n",
      "Epoch 00013: val_f2_metric did not improve from 0.69045\n",
      "Epoch 00013: early stopping\n",
      "TP= 238\n",
      "TN= 6037\n",
      "FP= 284\n",
      "FN= 32\n",
      "Accuracy:95.21%\n",
      "Precision:45.59%\n",
      "Recall:88.15%\n",
      "F1 score:60.10%\n",
      "Roc_Auc score:91.83%\n",
      "F2 score:74.28%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      6321\n",
      "           1       0.46      0.88      0.60       270\n",
      "\n",
      "    accuracy                           0.95      6591\n",
      "   macro avg       0.73      0.92      0.79      6591\n",
      "weighted avg       0.97      0.95      0.96      6591\n",
      "\n",
      "Cross Validation is completed after 2349905\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "BS = 64\n",
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"lstmCnn\"#\"lstm\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix)\n",
    "elif userModel == \"lstmCnn\":\n",
    "    myModel = buildLstmCNN(max_len, num_words, dim, seed, embedding_matrix)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f2_metric', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f2_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(lines_pad_x_train, y_train, validation_data=(lines_pad_x_val, y_val), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc], class_weight=class_weights)\n",
    "\n",
    "#load best model\n",
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "scores = myModel.evaluate(lines_pad_x_val, y_val, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(lines_pad_x_val)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(y_val, predictions)\n",
    "precision=precision_score(y_val, predictions)\n",
    "recall=recall_score(y_val, predictions)\n",
    "f1=f1_score(y_val, predictions)\n",
    "roc_auc=roc_auc_score(y_val, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(y_val, predictions, labels=[0, 1])\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(y_val, predictions))\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Cross Validation is completed after\", milli_sec2-milli_sec1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
